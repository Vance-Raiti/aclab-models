device: cuda:0
save interval: 100000
batch size: 32
n_blocks=12
writing to model-#.pth
device: cuda:0
save interval: 100000
batch size: 32
n_blocks=12
writing to model-#.pth
device: cuda:0
save interval: 100000
batch size: 32
epoch 1 iter 0: train loss 0.57557. lr 3.000000e-04, running loss 0.57557, it/sec: 0.03883319647030237
epoch 1 iter 20: train loss 1.09290. lr 3.000000e-04, running loss 1.12895, it/sec: 4.448903966742641
epoch 1 iter 40: train loss 1.11233. lr 3.000000e-04, running loss 1.11951, it/sec: 4.534479292569094
epoch 1 iter 60: train loss 1.08641. lr 3.000000e-04, running loss 1.11721, it/sec: 4.550631054693169
epoch 1 iter 80: train loss 1.12249. lr 3.000000e-04, running loss 1.11495, it/sec: 4.4514293694405325
epoch 1 iter 100: train loss 1.10727. lr 3.000000e-04, running loss 1.11438, it/sec: 4.466864610493013
epoch 1 iter 120: train loss 1.09756. lr 3.000000e-04, running loss 1.11445, it/sec: 4.5299653587773525
epoch 1 iter 140: train loss 1.10163. lr 3.000000e-04, running loss 1.11427, it/sec: 4.475505145619667
epoch 1 iter 160: train loss 1.10694. lr 3.000000e-04, running loss 1.11431, it/sec: 4.502592605618333
epoch 1 iter 180: train loss 1.10495. lr 3.000000e-04, running loss 1.11378, it/sec: 4.550488834131547
epoch 1 iter 200: train loss 1.10971. lr 3.000000e-04, running loss 1.11301, it/sec: 4.4821568741413875
epoch 1 iter 220: train loss 1.11122. lr 3.000000e-04, running loss 1.11314, it/sec: 4.498662505506583
epoch 1 iter 240: train loss 1.12551. lr 3.000000e-04, running loss 1.11410, it/sec: 4.475242406196586
epoch 1 iter 260: train loss 1.08700. lr 3.000000e-04, running loss 1.11362, it/sec: 4.533069992011479
epoch 1 iter 280: train loss 1.08727. lr 3.000000e-04, running loss 1.11354, it/sec: 4.456413102401279
epoch 1 iter 300: train loss 1.11700. lr 3.000000e-04, running loss 1.11379, it/sec: 4.541807494669101
epoch 1 iter 320: train loss 1.12109. lr 3.000000e-04, running loss 1.11361, it/sec: 4.439597135132926
epoch 1 iter 340: train loss 1.14154. lr 3.000000e-04, running loss 1.11369, it/sec: 4.4889472088571685
epoch 1 iter 360: train loss 1.10279. lr 3.000000e-04, running loss 1.11363, it/sec: 4.457736244758882
epoch 1 iter 380: train loss 1.11403. lr 3.000000e-04, running loss 1.11352, it/sec: 4.5252974514361135
epoch 1 iter 400: train loss 1.08712. lr 3.000000e-04, running loss 1.11349, it/sec: 4.4641924642754605
epoch 1 iter 420: train loss 1.11572. lr 3.000000e-04, running loss 1.11354, it/sec: 4.515804960235211
epoch 1 iter 440: train loss 1.13381. lr 3.000000e-04, running loss 1.11360, it/sec: 4.452799995634875
epoch 1 iter 460: train loss 1.10882. lr 3.000000e-04, running loss 1.11360, it/sec: 4.509801144080686
epoch 1 iter 480: train loss 1.11395. lr 3.000000e-04, running loss 1.11399, it/sec: 4.491774129047823
epoch 1 iter 500: train loss 1.10127. lr 3.000000e-04, running loss 1.11420, it/sec: 4.50706132827058
epoch 1 iter 520: train loss 1.11003. lr 3.000000e-04, running loss 1.11432, it/sec: 4.465196414681549
epoch 1 iter 540: train loss 1.10208. lr 3.000000e-04, running loss 1.11414, it/sec: 4.516585720609872
epoch 1 iter 560: train loss 1.12202. lr 3.000000e-04, running loss 1.11403, it/sec: 4.432470745236832
epoch 1 iter 580: train loss 1.11002. lr 3.000000e-04, running loss 1.11402, it/sec: 4.508028523799422
epoch 1 iter 600: train loss 1.12517. lr 3.000000e-04, running loss 1.11397, it/sec: 4.428476381977154
epoch 1 iter 620: train loss 1.10398. lr 3.000000e-04, running loss 1.11397, it/sec: 4.522743772317829
epoch 1 iter 640: train loss 1.11611. lr 3.000000e-04, running loss 1.11435, it/sec: 4.431464743468273
epoch 1 iter 660: train loss 1.12294. lr 3.000000e-04, running loss 1.11435, it/sec: 4.505176079479208
epoch 1 iter 680: train loss 1.10135. lr 3.000000e-04, running loss 1.11483, it/sec: 4.481100724971689
epoch 1 iter 700: train loss 1.10944. lr 3.000000e-04, running loss 1.11484, it/sec: 4.514364445356088
epoch 1 iter 720: train loss 1.13720. lr 3.000000e-04, running loss 1.11488, it/sec: 4.470636473287093
epoch 1 iter 740: train loss 1.11604. lr 3.000000e-04, running loss 1.11483, it/sec: 4.502070608316538
epoch 1 iter 760: train loss 1.12138. lr 3.000000e-04, running loss 1.11488, it/sec: 4.481369354633289
epoch 1 iter 780: train loss 1.11780. lr 3.000000e-04, running loss 1.11497, it/sec: 4.506196397849636
epoch 1 iter 800: train loss 1.11141. lr 3.000000e-04, running loss 1.11510, it/sec: 4.477426561026547
epoch 1 iter 820: train loss 1.10945. lr 3.000000e-04, running loss 1.11494, it/sec: 4.500398619818035
epoch 1 iter 840: train loss 1.10460. lr 3.000000e-04, running loss 1.11492, it/sec: 4.489556726646384
epoch 1 iter 860: train loss 1.11306. lr 3.000000e-04, running loss 1.11506, it/sec: 4.507208910533796
epoch 1 iter 880: train loss 1.11638. lr 3.000000e-04, running loss 1.11502, it/sec: 4.474610938829971
epoch 1 iter 900: train loss 1.31613. lr 3.000000e-04, running loss 1.11519, it/sec: 4.530651034513224
epoch 1 iter 920: train loss 1.10389. lr 3.000000e-04, running loss 1.11520, it/sec: 4.474711532731088
epoch 1 iter 940: train loss 1.12509. lr 3.000000e-04, running loss 1.11524, it/sec: 4.512664010000159
epoch 1 iter 960: train loss 1.09561. lr 3.000000e-04, running loss 1.11518, it/sec: 4.469277518095501
epoch 1 iter 980: train loss 1.10795. lr 3.000000e-04, running loss 1.11512, it/sec: 4.464730333512651
epoch 1 iter 1000: train loss 1.15618. lr 3.000000e-04, running loss 1.11498, it/sec: 4.515443512240329
epoch 1 iter 1020: train loss 1.11763. lr 3.000000e-04, running loss 1.11500, it/sec: 4.480474689158879
epoch 1 iter 1040: train loss 1.11494. lr 3.000000e-04, running loss 1.11502, it/sec: 4.472960901547662
epoch 1 iter 1060: train loss 1.12339. lr 3.000000e-04, running loss 1.11493, it/sec: 4.484880486321087
epoch 1 iter 1080: train loss 1.10333. lr 3.000000e-04, running loss 1.11485, it/sec: 4.4752962211681435
epoch 1 iter 1100: train loss 1.10973. lr 3.000000e-04, running loss 1.11473, it/sec: 4.531351270801239
epoch 1 iter 1120: train loss 1.12318. lr 3.000000e-04, running loss 1.11481, it/sec: 4.476787403045265
epoch 1 iter 1140: train loss 1.10787. lr 3.000000e-04, running loss 1.11473, it/sec: 4.505539113674359
epoch 1 iter 1160: train loss 1.10724. lr 3.000000e-04, running loss 1.11463, it/sec: 4.478755272542367
epoch 1 iter 1180: train loss 1.10958. lr 3.000000e-04, running loss 1.11456, it/sec: 4.519451878470338
epoch 1 iter 1200: train loss 1.11840. lr 3.000000e-04, running loss 1.11452, it/sec: 4.419467616618176
epoch 1 iter 1220: train loss 1.09969. lr 3.000000e-04, running loss 1.11457, it/sec: 4.5494689135700455
epoch 1 iter 1240: train loss 1.11016. lr 3.000000e-04, running loss 1.11468, it/sec: 4.459827439686547
epoch 1 iter 1260: train loss 1.12367. lr 3.000000e-04, running loss 1.11470, it/sec: 4.49518733860215
epoch 1 iter 1280: train loss 1.09459. lr 3.000000e-04, running loss 1.11470, it/sec: 4.502506385914383
epoch 1 iter 1300: train loss 1.10550. lr 3.000000e-04, running loss 1.11466, it/sec: 4.505450019777792
epoch 1 iter 1320: train loss 1.14031. lr 3.000000e-04, running loss 1.11472, it/sec: 4.4729012000100585
epoch 1 iter 1340: train loss 1.10198. lr 3.000000e-04, running loss 1.11459, it/sec: 4.530997861458484
epoch 1 iter 1360: train loss 1.10873. lr 3.000000e-04, running loss 1.11458, it/sec: 4.470535083003929
epoch 1 iter 1380: train loss 1.11603. lr 3.000000e-04, running loss 1.11464, it/sec: 4.512075807577326
epoch 1 iter 1400: train loss 1.11954. lr 3.000000e-04, running loss 1.11460, it/sec: 4.445181118969404
epoch 1 iter 1420: train loss 1.12414. lr 3.000000e-04, running loss 1.11462, it/sec: 4.529878536678643
epoch 1 iter 1440: train loss 1.12622. lr 3.000000e-04, running loss 1.11451, it/sec: 4.441958586211592
epoch 1 iter 1460: train loss 1.11643. lr 3.000000e-04, running loss 1.11442, it/sec: 4.519268527501963
epoch 1 iter 1480: train loss 1.10620. lr 3.000000e-04, running loss 1.11431, it/sec: 4.453811085454388
epoch 1 iter 1500: train loss 1.11873. lr 3.000000e-04, running loss 1.11436, it/sec: 4.4745718164457156
epoch 1 iter 1520: train loss 1.76683. lr 3.000000e-04, running loss 1.11515, it/sec: 4.429216326863531
epoch 1 iter 1540: train loss 1.11324. lr 3.000000e-04, running loss 1.11505, it/sec: 4.499858805639811
epoch 1 iter 1560: train loss 1.15349. lr 3.000000e-04, running loss 1.11527, it/sec: 4.48228617462903
epoch 1 iter 1580: train loss 1.10034. lr 3.000000e-04, running loss 1.11538, it/sec: 4.50141904556018
epoch 1 iter 1600: train loss 1.13187. lr 3.000000e-04, running loss 1.11533, it/sec: 4.472219611447972
epoch 1 iter 1620: train loss 1.11824. lr 3.000000e-04, running loss 1.11553, it/sec: 4.51966092240618
epoch 1 iter 1640: train loss 1.11198. lr 3.000000e-04, running loss 1.11536, it/sec: 4.437827210120695
epoch 1 iter 1660: train loss 1.11595. lr 3.000000e-04, running loss 1.11561, it/sec: 4.507702292415701
epoch 1 iter 1680: train loss 1.11697. lr 3.000000e-04, running loss 1.11553, it/sec: 4.463529763610781
epoch 1 iter 1700: train loss 1.11080. lr 3.000000e-04, running loss 1.11550, it/sec: 4.4945223324498
epoch 1 iter 1720: train loss 1.11264. lr 3.000000e-04, running loss 1.11554, it/sec: 4.462875066630219
epoch 1 iter 1740: train loss 1.11462. lr 3.000000e-04, running loss 1.11554, it/sec: 4.501528648494086
epoch 1 iter 1760: train loss 1.12305. lr 3.000000e-04, running loss 1.11568, it/sec: 4.4475637474418335
epoch 1 iter 1780: train loss 1.11039. lr 3.000000e-04, running loss 1.11570, it/sec: 4.524964539824303
epoch 1 iter 1800: train loss 1.12603. lr 3.000000e-04, running loss 1.11564, it/sec: 4.437265106511015
epoch 1 iter 1820: train loss 1.10406. lr 3.000000e-04, running loss 1.11556, it/sec: 4.4833263402348065
epoch 1 iter 1840: train loss 1.11626. lr 3.000000e-04, running loss 1.11549, it/sec: 4.441265287058481
epoch 1 iter 1860: train loss 1.11678. lr 3.000000e-04, running loss 1.11547, it/sec: 4.525344530957796
epoch 1 iter 1880: train loss 1.09817. lr 3.000000e-04, running loss 1.11536, it/sec: 4.45297176702932
epoch 1 iter 1900: train loss 1.11551. lr 3.000000e-04, running loss 1.11529, it/sec: 4.5230679500146
epoch 1 iter 1920: train loss 1.11695. lr 3.000000e-04, running loss 1.11526, it/sec: 4.460066790367016
epoch 1 iter 1940: train loss 1.13347. lr 3.000000e-04, running loss 1.11535, it/sec: 4.507265936579121
epoch 1 iter 1960: train loss 1.11575. lr 3.000000e-04, running loss 1.11532, it/sec: 4.456113004279861
epoch 1 iter 1980: train loss 1.11813. lr 3.000000e-04, running loss 1.11532, it/sec: 4.500162131493684
epoch 1 iter 2000: train loss 1.12543. lr 3.000000e-04, running loss 1.11540, it/sec: 4.459153984482453
epoch 1 iter 2020: train loss 1.11315. lr 3.000000e-04, running loss 1.11552, it/sec: 4.524030956158216
epoch 1 iter 2040: train loss 1.10514. lr 3.000000e-04, running loss 1.11541, it/sec: 4.466871774084103
epoch 1 iter 2060: train loss 1.10780. lr 3.000000e-04, running loss 1.11539, it/sec: 4.47007697850686
epoch 1 iter 2080: train loss 1.08883. lr 3.000000e-04, running loss 1.11524, it/sec: 4.5018757932869
epoch 1 iter 2100: train loss 1.11937. lr 3.000000e-04, running loss 1.11508, it/sec: 4.4903426315123625
epoch 1 iter 2120: train loss 1.11868. lr 3.000000e-04, running loss 1.11557, it/sec: 4.50320685002007
epoch 1 iter 2140: train loss 1.12990. lr 3.000000e-04, running loss 1.11549, it/sec: 4.444754312928459
epoch 1 iter 2160: train loss 1.14709. lr 3.000000e-04, running loss 1.11560, it/sec: 4.513287748480486
epoch 1 iter 2180: train loss 1.10931. lr 3.000000e-04, running loss 1.11557, it/sec: 4.468284729311836
epoch 1 iter 2200: train loss 1.09930. lr 3.000000e-04, running loss 1.11572, it/sec: 4.503348847815033
epoch 1 iter 2220: train loss 1.10787. lr 3.000000e-04, running loss 1.11564, it/sec: 4.436520523629079
epoch 1 iter 2240: train loss 1.11264. lr 3.000000e-04, running loss 1.11567, it/sec: 4.506922123035452
epoch 1 iter 2260: train loss 1.11815. lr 3.000000e-04, running loss 1.11564, it/sec: 4.450616678036495
epoch 1 iter 2280: train loss 1.10795. lr 3.000000e-04, running loss 1.11558, it/sec: 4.504135430156757
epoch 1 iter 2300: train loss 1.16608. lr 3.000000e-04, running loss 1.11549, it/sec: 4.449467952618313
epoch 1 iter 2320: train loss 1.11173. lr 3.000000e-04, running loss 1.11550, it/sec: 4.522092674575659
epoch 1 iter 2340: train loss 1.11212. lr 3.000000e-04, running loss 1.11551, it/sec: 4.463690549331321
epoch 1 iter 2360: train loss 1.12126. lr 3.000000e-04, running loss 1.11601, it/sec: 4.494844192748672
epoch 1 iter 2380: train loss 1.11259. lr 3.000000e-04, running loss 1.11608, it/sec: 4.509551219789293
epoch 1 iter 2400: train loss 1.09525. lr 3.000000e-04, running loss 1.11598, it/sec: 4.442080072754938
epoch 1 iter 2420: train loss 1.10791. lr 3.000000e-04, running loss 1.11593, it/sec: 4.515032523141214
epoch 1 iter 2440: train loss 1.10850. lr 3.000000e-04, running loss 1.11588, it/sec: 4.454085837939575
epoch 1 iter 2460: train loss 1.12068. lr 3.000000e-04, running loss 1.11899, it/sec: 4.452645882514824
epoch 1 iter 2480: train loss 1.12018. lr 3.000000e-04, running loss 1.11880, it/sec: 4.5029432416507005
epoch 1 iter 2500: train loss 1.11920. lr 3.000000e-04, running loss 1.11883, it/sec: 4.435696148137822
epoch 1 iter 2520: train loss 1.12968. lr 3.000000e-04, running loss 1.11878, it/sec: 4.4923998953196955
epoch 1 iter 2540: train loss 1.10395. lr 3.000000e-04, running loss 1.11867, it/sec: 4.504557381274284
epoch 1 iter 2560: train loss 1.08666. lr 3.000000e-04, running loss 1.11854, it/sec: 4.445223602914558
epoch 1 iter 2580: train loss 1.09140. lr 3.000000e-04, running loss 1.11829, it/sec: 4.517939902018891
epoch 1 iter 2600: train loss 1.10967. lr 3.000000e-04, running loss 1.11826, it/sec: 4.440815725703958
epoch 1 iter 2620: train loss 1.09506. lr 3.000000e-04, running loss 1.11828, it/sec: 4.507150871385486
epoch 1 iter 2640: train loss 1.11785. lr 3.000000e-04, running loss 1.11810, it/sec: 4.460723348357024
epoch 1 iter 2660: train loss 1.10655. lr 3.000000e-04, running loss 1.11795, it/sec: 4.481957593231482
epoch 1 iter 2680: train loss 1.11383. lr 3.000000e-04, running loss 1.11792, it/sec: 4.4737224318830755
epoch 1 iter 2700: train loss 1.12647. lr 3.000000e-04, running loss 1.11786, it/sec: 4.467368717186059
epoch 1 iter 2720: train loss 1.09584. lr 3.000000e-04, running loss 1.11779, it/sec: 4.512545983170721
epoch 1 iter 2740: train loss 1.12146. lr 3.000000e-04, running loss 1.11782, it/sec: 4.466682248625211
epoch 1 iter 2760: train loss 1.10842. lr 3.000000e-04, running loss 1.11767, it/sec: 4.489915595652849
epoch 1 iter 2780: train loss 1.11907. lr 3.000000e-04, running loss 1.11767, it/sec: 4.498428848657489
epoch 1 iter 2800: train loss 1.11318. lr 3.000000e-04, running loss 1.11751, it/sec: 4.493154592723883
epoch 1 iter 2820: train loss 1.11382. lr 3.000000e-04, running loss 1.11749, it/sec: 4.511689472665089
epoch 1 iter 2840: train loss 1.09596. lr 3.000000e-04, running loss 1.11730, it/sec: 4.48686381403765
epoch 1 iter 2860: train loss 1.10701. lr 3.000000e-04, running loss 1.11724, it/sec: 4.492192134481672
epoch 1 iter 2880: train loss 1.09857. lr 3.000000e-04, running loss 1.11722, it/sec: 4.499077320338121
epoch 1 iter 2900: train loss 1.11839. lr 3.000000e-04, running loss 1.11712, it/sec: 4.469129251768599
epoch 1 iter 2920: train loss 1.13558. lr 3.000000e-04, running loss 1.11713, it/sec: 4.498595539299887
epoch 1 iter 2940: train loss 1.09352. lr 3.000000e-04, running loss 1.11713, it/sec: 4.452364805506145
epoch 1 iter 2960: train loss 1.11818. lr 3.000000e-04, running loss 1.11706, it/sec: 4.501016156018273
epoch 1 iter 2980: train loss 1.10580. lr 3.000000e-04, running loss 1.11710, it/sec: 4.470663714987793
epoch 1 iter 3000: train loss 1.12149. lr 3.000000e-04, running loss 1.11698, it/sec: 4.489275547822745
epoch 1 iter 3020: train loss 1.08962. lr 3.000000e-04, running loss 1.11690, it/sec: 4.459526961430788
epoch 1 iter 3040: train loss 1.08469. lr 3.000000e-04, running loss 1.11683, it/sec: 4.49176763227756
epoch 1 iter 3060: train loss 1.13666. lr 3.000000e-04, running loss 1.11687, it/sec: 4.486258587416503
epoch 1 iter 3080: train loss 1.09648. lr 3.000000e-04, running loss 1.11680, it/sec: 4.5057536532183295
epoch 1 iter 3100: train loss 1.17323. lr 3.000000e-04, running loss 1.11678, it/sec: 4.455241772997917
epoch 1 iter 3120: train loss 1.10510. lr 3.000000e-04, running loss 1.11675, it/sec: 4.49752074584809
epoch 1 iter 3140: train loss 1.10847. lr 3.000000e-04, running loss 1.11658, it/sec: 4.445936796976119
epoch 1 iter 3160: train loss 1.10679. lr 3.000000e-04, running loss 1.11666, it/sec: 4.507872412844692
epoch 1 iter 3180: train loss 1.11558. lr 3.000000e-04, running loss 1.11654, it/sec: 4.454791519461762
epoch 1 iter 3200: train loss 1.09612. lr 3.000000e-04, running loss 1.11652, it/sec: 4.493164829232898
epoch 1 iter 3220: train loss 1.09679. lr 3.000000e-04, running loss 1.11644, it/sec: 4.474343759698012
epoch 1 iter 3240: train loss 1.12579. lr 3.000000e-04, running loss 1.11642, it/sec: 4.489641324752131
epoch 1 iter 3260: train loss 1.11298. lr 3.000000e-04, running loss 1.11632, it/sec: 4.508560990602657
epoch 1 iter 3280: train loss 1.10305. lr 3.000000e-04, running loss 1.11620, it/sec: 4.451064819478388
epoch 1 iter 3300: train loss 1.10349. lr 3.000000e-04, running loss 1.11616, it/sec: 4.512437491742101
epoch 1 iter 3320: train loss 1.09729. lr 3.000000e-04, running loss 1.11618, it/sec: 4.427947781325552
epoch 1 iter 3340: train loss 1.13032. lr 3.000000e-04, running loss 1.11615, it/sec: 4.509381868631768
epoch 1 iter 3360: train loss 1.10533. lr 3.000000e-04, running loss 1.11610, it/sec: 4.4677234481386945
epoch 1 iter 3380: train loss 1.10020. lr 3.000000e-04, running loss 1.11605, it/sec: 4.499734158239052
epoch 1 iter 3400: train loss 1.10782. lr 3.000000e-04, running loss 1.11646, it/sec: 4.477648315109801
epoch 1 iter 3420: train loss 1.11806. lr 3.000000e-04, running loss 1.11631, it/sec: 4.51592968526809
epoch 1 iter 3440: train loss 1.09059. lr 3.000000e-04, running loss 1.11616, it/sec: 4.483335466148108
epoch 1 iter 3460: train loss 1.10416. lr 3.000000e-04, running loss 1.11601, it/sec: 4.520015119802492
epoch 1 iter 3480: train loss 1.10361. lr 3.000000e-04, running loss 1.11598, it/sec: 4.450131871868091
epoch 1 iter 3500: train loss 1.12623. lr 3.000000e-04, running loss 1.11597, it/sec: 4.529634651382982
epoch 1 iter 3520: train loss 1.10494. lr 3.000000e-04, running loss 1.11671, it/sec: 4.452781356865557
epoch 1 iter 3540: train loss 1.09918. lr 3.000000e-04, running loss 1.11658, it/sec: 4.519067116926479
epoch 1 iter 3560: train loss 1.13604. lr 3.000000e-04, running loss 1.11659, it/sec: 4.466086983000635
epoch 1 iter 3580: train loss 1.11622. lr 3.000000e-04, running loss 1.11644, it/sec: 4.4874802573787935
epoch 1 iter 3600: train loss 1.09917. lr 3.000000e-04, running loss 1.11672, it/sec: 4.498667665172499
epoch 1 iter 3620: train loss 1.10061. lr 3.000000e-04, running loss 1.11673, it/sec: 4.445555458108824
epoch 1 iter 3640: train loss 1.09804. lr 3.000000e-04, running loss 1.11662, it/sec: 4.511045927926673
epoch 1 iter 3660: train loss 1.10948. lr 3.000000e-04, running loss 1.11664, it/sec: 4.477254281670289
epoch 1 iter 3680: train loss 1.11242. lr 3.000000e-04, running loss 1.11654, it/sec: 4.4485808740414585
epoch 1 iter 3700: train loss 1.11672. lr 3.000000e-04, running loss 1.11669, it/sec: 4.501319374921081
epoch 1 iter 3720: train loss 1.11049. lr 3.000000e-04, running loss 1.11664, it/sec: 4.500233234876596
epoch 1 iter 3740: train loss 1.11310. lr 3.000000e-04, running loss 1.11657, it/sec: 4.423869558162993
epoch 1 iter 3760: train loss 1.12315. lr 3.000000e-04, running loss 1.11646, it/sec: 4.515809773522913
epoch 1 iter 3780: train loss 1.10654. lr 3.000000e-04, running loss 1.11630, it/sec: 4.449414341015318
epoch 1 iter 3800: train loss 1.11112. lr 3.000000e-04, running loss 1.11621, it/sec: 4.499553232081485
epoch 1 iter 3820: train loss 1.15460. lr 3.000000e-04, running loss 1.11619, it/sec: 4.4601782678756825
epoch 1 iter 3840: train loss 1.10183. lr 3.000000e-04, running loss 1.11617, it/sec: 4.530548895499807
epoch 1 iter 3860: train loss 1.11393. lr 3.000000e-04, running loss 1.11616, it/sec: 4.461698327880013
epoch 1 iter 3880: train loss 1.10068. lr 3.000000e-04, running loss 1.11610, it/sec: 4.524234691920602
epoch 1 iter 3900: train loss 1.11121. lr 3.000000e-04, running loss 1.11614, it/sec: 4.472190631285513
epoch 1 iter 3920: train loss 1.10752. lr 3.000000e-04, running loss 1.11608, it/sec: 4.4700198917061575
epoch 1 iter 3940: train loss 1.09954. lr 3.000000e-04, running loss 1.11613, it/sec: 4.489853667663147
epoch 1 iter 3960: train loss 1.10119. lr 3.000000e-04, running loss 1.11603, it/sec: 4.4968145457876805
epoch 1 iter 3980: train loss 1.09978. lr 3.000000e-04, running loss 1.11600, it/sec: 4.46147034754786
epoch 1 iter 4000: train loss 1.11162. lr 3.000000e-04, running loss 1.11590, it/sec: 4.507866561378161
epoch 1 iter 4020: train loss 1.11313. lr 3.000000e-04, running loss 1.11593, it/sec: 4.436493813295323
epoch 1 iter 4040: train loss 1.10911. lr 3.000000e-04, running loss 1.11594, it/sec: 4.4404888966599705
epoch 1 iter 4060: train loss 1.11761. lr 3.000000e-04, running loss 1.11585, it/sec: 4.47298483035735
epoch 1 iter 4080: train loss 1.09870. lr 3.000000e-04, running loss 1.11577, it/sec: 4.44261229097862
epoch 1 iter 4100: train loss 1.11417. lr 3.000000e-04, running loss 1.11564, it/sec: 4.520792022763116
epoch 1 iter 4120: train loss 1.13299. lr 3.000000e-04, running loss 1.11563, it/sec: 4.460610211751053
epoch 1 iter 4140: train loss 1.11559. lr 3.000000e-04, running loss 1.11557, it/sec: 4.488249500257762
epoch 1 iter 4160: train loss 1.10492. lr 3.000000e-04, running loss 1.11577, it/sec: 4.475697122932352
epoch 1 iter 4180: train loss 1.11287. lr 3.000000e-04, running loss 1.11584, it/sec: 4.4927023174349605
epoch 1 iter 4200: train loss 1.12330. lr 3.000000e-04, running loss 1.11577, it/sec: 4.501968071554043
epoch 1 iter 4220: train loss 1.10718. lr 3.000000e-04, running loss 1.11577, it/sec: 4.442125734145697
epoch 1 iter 4240: train loss 1.10174. lr 3.000000e-04, running loss 1.11589, it/sec: 4.5032634702928975
epoch 1 iter 4260: train loss 1.11404. lr 3.000000e-04, running loss 1.11581, it/sec: 4.463233547277396
epoch 1 iter 4280: train loss 1.11729. lr 3.000000e-04, running loss 1.11585, it/sec: 4.494933555671429
epoch 1 iter 4300: train loss 1.11057. lr 3.000000e-04, running loss 1.11580, it/sec: 4.46261659587682
epoch 1 iter 4320: train loss 1.10977. lr 3.000000e-04, running loss 1.11582, it/sec: 4.501917828322494
epoch 1 iter 4340: train loss 1.10820. lr 3.000000e-04, running loss 1.11579, it/sec: 4.449618124532239
epoch 1 iter 4360: train loss 1.10181. lr 3.000000e-04, running loss 1.11576, it/sec: 4.503048781856701
epoch 1 iter 4380: train loss 1.11529. lr 3.000000e-04, running loss 1.11569, it/sec: 4.459318770220253
epoch 1 iter 4400: train loss 1.10361. lr 3.000000e-04, running loss 1.11566, it/sec: 4.508136133759771
epoch 1 iter 4420: train loss 1.12484. lr 3.000000e-04, running loss 1.11562, it/sec: 4.456775033889434
epoch 1 iter 4440: train loss 1.11672. lr 3.000000e-04, running loss 1.11573, it/sec: 4.452143962174625
epoch 1 iter 4460: train loss 1.12162. lr 3.000000e-04, running loss 1.11570, it/sec: 4.5236106054145075
epoch 1 iter 4480: train loss 1.11369. lr 3.000000e-04, running loss 1.11560, it/sec: 4.4410644985188155
epoch 1 iter 4500: train loss 1.12567. lr 3.000000e-04, running loss 1.11556, it/sec: 4.509685035528924
epoch 1 iter 4520: train loss 1.12978. lr 3.000000e-04, running loss 1.11547, it/sec: 4.468494437325683
epoch 1 iter 4540: train loss 1.11521. lr 3.000000e-04, running loss 1.11552, it/sec: 4.4922738435972525
epoch 1 iter 4560: train loss 1.09557. lr 3.000000e-04, running loss 1.11553, it/sec: 4.466193795794448
epoch 1 iter 4580: train loss 1.09683. lr 3.000000e-04, running loss 1.11552, it/sec: 4.4950077893295814
epoch 1 iter 4600: train loss 1.09280. lr 3.000000e-04, running loss 1.11542, it/sec: 4.431324396586721
epoch 1 iter 4620: train loss 1.12111. lr 3.000000e-04, running loss 1.11567, it/sec: 4.514654729190973
epoch 1 iter 4640: train loss 1.11968. lr 3.000000e-04, running loss 1.11551, it/sec: 4.426063787779843
epoch 1 iter 4660: train loss 1.12182. lr 3.000000e-04, running loss 1.11556, it/sec: 4.4892671426226896
epoch 1 iter 4680: train loss 1.11161. lr 3.000000e-04, running loss 1.11554, it/sec: 4.467242150734123
epoch 1 iter 4700: train loss 1.12731. lr 3.000000e-04, running loss 1.11567, it/sec: 4.472023372573505
epoch 1 iter 4720: train loss 1.10929. lr 3.000000e-04, running loss 1.11564, it/sec: 4.516196733698458
epoch 1 iter 4740: train loss 1.10438. lr 3.000000e-04, running loss 1.11563, it/sec: 4.47803229454595
epoch 1 iter 4760: train loss 1.10403. lr 3.000000e-04, running loss 1.11568, it/sec: 4.521261889243756
epoch 1 iter 4780: train loss 1.12821. lr 3.000000e-04, running loss 1.11565, it/sec: 4.427117121708285
epoch 1 iter 4800: train loss 1.09502. lr 3.000000e-04, running loss 1.11553, it/sec: 4.471936458303081
epoch 1 iter 4820: train loss 1.11190. lr 3.000000e-04, running loss 1.11542, it/sec: 4.4652728191848
epoch 1 iter 4840: train loss 1.12756. lr 3.000000e-04, running loss 1.11540, it/sec: 4.497187620947095
epoch 1 iter 4860: train loss 1.11040. lr 3.000000e-04, running loss 1.11534, it/sec: 4.459563494362615
epoch 1 iter 4880: train loss 1.12701. lr 3.000000e-04, running loss 1.11583, it/sec: 4.4925126531497135
epoch 1 iter 4900: train loss 1.12138. lr 3.000000e-04, running loss 1.11576, it/sec: 4.447196527518847
epoch 1 iter 4920: train loss 1.12523. lr 3.000000e-04, running loss 1.11575, it/sec: 4.532922601051152
epoch 1 iter 4940: train loss 1.10394. lr 3.000000e-04, running loss 1.11565, it/sec: 4.449767770653547
epoch 1 iter 4960: train loss 1.08838. lr 3.000000e-04, running loss 1.11560, it/sec: 4.4892135952574135
epoch 1 iter 4980: train loss 1.09336. lr 3.000000e-04, running loss 1.11560, it/sec: 4.4900723206662825
epoch 1 iter 5000: train loss 1.11165. lr 3.000000e-04, running loss 1.11558, it/sec: 4.441633008262242
epoch 1 iter 5020: train loss 1.11938. lr 3.000000e-04, running loss 1.11564, it/sec: 4.52618452077563
epoch 1 iter 5040: train loss 1.12587. lr 3.000000e-04, running loss 1.11554, it/sec: 4.476401914325708
epoch 1 iter 5060: train loss 1.10495. lr 3.000000e-04, running loss 1.11557, it/sec: 4.524754226818835
epoch 1 iter 5080: train loss 1.11455. lr 3.000000e-04, running loss 1.11560, it/sec: 4.46598767390806
epoch 1 iter 5100: train loss 1.12201. lr 3.000000e-04, running loss 1.11557, it/sec: 4.490449398768447
epoch 1 iter 5120: train loss 1.12811. lr 3.000000e-04, running loss 1.11560, it/sec: 4.43212337716641
epoch 1 iter 5140: train loss 1.10672. lr 3.000000e-04, running loss 1.11550, it/sec: 4.492770964676562
epoch 1 iter 5160: train loss 1.09672. lr 3.000000e-04, running loss 1.11539, it/sec: 4.504033751059305
epoch 1 iter 5180: train loss 1.10784. lr 3.000000e-04, running loss 1.11539, it/sec: 4.5050372352121295
epoch 1 iter 5200: train loss 1.10272. lr 3.000000e-04, running loss 1.11532, it/sec: 4.428504229871753
epoch 1 iter 5220: train loss 1.12406. lr 3.000000e-04, running loss 1.11523, it/sec: 4.529850076467062
epoch 1 iter 5240: train loss 1.12084. lr 3.000000e-04, running loss 1.11520, it/sec: 4.459243563586807
epoch 1 iter 5260: train loss 1.10631. lr 3.000000e-04, running loss 1.11502, it/sec: 4.478974170075484
epoch 1 iter 5280: train loss 1.09492. lr 3.000000e-04, running loss 1.11503, it/sec: 4.509809564787602
epoch 1 iter 5300: train loss 1.12169. lr 3.000000e-04, running loss 1.11497, it/sec: 4.452645465910644
epoch 1 iter 5320: train loss 1.13516. lr 3.000000e-04, running loss 1.11508, it/sec: 4.507404429849884
epoch 1 iter 5340: train loss 1.13381. lr 3.000000e-04, running loss 1.11500, it/sec: 4.462246766749553
epoch 1 iter 5360: train loss 1.12214. lr 3.000000e-04, running loss 1.11495, it/sec: 4.506222552267796
epoch 1 iter 5380: train loss 1.10902. lr 3.000000e-04, running loss 1.11487, it/sec: 4.4740812160375745
epoch 1 iter 5400: train loss 1.11429. lr 3.000000e-04, running loss 1.11481, it/sec: 4.479690449551822
epoch 1 iter 5420: train loss 1.10736. lr 3.000000e-04, running loss 1.11474, it/sec: 4.48358661299139
epoch 1 iter 5440: train loss 1.10752. lr 3.000000e-04, running loss 1.11472, it/sec: 4.496817458956053
epoch 1 iter 5460: train loss 1.10114. lr 3.000000e-04, running loss 1.11469, it/sec: 4.4821390342089975
epoch 1 iter 5480: train loss 1.09704. lr 3.000000e-04, running loss 1.11460, it/sec: 4.498805996929442
epoch 1 iter 5500: train loss 1.09369. lr 3.000000e-04, running loss 1.11462, it/sec: 4.460083617874188
epoch 1 iter 5520: train loss 1.10519. lr 3.000000e-04, running loss 1.11451, it/sec: 4.512249740197692
epoch 1 iter 5540: train loss 1.10864. lr 3.000000e-04, running loss 1.11453, it/sec: 4.432322495800834
epoch 1 iter 5560: train loss 1.08993. lr 3.000000e-04, running loss 1.11443, it/sec: 4.485105412540853
epoch 1 iter 5580: train loss 1.12926. lr 3.000000e-04, running loss 1.11443, it/sec: 4.499712290055586
epoch 1 iter 5600: train loss 1.11389. lr 3.000000e-04, running loss 1.11447, it/sec: 4.426042122367466
epoch 1 iter 5620: train loss 1.09249. lr 3.000000e-04, running loss 1.11451, it/sec: 4.509215333992429
epoch 1 iter 5640: train loss 1.11107. lr 3.000000e-04, running loss 1.11448, it/sec: 4.4577579057939465
epoch 1 iter 5660: train loss 1.11771. lr 3.000000e-04, running loss 1.11454, it/sec: 4.505846881134356
epoch 1 iter 5680: train loss 1.11988. lr 3.000000e-04, running loss 1.11457, it/sec: 4.422552397853755
epoch 1 iter 5700: train loss 1.11279. lr 3.000000e-04, running loss 1.11458, it/sec: 4.5128785576556725
epoch 1 iter 5720: train loss 1.08925. lr 3.000000e-04, running loss 1.11457, it/sec: 4.456791678170178
epoch 1 iter 5740: train loss 1.10820. lr 3.000000e-04, running loss 1.11444, it/sec: 4.505815859882204
epoch 1 iter 5760: train loss 1.09826. lr 3.000000e-04, running loss 1.11445, it/sec: 4.501516125712861
epoch 1 iter 5780: train loss 1.10994. lr 3.000000e-04, running loss 1.11445, it/sec: 4.496495396034845
epoch 1 iter 5800: train loss 1.08842. lr 3.000000e-04, running loss 1.11448, it/sec: 4.46913826068411
epoch 1 iter 5820: train loss 1.10346. lr 3.000000e-04, running loss 1.11443, it/sec: 4.482066271535062
epoch 1 iter 5840: train loss 1.09575. lr 3.000000e-04, running loss 1.11439, it/sec: 4.4728667295466265
epoch 1 iter 5860: train loss 1.13807. lr 3.000000e-04, running loss 1.11442, it/sec: 4.4936967182942915
epoch 1 iter 5880: train loss 1.12295. lr 3.000000e-04, running loss 1.11438, it/sec: 4.513856687595234
epoch 1 iter 5900: train loss 1.11919. lr 3.000000e-04, running loss 1.11446, it/sec: 4.464305225835073
epoch 1 iter 5920: train loss 1.10637. lr 3.000000e-04, running loss 1.11446, it/sec: 4.525439371227568
epoch 1 iter 5940: train loss 1.11825. lr 3.000000e-04, running loss 1.11445, it/sec: 4.469086910700978
epoch 1 iter 5960: train loss 1.13255. lr 3.000000e-04, running loss 1.11491, it/sec: 4.510943469918435
epoch 1 iter 5980: train loss 1.11702. lr 3.000000e-04, running loss 1.11487, it/sec: 4.45460914983619
epoch 1 iter 6000: train loss 1.09640. lr 3.000000e-04, running loss 1.11478, it/sec: 4.493747928938289
epoch 1 iter 6020: train loss 1.10574. lr 3.000000e-04, running loss 1.11471, it/sec: 4.445201472331902
epoch 1 iter 6040: train loss 1.12365. lr 3.000000e-04, running loss 1.11466, it/sec: 4.458535755901491
epoch 1 iter 6060: train loss 1.11725. lr 3.000000e-04, running loss 1.11462, it/sec: 4.50131064268697
epoch 1 iter 6080: train loss 1.12613. lr 3.000000e-04, running loss 1.11453, it/sec: 4.491858607562577
epoch 1 iter 6100: train loss 1.10566. lr 3.000000e-04, running loss 1.11451, it/sec: 4.4494357230433055
epoch 1 iter 6120: train loss 1.12014. lr 3.000000e-04, running loss 1.11455, it/sec: 4.492852676051117
epoch 1 iter 6140: train loss 1.11560. lr 3.000000e-04, running loss 1.11451, it/sec: 4.521900295786436
epoch 1 iter 6160: train loss 1.12374. lr 3.000000e-04, running loss 1.11446, it/sec: 4.49783994108233
epoch 1 iter 6180: train loss 1.09683. lr 3.000000e-04, running loss 1.11442, it/sec: 4.514109860102834
epoch 1 iter 6200: train loss 1.09965. lr 3.000000e-04, running loss 1.11439, it/sec: 4.486636173486156
epoch 1 iter 6220: train loss 1.10397. lr 3.000000e-04, running loss 1.11439, it/sec: 4.502542897163385
epoch 1 iter 6240: train loss 1.13226. lr 3.000000e-04, running loss 1.11448, it/sec: 4.424862030244616
epoch 1 iter 6260: train loss 1.10016. lr 3.000000e-04, running loss 1.11449, it/sec: 4.4897929698935535
epoch 1 iter 6280: train loss 1.12483. lr 3.000000e-04, running loss 1.11457, it/sec: 4.4594733849702335
epoch 1 iter 6300: train loss 1.10899. lr 3.000000e-04, running loss 1.11461, it/sec: 4.511640172142947
epoch 1 iter 6320: train loss 1.11319. lr 3.000000e-04, running loss 1.11465, it/sec: 4.435304326648487
epoch 1 iter 6340: train loss 1.11733. lr 3.000000e-04, running loss 1.11474, it/sec: 4.512211708475268
epoch 1 iter 6360: train loss 1.10756. lr 3.000000e-04, running loss 1.11462, it/sec: 4.48013328572075
epoch 1 iter 6380: train loss 1.11857. lr 3.000000e-04, running loss 1.11474, it/sec: 4.50751422392692
epoch 1 iter 6400: train loss 1.10736. lr 3.000000e-04, running loss 1.11463, it/sec: 4.456315217160471
epoch 1 iter 6420: train loss 1.11772. lr 3.000000e-04, running loss 1.11480, it/sec: 4.492812969673824
epoch 1 iter 6440: train loss 1.11857. lr 3.000000e-04, running loss 1.11467, it/sec: 4.468205187437007
epoch 1 iter 6460: train loss 1.10384. lr 3.000000e-04, running loss 1.11464, it/sec: 4.513987681193354
epoch 1 iter 6480: train loss 1.11125. lr 3.000000e-04, running loss 1.11451, it/sec: 4.465986516436755
epoch 1 iter 6500: train loss 1.11911. lr 3.000000e-04, running loss 1.11452, it/sec: 4.4729367132948346
epoch 1 iter 6520: train loss 1.11428. lr 3.000000e-04, running loss 1.11451, it/sec: 4.490806755313675
epoch 1 iter 6540: train loss 1.11759. lr 3.000000e-04, running loss 1.11454, it/sec: 4.475146175029289
epoch 1 iter 6560: train loss 1.09674. lr 3.000000e-04, running loss 1.11484, it/sec: 4.503493226260644
epoch 1 iter 6580: train loss 1.11368. lr 3.000000e-04, running loss 1.11480, it/sec: 4.442856962267265
epoch 1 iter 6600: train loss 1.13123. lr 3.000000e-04, running loss 1.11475, it/sec: 4.511834365090997
epoch 1 iter 6620: train loss 1.11907. lr 3.000000e-04, running loss 1.11480, it/sec: 4.4358849406714045
epoch 1 iter 6640: train loss 1.13295. lr 3.000000e-04, running loss 1.11492, it/sec: 4.494934121352741
epoch 1 iter 6660: train loss 1.10654. lr 3.000000e-04, running loss 1.11484, it/sec: 4.427532118166242
epoch 1 iter 6680: train loss 1.10352. lr 3.000000e-04, running loss 1.11479, it/sec: 4.507336837722582
epoch 1 iter 6700: train loss 1.10801. lr 3.000000e-04, running loss 1.11477, it/sec: 4.485169463689811
epoch 1 iter 6720: train loss 1.13226. lr 3.000000e-04, running loss 1.11472, it/sec: 4.501270301950671
epoch 1 iter 6740: train loss 1.10542. lr 3.000000e-04, running loss 1.11472, it/sec: 4.440913168839561
epoch 1 iter 6760: train loss 1.10603. lr 3.000000e-04, running loss 1.11461, it/sec: 4.505389812911194
epoch 1 iter 6780: train loss 1.10077. lr 3.000000e-04, running loss 1.11453, it/sec: 4.476394942911736
epoch 1 iter 6800: train loss 1.09673. lr 3.000000e-04, running loss 1.11451, it/sec: 4.4268895052347315
epoch 1 iter 6820: train loss 1.12205. lr 3.000000e-04, running loss 1.11459, it/sec: 4.496323261865138
epoch 1 iter 6840: train loss 1.10552. lr 3.000000e-04, running loss 1.11442, it/sec: 4.46662642604267
epoch 1 iter 6860: train loss 1.09923. lr 3.000000e-04, running loss 1.11439, it/sec: 4.511367575464102
epoch 1 iter 6880: train loss 1.10287. lr 3.000000e-04, running loss 1.11439, it/sec: 4.427629763146869
epoch 1 iter 6900: train loss 1.11026. lr 3.000000e-04, running loss 1.11437, it/sec: 4.515702734599921
epoch 1 iter 6920: train loss 1.13223. lr 3.000000e-04, running loss 1.11426, it/sec: 4.485407981054863
epoch 1 iter 6940: train loss 1.12171. lr 3.000000e-04, running loss 1.11415, it/sec: 4.4835782103585355
epoch 1 iter 6960: train loss 1.11533. lr 3.000000e-04, running loss 1.11407, it/sec: 4.456986861177703
epoch 1 iter 6980: train loss 1.10639. lr 3.000000e-04, running loss 1.11403, it/sec: 4.517037635843461
epoch 1 iter 7000: train loss 1.10910. lr 3.000000e-04, running loss 1.11403, it/sec: 4.4580093347123935
epoch 1 iter 7020: train loss 1.11864. lr 3.000000e-04, running loss 1.11401, it/sec: 4.5123538255546505
epoch 1 iter 7040: train loss 1.10059. lr 3.000000e-04, running loss 1.11400, it/sec: 4.461191021400074
epoch 1 iter 7060: train loss 1.10213. lr 3.000000e-04, running loss 1.11412, it/sec: 4.479718465187996
epoch 1 iter 7080: train loss 1.10891. lr 3.000000e-04, running loss 1.11404, it/sec: 4.500282590560174
epoch 1 iter 7100: train loss 1.11763. lr 3.000000e-04, running loss 1.11404, it/sec: 4.441734315960999
epoch 1 iter 7120: train loss 1.09620. lr 3.000000e-04, running loss 1.11404, it/sec: 4.524456911442495
epoch 1 iter 7140: train loss 1.10655. lr 3.000000e-04, running loss 1.11400, it/sec: 4.420506556918345
epoch 1 iter 7160: train loss 1.10016. lr 3.000000e-04, running loss 1.11438, it/sec: 4.5133267978865375
epoch 1 iter 7180: train loss 1.11235. lr 3.000000e-04, running loss 1.11440, it/sec: 4.43403372703699
epoch 1 iter 7200: train loss 1.09684. lr 3.000000e-04, running loss 1.11441, it/sec: 4.4939444439264395
epoch 1 iter 7220: train loss 1.10629. lr 3.000000e-04, running loss 1.11440, it/sec: 4.489223128746382
epoch 1 iter 7240: train loss 1.12412. lr 3.000000e-04, running loss 1.11435, it/sec: 4.459162555077676
epoch 1 iter 7260: train loss 1.12498. lr 3.000000e-04, running loss 1.11443, it/sec: 4.489732998884565
epoch 1 iter 7280: train loss 1.10046. lr 3.000000e-04, running loss 1.11446, it/sec: 4.460158673367613
epoch 1 iter 7300: train loss 1.13295. lr 3.000000e-04, running loss 1.11444, it/sec: 4.483410019231316
epoch 1 iter 7320: train loss 1.09118. lr 3.000000e-04, running loss 1.11439, it/sec: 4.494511464211916
epoch 1 iter 7340: train loss 1.11351. lr 3.000000e-04, running loss 1.11457, it/sec: 4.518474140093889
epoch 1 iter 7360: train loss 1.12155. lr 3.000000e-04, running loss 1.11475, it/sec: 4.440909145262351
epoch 1 iter 7380: train loss 1.13867. lr 3.000000e-04, running loss 1.11466, it/sec: 4.488190276335084
epoch 1 iter 7400: train loss 1.09806. lr 3.000000e-04, running loss 1.11462, it/sec: 4.463686026270035
epoch 1 iter 7420: train loss 1.11438. lr 3.000000e-04, running loss 1.11449, it/sec: 4.5094374831278525
epoch 1 iter 7440: train loss 1.11076. lr 3.000000e-04, running loss 1.11442, it/sec: 4.459262692858075
epoch 1 iter 7460: train loss 1.10648. lr 3.000000e-04, running loss 1.11436, it/sec: 4.47116673978442
epoch 1 iter 7480: train loss 1.13477. lr 3.000000e-04, running loss 1.11483, it/sec: 4.484375274912341
epoch 1 iter 7500: train loss 1.38907. lr 3.000000e-04, running loss 1.11646, it/sec: 4.426243338354826
epoch 1 iter 7520: train loss 1.10084. lr 3.000000e-04, running loss 1.11648, it/sec: 4.518958944076568
epoch 1 iter 7540: train loss 1.11853. lr 3.000000e-04, running loss 1.11634, it/sec: 4.426719270193157
epoch 1 iter 7560: train loss 1.10755. lr 3.000000e-04, running loss 1.11627, it/sec: 4.495822523698244
epoch 1 iter 7580: train loss 1.10623. lr 3.000000e-04, running loss 1.11620, it/sec: 4.460016721960548
epoch 1 iter 7600: train loss 1.12231. lr 3.000000e-04, running loss 1.11615, it/sec: 4.515693559528603
epoch 1 iter 7620: train loss 1.10771. lr 3.000000e-04, running loss 1.11600, it/sec: 4.461000863337476
epoch 1 iter 7640: train loss 1.11191. lr 3.000000e-04, running loss 1.11597, it/sec: 4.518242342091157
epoch 1 iter 7660: train loss 1.13786. lr 3.000000e-04, running loss 1.11594, it/sec: 4.4612601235692715
epoch 1 iter 7680: train loss 1.09357. lr 3.000000e-04, running loss 1.11586, it/sec: 4.490623118846204
epoch 1 iter 7700: train loss 1.11338. lr 3.000000e-04, running loss 1.11585, it/sec: 4.450840598529269
epoch 1 iter 7720: train loss 1.12284. lr 3.000000e-04, running loss 1.11586, it/sec: 4.5099474626286895
epoch 1 iter 7740: train loss 1.10685. lr 3.000000e-04, running loss 1.11632, it/sec: 4.4782365612391635
epoch 1 iter 7760: train loss 1.11275. lr 3.000000e-04, running loss 1.11623, it/sec: 4.445392992385053
epoch 1 iter 7780: train loss 1.11640. lr 3.000000e-04, running loss 1.11612, it/sec: 4.4676157442592705
epoch 1 iter 7800: train loss 1.14490. lr 3.000000e-04, running loss 1.11602, it/sec: 4.4762733146523255
epoch 1 iter 7820: train loss 1.10177. lr 3.000000e-04, running loss 1.11608, it/sec: 4.51001198020038
epoch 1 iter 7840: train loss 1.11898. lr 3.000000e-04, running loss 1.11683, it/sec: 4.431352436905025
epoch 1 iter 7860: train loss 1.10799. lr 3.000000e-04, running loss 1.11695, it/sec: 4.499915705957536
epoch 1 iter 7880: train loss 1.09156. lr 3.000000e-04, running loss 1.11681, it/sec: 4.426855132308089
epoch 1 iter 7900: train loss 1.11382. lr 3.000000e-04, running loss 1.11679, it/sec: 4.509668989390065
epoch 1 iter 7920: train loss 1.09416. lr 3.000000e-04, running loss 1.11668, it/sec: 4.442062827224235
epoch 1 iter 7940: train loss 1.08869. lr 3.000000e-04, running loss 1.11664, it/sec: 4.516807800926573
epoch 1 iter 7960: train loss 1.12289. lr 3.000000e-04, running loss 1.11669, it/sec: 4.477182698440143
epoch 1 iter 7980: train loss 1.11896. lr 3.000000e-04, running loss 1.11657, it/sec: 4.457152340179149
epoch 1 iter 8000: train loss 1.11577. lr 3.000000e-04, running loss 1.11669, it/sec: 4.512054127379048
epoch 1 iter 8020: train loss 1.10054. lr 3.000000e-04, running loss 1.11660, it/sec: 4.430640008310603
epoch 1 iter 8040: train loss 1.10674. lr 3.000000e-04, running loss 1.11652, it/sec: 4.492827645935753
epoch 1 iter 8060: train loss 1.10951. lr 3.000000e-04, running loss 1.11651, it/sec: 4.389795047207863
epoch 1 iter 8080: train loss 1.10913. lr 3.000000e-04, running loss 1.11651, it/sec: 4.480892562202001
epoch 1 iter 8100: train loss 1.10615. lr 3.000000e-04, running loss 1.11653, it/sec: 4.455982607782544
epoch 1 iter 8120: train loss 1.12290. lr 3.000000e-04, running loss 1.11648, it/sec: 4.502828741669766
epoch 1 iter 8140: train loss 1.12344. lr 3.000000e-04, running loss 1.11639, it/sec: 4.49397869574934
epoch 1 iter 8160: train loss 1.11438. lr 3.000000e-04, running loss 1.11631, it/sec: 4.48572298439785
epoch 1 iter 8180: train loss 1.10523. lr 3.000000e-04, running loss 1.11621, it/sec: 4.503373853641106
epoch 1 iter 8200: train loss 1.07989. lr 3.000000e-04, running loss 1.11610, it/sec: 4.4888945957103195
epoch 1 iter 8220: train loss 1.12751. lr 3.000000e-04, running loss 1.11601, it/sec: 4.464779272058347
epoch 1 iter 8240: train loss 1.10645. lr 3.000000e-04, running loss 1.11612, it/sec: 4.489028175952376
epoch 1 iter 8260: train loss 1.12057. lr 3.000000e-04, running loss 1.11607, it/sec: 4.480294727735428
epoch 1 iter 8280: train loss 1.14314. lr 3.000000e-04, running loss 1.11602, it/sec: 4.459740900418477
epoch 1 iter 8300: train loss 1.09635. lr 3.000000e-04, running loss 1.11595, it/sec: 4.503080172229461
epoch 1 iter 8320: train loss 1.11500. lr 3.000000e-04, running loss 1.11593, it/sec: 4.453153486641073
epoch 1 iter 8340: train loss 1.12273. lr 3.000000e-04, running loss 1.11590, it/sec: 4.530299724638955
epoch 1 iter 8360: train loss 1.12729. lr 3.000000e-04, running loss 1.11618, it/sec: 4.381549156620806
epoch 1 iter 8380: train loss 1.09645. lr 3.000000e-04, running loss 1.11611, it/sec: 4.479455429405931
epoch 1 iter 8400: train loss 1.09511. lr 3.000000e-04, running loss 1.11599, it/sec: 4.502759036154989
epoch 1 iter 8420: train loss 1.13596. lr 3.000000e-04, running loss 1.11582, it/sec: 4.45479987461069
epoch 1 iter 8440: train loss 1.09580. lr 3.000000e-04, running loss 1.11578, it/sec: 4.5071315738150854
epoch 1 iter 8460: train loss 1.11158. lr 3.000000e-04, running loss 1.11574, it/sec: 4.4703910314142785
epoch 1 iter 8480: train loss 1.08945. lr 3.000000e-04, running loss 1.11565, it/sec: 4.504915973364587
epoch 1 iter 8500: train loss 1.09163. lr 3.000000e-04, running loss 1.11566, it/sec: 4.503033879402403
epoch 1 iter 8520: train loss 1.11260. lr 3.000000e-04, running loss 1.11555, it/sec: 4.4567538992080715
epoch 1 iter 8540: train loss 1.12453. lr 3.000000e-04, running loss 1.11561, it/sec: 4.501318463247838
epoch 1 iter 8560: train loss 1.13345. lr 3.000000e-04, running loss 1.11553, it/sec: 4.445851646162354
epoch 1 iter 8580: train loss 1.09822. lr 3.000000e-04, running loss 1.11562, it/sec: 4.461648541478362
epoch 1 iter 8600: train loss 1.09588. lr 3.000000e-04, running loss 1.11553, it/sec: 4.494103064455818
epoch 1 iter 8620: train loss 1.09790. lr 3.000000e-04, running loss 1.11532, it/sec: 4.482411968198714
epoch 1 iter 8640: train loss 1.09401. lr 3.000000e-04, running loss 1.11539, it/sec: 4.51093789355804
epoch 1 iter 8660: train loss 1.14018. lr 3.000000e-04, running loss 1.11545, it/sec: 4.425636276034889
epoch 1 iter 8680: train loss 1.10546. lr 3.000000e-04, running loss 1.11526, it/sec: 4.515630999117644
epoch 1 iter 8700: train loss 1.11588. lr 3.000000e-04, running loss 1.11525, it/sec: 4.455357277605311
epoch 1 iter 8720: train loss 1.10246. lr 3.000000e-04, running loss 1.11522, it/sec: 4.4802313169700385
epoch 1 iter 8740: train loss 1.13131. lr 3.000000e-04, running loss 1.11516, it/sec: 4.45500556000467
epoch 1 iter 8760: train loss 1.10876. lr 3.000000e-04, running loss 1.11525, it/sec: 4.4846177091284725
epoch 1 iter 8780: train loss 1.09211. lr 3.000000e-04, running loss 1.11538, it/sec: 4.45469477736727
epoch 1 iter 8800: train loss 1.11226. lr 3.000000e-04, running loss 1.11532, it/sec: 4.498320692224978
epoch 1 iter 8820: train loss 1.13410. lr 3.000000e-04, running loss 1.11538, it/sec: 4.4364023516848645
epoch 1 iter 8840: train loss 1.09922. lr 3.000000e-04, running loss 1.11533, it/sec: 4.448229433081897
epoch 1 iter 8860: train loss 1.08906. lr 3.000000e-04, running loss 1.11546, it/sec: 4.46828453058436
epoch 1 iter 8880: train loss 1.12723. lr 3.000000e-04, running loss 1.11550, it/sec: 4.501457524221679
epoch 1 iter 8900: train loss 1.08253. lr 3.000000e-04, running loss 1.11569, it/sec: 4.520893558142139
epoch 1 iter 8920: train loss 1.11634. lr 3.000000e-04, running loss 1.11566, it/sec: 4.44501115335659
epoch 1 iter 8940: train loss 1.09596. lr 3.000000e-04, running loss 1.11565, it/sec: 4.5256891128825965
epoch 1 iter 8960: train loss 1.18537. lr 3.000000e-04, running loss 1.11563, it/sec: 4.463427659871826
epoch 1 iter 8980: train loss 1.10027. lr 3.000000e-04, running loss 1.11575, it/sec: 4.502855526184156
epoch 1 iter 9000: train loss 1.12549. lr 3.000000e-04, running loss 1.11598, it/sec: 4.428879472594931
epoch 1 iter 9020: train loss 1.10656. lr 3.000000e-04, running loss 1.11597, it/sec: 4.495802958294054
epoch 1 iter 9040: train loss 1.12164. lr 3.000000e-04, running loss 1.11595, it/sec: 4.503355905797294
epoch 1 iter 9060: train loss 1.11907. lr 3.000000e-04, running loss 1.11584, it/sec: 4.478628822485984
epoch 1 iter 9080: train loss 1.12939. lr 3.000000e-04, running loss 1.11589, it/sec: 4.482001626117531
epoch 1 iter 9100: train loss 1.12127. lr 3.000000e-04, running loss 1.11583, it/sec: 4.531437470472353
epoch 1 iter 9120: train loss 1.11758. lr 3.000000e-04, running loss 1.11573, it/sec: 4.405799609706423
epoch 1 iter 9140: train loss 1.11757. lr 3.000000e-04, running loss 1.11557, it/sec: 4.491341575844633
epoch 1 iter 9160: train loss 1.11321. lr 3.000000e-04, running loss 1.11555, it/sec: 4.48417911369443
epoch 1 iter 9180: train loss 1.11862. lr 3.000000e-04, running loss 1.11550, it/sec: 4.487735232108324
epoch 1 iter 9200: train loss 1.10734. lr 3.000000e-04, running loss 1.11554, it/sec: 4.500699102608702
epoch 1 iter 9220: train loss 1.12372. lr 3.000000e-04, running loss 1.11557, it/sec: 4.469305841969727
epoch 1 iter 9240: train loss 1.13128. lr 3.000000e-04, running loss 1.11547, it/sec: 4.491957414532639
epoch 1 iter 9260: train loss 1.10673. lr 3.000000e-04, running loss 1.11555, it/sec: 4.422284260644436
epoch 1 iter 9280: train loss 1.12167. lr 3.000000e-04, running loss 1.11568, it/sec: 4.495680961310646
epoch 1 iter 9300: train loss 1.10116. lr 3.000000e-04, running loss 1.11553, it/sec: 4.453127608731614
epoch 1 iter 9320: train loss 1.13882. lr 3.000000e-04, running loss 1.11553, it/sec: 4.523550607796431
epoch 1 iter 9340: train loss 1.10614. lr 3.000000e-04, running loss 1.11544, it/sec: 4.434931850373143
epoch 1 iter 9360: train loss 1.13131. lr 3.000000e-04, running loss 1.11545, it/sec: 4.491824346944432
epoch 1 iter 9380: train loss 1.09497. lr 3.000000e-04, running loss 1.11538, it/sec: 4.450318369924043
epoch 1 iter 9400: train loss 1.11558. lr 3.000000e-04, running loss 1.11540, it/sec: 4.520496206774729
epoch 1 iter 9420: train loss 1.10891. lr 3.000000e-04, running loss 1.11531, it/sec: 4.479622903453016
epoch 1 iter 9440: train loss 1.08678. lr 3.000000e-04, running loss 1.11522, it/sec: 4.529958525308773
epoch 1 iter 9460: train loss 1.09968. lr 3.000000e-04, running loss 1.11516, it/sec: 4.436777928101561
epoch 1 iter 9480: train loss 1.11037. lr 3.000000e-04, running loss 1.11508, it/sec: 4.525924522884495
epoch 1 iter 9500: train loss 1.10821. lr 3.000000e-04, running loss 1.11543, it/sec: 4.4338571232818795
epoch 1 iter 9520: train loss 1.10842. lr 3.000000e-04, running loss 1.11531, it/sec: 4.518690751190153
epoch 1 iter 9540: train loss 1.12596. lr 3.000000e-04, running loss 1.11538, it/sec: 4.466006282924782
epoch 1 iter 9560: train loss 1.16829. lr 3.000000e-04, running loss 1.11527, it/sec: 4.510203857894149
epoch 1 iter 9580: train loss 1.09369. lr 3.000000e-04, running loss 1.11517, it/sec: 4.484543799518821
epoch 1 iter 9600: train loss 1.09680. lr 3.000000e-04, running loss 1.11545, it/sec: 4.47166515930276
epoch 1 iter 9620: train loss 1.09561. lr 3.000000e-04, running loss 1.11542, it/sec: 4.51575122659869
epoch 1 iter 9640: train loss 1.08162. lr 3.000000e-04, running loss 1.11532, it/sec: 4.459454631255927
epoch 1 iter 9660: train loss 1.10141. lr 3.000000e-04, running loss 1.11527, it/sec: 4.510443417124237
epoch 1 iter 9680: train loss 1.12339. lr 3.000000e-04, running loss 1.11518, it/sec: 4.446903344060747
epoch 1 iter 9700: train loss 1.11068. lr 3.000000e-04, running loss 1.11508, it/sec: 4.473626146084302
epoch 1 iter 9720: train loss 1.09679. lr 3.000000e-04, running loss 1.11495, it/sec: 4.507203689869969
epoch 1 iter 9740: train loss 1.11009. lr 3.000000e-04, running loss 1.11498, it/sec: 4.500251523277979
epoch 1 iter 9760: train loss 1.11409. lr 3.000000e-04, running loss 1.11499, it/sec: 4.481116970531775
epoch 1 iter 9780: train loss 1.11591. lr 3.000000e-04, running loss 1.11498, it/sec: 4.482789549185841
epoch 1 iter 9800: train loss 1.11083. lr 3.000000e-04, running loss 1.11487, it/sec: 4.4843422953584335
epoch 1 iter 9820: train loss 1.09824. lr 3.000000e-04, running loss 1.11486, it/sec: 4.510263522115383
epoch 1 iter 9840: train loss 1.10741. lr 3.000000e-04, running loss 1.11479, it/sec: 4.456032049371128
epoch 1 iter 9860: train loss 1.09821. lr 3.000000e-04, running loss 1.11480, it/sec: 4.484156130046279
epoch 1 iter 9880: train loss 1.12556. lr 3.000000e-04, running loss 1.11466, it/sec: 4.431794331375367
epoch 1 iter 9900: train loss 1.11817. lr 3.000000e-04, running loss 1.11456, it/sec: 4.502334075584923
epoch 1 iter 9920: train loss 1.11914. lr 3.000000e-04, running loss 1.11450, it/sec: 4.456088122947944
epoch 1 iter 9940: train loss 1.10530. lr 3.000000e-04, running loss 1.11439, it/sec: 4.50306218659592
epoch 1 iter 9960: train loss 1.12654. lr 3.000000e-04, running loss 1.11443, it/sec: 4.483670281811166
epoch 1 iter 9980: train loss 1.09199. lr 3.000000e-04, running loss 1.11434, it/sec: 4.502959278910992
epoch 1 iter 10000: train loss 1.10569. lr 3.000000e-04, running loss 1.11430, it/sec: 4.435538041602418
epoch 1 iter 10020: train loss 1.10704. lr 3.000000e-04, running loss 1.11433, it/sec: 4.501715327226736
epoch 1 iter 10040: train loss 1.10606. lr 3.000000e-04, running loss 1.11432, it/sec: 4.430363115908709
epoch 1 iter 10060: train loss 1.10509. lr 3.000000e-04, running loss 1.11434, it/sec: 4.523145016479469
epoch 1 iter 10080: train loss 1.09254. lr 3.000000e-04, running loss 1.11423, it/sec: 4.466009174888337
epoch 1 iter 10100: train loss 1.09978. lr 3.000000e-04, running loss 1.11410, it/sec: 4.471756221836978
epoch 1 iter 10120: train loss 1.11704. lr 3.000000e-04, running loss 1.11412, it/sec: 4.467813632144517
epoch 1 iter 10140: train loss 1.11849. lr 3.000000e-04, running loss 1.11477, it/sec: 4.486107321500978
epoch 1 iter 10160: train loss 1.09399. lr 3.000000e-04, running loss 1.11474, it/sec: 4.50558111552837
epoch 1 iter 10180: train loss 1.10130. lr 3.000000e-04, running loss 1.11484, it/sec: 4.461538899904877
epoch 1 iter 10200: train loss 1.14488. lr 3.000000e-04, running loss 1.11484, it/sec: 4.497346410465669
epoch 1 iter 10220: train loss 1.13964. lr 3.000000e-04, running loss 1.11481, it/sec: 4.453479445464557
epoch 1 iter 10240: train loss 1.12501. lr 3.000000e-04, running loss 1.11489, it/sec: 4.515087054884243
epoch 1 iter 10260: train loss 1.09625. lr 3.000000e-04, running loss 1.11485, it/sec: 4.445964786660786
epoch 1 iter 10280: train loss 1.09355. lr 3.000000e-04, running loss 1.11490, it/sec: 4.492285083957584
epoch 1 iter 10300: train loss 1.11777. lr 3.000000e-04, running loss 1.11487, it/sec: 4.465140329818542
epoch 1 iter 10320: train loss 1.12169. lr 3.000000e-04, running loss 1.11493, it/sec: 4.500524966396016
epoch 1 iter 10340: train loss 1.09452. lr 3.000000e-04, running loss 1.11492, it/sec: 4.448757326715007
epoch 1 iter 10360: train loss 1.12967. lr 3.000000e-04, running loss 1.11501, it/sec: 4.527760797564507
epoch 1 iter 10380: train loss 1.14627. lr 3.000000e-04, running loss 1.11494, it/sec: 4.483199411992658
epoch 1 iter 10400: train loss 1.13583. lr 3.000000e-04, running loss 1.11491, it/sec: 4.5182898880948805
epoch 1 iter 10420: train loss 1.12053. lr 3.000000e-04, running loss 1.11486, it/sec: 4.447138758483462
epoch 1 iter 10440: train loss 1.17094. lr 3.000000e-04, running loss 1.11492, it/sec: 4.511209787974038
epoch 1 iter 10460: train loss 1.11489. lr 3.000000e-04, running loss 1.11488, it/sec: 4.468562628050443
epoch 1 iter 10480: train loss 1.09494. lr 3.000000e-04, running loss 1.11472, it/sec: 4.490692912785896
epoch 1 iter 10500: train loss 1.09545. lr 3.000000e-04, running loss 1.11468, it/sec: 4.499345133138247
epoch 1 iter 10520: train loss 1.12362. lr 3.000000e-04, running loss 1.11464, it/sec: 4.48198664048035
epoch 1 iter 10540: train loss 1.09766. lr 3.000000e-04, running loss 1.11477, it/sec: 4.494596692455055
epoch 1 iter 10560: train loss 1.10263. lr 3.000000e-04, running loss 1.11478, it/sec: 4.472884555135149
epoch 1 iter 10580: train loss 1.12625. lr 3.000000e-04, running loss 1.11488, it/sec: 4.4931501119890624
epoch 1 iter 10600: train loss 1.09953. lr 3.000000e-04, running loss 1.11487, it/sec: 4.506398450034957
epoch 1 iter 10620: train loss 1.09818. lr 3.000000e-04, running loss 1.11501, it/sec: 4.478514033286451
epoch 1 iter 10640: train loss 1.10368. lr 3.000000e-04, running loss 1.11499, it/sec: 4.470916322651672
epoch 1 iter 10660: train loss 1.10624. lr 3.000000e-04, running loss 1.11506, it/sec: 4.510575902612953
epoch 1 iter 10680: train loss 1.11158. lr 3.000000e-04, running loss 1.11495, it/sec: 4.485149648983811
epoch 1 iter 10700: train loss 1.12092. lr 3.000000e-04, running loss 1.11498, it/sec: 4.502707721416187
epoch 1 iter 10720: train loss 1.10036. lr 3.000000e-04, running loss 1.11504, it/sec: 4.483049276474398
epoch 1 iter 10740: train loss 1.12552. lr 3.000000e-04, running loss 1.11499, it/sec: 4.526263025507816
epoch 1 iter 10760: train loss 1.10882. lr 3.000000e-04, running loss 1.11509, it/sec: 4.4404068316622824
epoch 1 iter 10780: train loss 1.10564. lr 3.000000e-04, running loss 1.11507, it/sec: 4.518079971690579
epoch 1 iter 10800: train loss 1.11789. lr 3.000000e-04, running loss 1.11503, it/sec: 4.4516243206617325
epoch 1 iter 10820: train loss 1.11409. lr 3.000000e-04, running loss 1.11498, it/sec: 4.49394345412623
epoch 1 iter 10840: train loss 1.15874. lr 3.000000e-04, running loss 1.11492, it/sec: 4.4925009264663895
epoch 1 iter 10860: train loss 1.10143. lr 3.000000e-04, running loss 1.11478, it/sec: 4.4807599682161054
epoch 1 iter 10880: train loss 1.11442. lr 3.000000e-04, running loss 1.11484, it/sec: 4.502812482285439
epoch 1 iter 10900: train loss 1.11767. lr 3.000000e-04, running loss 1.11485, it/sec: 4.5088108856100035
epoch 1 iter 10920: train loss 1.11993. lr 3.000000e-04, running loss 1.11482, it/sec: 4.456230083304471
epoch 1 iter 10940: train loss 1.13510. lr 3.000000e-04, running loss 1.11489, it/sec: 4.487625795760035
epoch 1 iter 10960: train loss 1.09795. lr 3.000000e-04, running loss 1.11480, it/sec: 4.451655275018214
epoch 1 iter 10980: train loss 1.12460. lr 3.000000e-04, running loss 1.11477, it/sec: 4.525270748523185
epoch 1 iter 11000: train loss 1.08906. lr 3.000000e-04, running loss 1.11473, it/sec: 4.424860601097257
epoch 1 iter 11020: train loss 1.10408. lr 3.000000e-04, running loss 1.11473, it/sec: 4.490299100031536
epoch 1 iter 11040: train loss 1.12149. lr 3.000000e-04, running loss 1.11476, it/sec: 4.4700570968799545
epoch 1 iter 11060: train loss 1.10945. lr 3.000000e-04, running loss 1.11472, it/sec: 4.487614981949721
epoch 1 iter 11080: train loss 1.11303. lr 3.000000e-04, running loss 1.11462, it/sec: 4.4557543571578035
epoch 1 iter 11100: train loss 1.10689. lr 3.000000e-04, running loss 1.11464, it/sec: 4.461231443980324
epoch 1 iter 11120: train loss 1.12265. lr 3.000000e-04, running loss 1.11455, it/sec: 4.521748171162
epoch 1 iter 11140: train loss 1.11592. lr 3.000000e-04, running loss 1.11459, it/sec: 4.474897615164861
epoch 1 iter 11160: train loss 1.12831. lr 3.000000e-04, running loss 1.11496, it/sec: 4.5057434620775725
epoch 1 iter 11180: train loss 1.11364. lr 3.000000e-04, running loss 1.11495, it/sec: 4.459397318863818
epoch 1 iter 11200: train loss 1.12249. lr 3.000000e-04, running loss 1.11495, it/sec: 4.492577158051229
epoch 1 iter 11220: train loss 1.12505. lr 3.000000e-04, running loss 1.11512, it/sec: 4.505071879615553
epoch 1 iter 11240: train loss 1.10606. lr 3.000000e-04, running loss 1.11514, it/sec: 4.48811856575987
epoch 1 iter 11260: train loss 1.10755. lr 3.000000e-04, running loss 1.11505, it/sec: 4.4782647372971836
epoch 1 iter 11280: train loss 1.13110. lr 3.000000e-04, running loss 1.11496, it/sec: 4.495724577204155
epoch 1 iter 11300: train loss 1.12952. lr 3.000000e-04, running loss 1.11498, it/sec: 4.451982423188711
epoch 1 iter 11320: train loss 1.09969. lr 3.000000e-04, running loss 1.11496, it/sec: 4.5059600301925125
epoch 1 iter 11340: train loss 1.10637. lr 3.000000e-04, running loss 1.11487, it/sec: 4.449061105776513
epoch 1 iter 11360: train loss 1.11906. lr 3.000000e-04, running loss 1.11480, it/sec: 4.48896270555653
epoch 1 iter 11380: train loss 1.09441. lr 3.000000e-04, running loss 1.11468, it/sec: 4.450898385367942
epoch 1 iter 11400: train loss 1.12475. lr 3.000000e-04, running loss 1.11461, it/sec: 4.524684782247671
epoch 1 iter 11420: train loss 1.11628. lr 3.000000e-04, running loss 1.11455, it/sec: 4.473294290534874
epoch 1 iter 11440: train loss 1.09593. lr 3.000000e-04, running loss 1.11454, it/sec: 4.493711782339789
epoch 1 iter 11460: train loss 1.12045. lr 3.000000e-04, running loss 1.11459, it/sec: 4.489809742021962
epoch 1 iter 11480: train loss 1.10621. lr 3.000000e-04, running loss 1.11453, it/sec: 4.486335432075221
epoch 1 iter 11500: train loss 1.10026. lr 3.000000e-04, running loss 1.11459, it/sec: 4.457258727182511
epoch 1 iter 11520: train loss 1.11854. lr 3.000000e-04, running loss 1.11448, it/sec: 4.519943573078867
epoch 1 iter 11540: train loss 1.24347. lr 3.000000e-04, running loss 1.11460, it/sec: 4.452473838544359
epoch 1 iter 11560: train loss 1.11107. lr 3.000000e-04, running loss 1.11458, it/sec: 4.533921486763201
epoch 1 iter 11580: train loss 1.11662. lr 3.000000e-04, running loss 1.11457, it/sec: 4.439182158912808
epoch 1 iter 11600: train loss 1.09976. lr 3.000000e-04, running loss 1.11453, it/sec: 4.473186277132967
epoch 1 iter 11620: train loss 1.12077. lr 3.000000e-04, running loss 1.11449, it/sec: 4.477129158947308
epoch 1 iter 11640: train loss 1.09747. lr 3.000000e-04, running loss 1.11454, it/sec: 4.499733205958029
epoch 1 iter 11660: train loss 1.12677. lr 3.000000e-04, running loss 1.11459, it/sec: 4.519189141812561
epoch 1 iter 11680: train loss 1.13319. lr 3.000000e-04, running loss 1.11468, it/sec: 4.446818097518241
epoch 1 iter 11700: train loss 1.09845. lr 3.000000e-04, running loss 1.11465, it/sec: 4.497770389937888
epoch 1 iter 11720: train loss 1.12453. lr 3.000000e-04, running loss 1.11473, it/sec: 4.496887161901687
epoch 1 iter 11740: train loss 1.11396. lr 3.000000e-04, running loss 1.11472, it/sec: 4.513780567048001
epoch 1 iter 11760: train loss 1.10539. lr 3.000000e-04, running loss 1.11451, it/sec: 4.474103955536519
epoch 1 iter 11780: train loss 1.12255. lr 3.000000e-04, running loss 1.11452, it/sec: 4.493718446942705
epoch 1 iter 11800: train loss 1.19734. lr 3.000000e-04, running loss 1.11449, it/sec: 4.477869451566944
epoch 1 iter 11820: train loss 1.12173. lr 3.000000e-04, running loss 1.11462, it/sec: 4.448034600583234
epoch 1 iter 11840: train loss 1.11708. lr 3.000000e-04, running loss 1.11461, it/sec: 4.518403438350462
epoch 1 iter 11860: train loss 1.13179. lr 3.000000e-04, running loss 1.11465, it/sec: 4.459874580450945
epoch 1 iter 11880: train loss 1.11408. lr 3.000000e-04, running loss 1.11467, it/sec: 4.489135141119503
epoch 1 iter 11900: train loss 1.09987. lr 3.000000e-04, running loss 1.11462, it/sec: 4.489458872302615
epoch 1 iter 11920: train loss 1.11835. lr 3.000000e-04, running loss 1.11460, it/sec: 4.454928177963233
epoch 1 iter 11940: train loss 1.10995. lr 3.000000e-04, running loss 1.11447, it/sec: 4.509469286960138
epoch 1 iter 11960: train loss 1.10624. lr 3.000000e-04, running loss 1.11443, it/sec: 4.486060511309956
epoch 1 iter 11980: train loss 1.09525. lr 3.000000e-04, running loss 1.11479, it/sec: 4.541850443218317
epoch 1 iter 12000: train loss 1.10642. lr 3.000000e-04, running loss 1.11474, it/sec: 4.43963170621919
epoch 1 iter 12020: train loss 1.10384. lr 3.000000e-04, running loss 1.11467, it/sec: 4.511547456667814
epoch 1 iter 12040: train loss 1.10329. lr 3.000000e-04, running loss 1.11461, it/sec: 4.455887102564557
epoch 1 iter 12060: train loss 1.12618. lr 3.000000e-04, running loss 1.11461, it/sec: 4.498991315324055
epoch 1 iter 12080: train loss 1.14232. lr 3.000000e-04, running loss 1.11456, it/sec: 4.451271132346817
epoch 1 iter 12100: train loss 1.09999. lr 3.000000e-04, running loss 1.11449, it/sec: 4.505038778049831
epoch 1 iter 12120: train loss 1.11745. lr 3.000000e-04, running loss 1.11444, it/sec: 4.4944107459088025
epoch 1 iter 12140: train loss 1.10458. lr 3.000000e-04, running loss 1.11447, it/sec: 4.433471936368559
epoch 1 iter 12160: train loss 1.10695. lr 3.000000e-04, running loss 1.11458, it/sec: 4.462920299771662
epoch 1 iter 12180: train loss 1.10048. lr 3.000000e-04, running loss 1.11458, it/sec: 4.50806802995053
epoch 1 iter 12200: train loss 1.11972. lr 3.000000e-04, running loss 1.11462, it/sec: 4.472835139463813
epoch 1 iter 12220: train loss 1.10489. lr 3.000000e-04, running loss 1.11448, it/sec: 4.528161722236368
epoch 1 iter 12240: train loss 1.10039. lr 3.000000e-04, running loss 1.11445, it/sec: 4.470149772311656
epoch 1 iter 12260: train loss 1.12412. lr 3.000000e-04, running loss 1.11444, it/sec: 4.501585611102055
epoch 1 iter 12280: train loss 1.09503. lr 3.000000e-04, running loss 1.11443, it/sec: 4.463963831145176
epoch 1 iter 12300: train loss 1.12417. lr 3.000000e-04, running loss 1.11444, it/sec: 4.5191899585039925
epoch 1 iter 12320: train loss 1.13829. lr 3.000000e-04, running loss 1.11444, it/sec: 4.462454513959337
epoch 1 iter 12340: train loss 1.13825. lr 3.000000e-04, running loss 1.11445, it/sec: 4.484004464007118
epoch 1 iter 12360: train loss 1.10244. lr 3.000000e-04, running loss 1.11428, it/sec: 4.4963176415718795
epoch 1 iter 12380: train loss 1.10075. lr 3.000000e-04, running loss 1.11422, it/sec: 4.502177243811787
epoch 1 iter 12400: train loss 1.13647. lr 3.000000e-04, running loss 1.11426, it/sec: 4.437031840941057
epoch 1 iter 12420: train loss 1.12720. lr 3.000000e-04, running loss 1.11422, it/sec: 4.516829140881516
epoch 1 iter 12440: train loss 1.10620. lr 3.000000e-04, running loss 1.11418, it/sec: 4.424919634541117
epoch 1 iter 12460: train loss 1.14072. lr 3.000000e-04, running loss 1.11413, it/sec: 4.507606042798472
epoch 1 iter 12480: train loss 1.11257. lr 3.000000e-04, running loss 1.11407, it/sec: 4.456468769404796
epoch 1 iter 12500: train loss 1.14306. lr 3.000000e-04, running loss 1.11420, it/sec: 1.1576625969615861
epoch 1 iter 12520: train loss 1.12083. lr 3.000000e-04, running loss 1.11415, it/sec: 4.444585030769199
epoch 1 iter 12540: train loss 1.10816. lr 3.000000e-04, running loss 1.11404, it/sec: 4.517873115102818
epoch 1 iter 12560: train loss 1.11018. lr 3.000000e-04, running loss 1.11408, it/sec: 4.453147954129064
epoch 1 iter 12580: train loss 1.11804. lr 3.000000e-04, running loss 1.11395, it/sec: 4.508526801281259
epoch 1 iter 12600: train loss 1.12114. lr 3.000000e-04, running loss 1.11399, it/sec: 4.504905259163545
epoch 1 iter 12620: train loss 1.10625. lr 3.000000e-04, running loss 1.11398, it/sec: 4.496062965007544
epoch 1 iter 12640: train loss 1.11451. lr 3.000000e-04, running loss 1.11385, it/sec: 4.491169453285515
epoch 1 iter 12660: train loss 1.11048. lr 3.000000e-04, running loss 1.11376, it/sec: 4.492096584832413
epoch 1 iter 12680: train loss 1.09571. lr 3.000000e-04, running loss 1.11370, it/sec: 4.483196477834574
epoch 1 iter 12700: train loss 1.09568. lr 3.000000e-04, running loss 1.11383, it/sec: 4.503425163495658
epoch 1 iter 12720: train loss 1.10567. lr 3.000000e-04, running loss 1.11423, it/sec: 4.456275638430668
epoch 1 iter 12740: train loss 1.11846. lr 3.000000e-04, running loss 1.11410, it/sec: 4.5225994046605935
epoch 1 iter 12760: train loss 1.13972. lr 3.000000e-04, running loss 1.11403, it/sec: 4.458163581018235
epoch 1 iter 12780: train loss 1.53218. lr 3.000000e-04, running loss 1.11447, it/sec: 4.47423547537762
epoch 1 iter 12800: train loss 1.11936. lr 3.000000e-04, running loss 1.11443, it/sec: 4.467278331593367
epoch 1 iter 12820: train loss 1.08944. lr 3.000000e-04, running loss 1.11468, it/sec: 4.500646010828923
epoch 1 iter 12840: train loss 1.10106. lr 3.000000e-04, running loss 1.11468, it/sec: 4.482118241792653
epoch 1 iter 12860: train loss 1.10436. lr 3.000000e-04, running loss 1.11455, it/sec: 4.488381107274499
epoch 1 iter 12880: train loss 1.11070. lr 3.000000e-04, running loss 1.11447, it/sec: 4.4608855049268925
epoch 1 iter 12900: train loss 1.09887. lr 3.000000e-04, running loss 1.11458, it/sec: 4.470639330532224
epoch 1 iter 12920: train loss 1.09106. lr 3.000000e-04, running loss 1.11455, it/sec: 4.514518968185589
epoch 1 iter 12940: train loss 1.10787. lr 3.000000e-04, running loss 1.11454, it/sec: 4.4472197865929575
epoch 1 iter 12960: train loss 1.11005. lr 3.000000e-04, running loss 1.11463, it/sec: 4.51434987360356
epoch 1 iter 12980: train loss 1.12303. lr 3.000000e-04, running loss 1.11466, it/sec: 4.456072833902019
epoch 1 iter 13000: train loss 1.11199. lr 3.000000e-04, running loss 1.11452, it/sec: 4.487857745489486
epoch 1 iter 13020: train loss 1.10378. lr 3.000000e-04, running loss 1.11438, it/sec: 4.515432788256126
epoch 1 iter 13040: train loss 1.11569. lr 3.000000e-04, running loss 1.11443, it/sec: 4.440340976876204
epoch 1 iter 13060: train loss 1.13528. lr 3.000000e-04, running loss 1.11451, it/sec: 4.516750799178624
epoch 1 iter 13080: train loss 1.12017. lr 3.000000e-04, running loss 1.11449, it/sec: 4.464473600383385
epoch 1 iter 13100: train loss 1.12048. lr 3.000000e-04, running loss 1.11443, it/sec: 4.458350972716149
epoch 1 iter 13120: train loss 1.11615. lr 3.000000e-04, running loss 1.11437, it/sec: 4.5000096436622
epoch 1 iter 13140: train loss 1.13740. lr 3.000000e-04, running loss 1.11433, it/sec: 4.4530624668498335
epoch 1 iter 13160: train loss 1.09895. lr 3.000000e-04, running loss 1.11431, it/sec: 4.529343465002033
epoch 1 iter 13180: train loss 1.10835. lr 3.000000e-04, running loss 1.11431, it/sec: 4.4858593334956325
epoch 1 iter 13200: train loss 1.11170. lr 3.000000e-04, running loss 1.11434, it/sec: 4.512729318920065
epoch 1 iter 13220: train loss 1.12679. lr 3.000000e-04, running loss 1.11436, it/sec: 4.477295075217553
epoch 1 iter 13240: train loss 1.12403. lr 3.000000e-04, running loss 1.11447, it/sec: 4.515953076504513
epoch 1 iter 13260: train loss 1.08031. lr 3.000000e-04, running loss 1.11454, it/sec: 4.457097471314493
epoch 1 iter 13280: train loss 1.11222. lr 3.000000e-04, running loss 1.11459, it/sec: 4.515646068151269
epoch 1 iter 13300: train loss 1.11622. lr 3.000000e-04, running loss 1.11469, it/sec: 4.4537235106199
epoch 1 iter 13320: train loss 1.10467. lr 3.000000e-04, running loss 1.11475, it/sec: 4.491116507434622
epoch 1 iter 13340: train loss 1.14057. lr 3.000000e-04, running loss 1.11502, it/sec: 4.4417927141058025
epoch 1 iter 13360: train loss 1.09189. lr 3.000000e-04, running loss 1.11504, it/sec: 4.509581154932136
epoch 1 iter 13380: train loss 1.11154. lr 3.000000e-04, running loss 1.11513, it/sec: 4.458749818504078
epoch 1 iter 13400: train loss 1.11693. lr 3.000000e-04, running loss 1.11521, it/sec: 4.494060793159292
epoch 1 iter 13420: train loss 1.10959. lr 3.000000e-04, running loss 1.11511, it/sec: 4.458186477150024
epoch 1 iter 13440: train loss 1.10816. lr 3.000000e-04, running loss 1.11569, it/sec: 4.528592803334479
epoch 1 iter 13460: train loss 1.10566. lr 3.000000e-04, running loss 1.11557, it/sec: 4.440242908155049
epoch 1 iter 13480: train loss 1.10243. lr 3.000000e-04, running loss 1.11552, it/sec: 4.532932422750513
epoch 1 iter 13500: train loss 1.11316. lr 3.000000e-04, running loss 1.11577, it/sec: 4.453009223353143
epoch 1 iter 13520: train loss 1.10589. lr 3.000000e-04, running loss 1.11567, it/sec: 4.512949901039883
epoch 1 iter 13540: train loss 1.11217. lr 3.000000e-04, running loss 1.11562, it/sec: 4.498541161351407
epoch 1 iter 13560: train loss 1.12474. lr 3.000000e-04, running loss 1.11551, it/sec: 4.5139492726636945
epoch 1 iter 13580: train loss 1.10328. lr 3.000000e-04, running loss 1.11539, it/sec: 4.46552723111639
epoch 1 iter 13600: train loss 1.10336. lr 3.000000e-04, running loss 1.11524, it/sec: 4.5007810813991265
epoch 1 iter 13620: train loss 1.11188. lr 3.000000e-04, running loss 1.11534, it/sec: 4.460699452108367
epoch 1 iter 13640: train loss 1.14283. lr 3.000000e-04, running loss 1.11529, it/sec: 4.516807169162477
epoch 1 iter 13660: train loss 1.10111. lr 3.000000e-04, running loss 1.11525, it/sec: 4.469165664333307
epoch 1 iter 13680: train loss 1.12498. lr 3.000000e-04, running loss 1.11520, it/sec: 4.521098136951325
epoch 1 iter 13700: train loss 1.11328. lr 3.000000e-04, running loss 1.11519, it/sec: 4.463082914088555
epoch 1 iter 13720: train loss 1.11002. lr 3.000000e-04, running loss 1.11508, it/sec: 4.480497152488979
epoch 1 iter 13740: train loss 1.10208. lr 3.000000e-04, running loss 1.11497, it/sec: 4.49573288424901
epoch 1 iter 13760: train loss 1.12255. lr 3.000000e-04, running loss 1.11494, it/sec: 4.476318237797185
epoch 1 iter 13780: train loss 1.11000. lr 3.000000e-04, running loss 1.11497, it/sec: 4.5154306662520085
epoch 1 iter 13800: train loss 1.12325. lr 3.000000e-04, running loss 1.11486, it/sec: 4.461719787578906
epoch 1 iter 13820: train loss 1.10259. lr 3.000000e-04, running loss 1.11479, it/sec: 4.529436789173963
epoch 1 iter 13840: train loss 1.10362. lr 3.000000e-04, running loss 1.11484, it/sec: 4.444308211534837
epoch 1 iter 13860: train loss 1.12941. lr 3.000000e-04, running loss 1.11484, it/sec: 4.5201740947311055
epoch 1 iter 13880: train loss 1.10583. lr 3.000000e-04, running loss 1.11483, it/sec: 4.420704201973173
epoch 1 iter 13900: train loss 1.13677. lr 3.000000e-04, running loss 1.11479, it/sec: 4.468155575287604
epoch 1 iter 13920: train loss 1.09908. lr 3.000000e-04, running loss 1.11483, it/sec: 4.4676009534187875
epoch 1 iter 13940: train loss 1.10764. lr 3.000000e-04, running loss 1.11469, it/sec: 4.517385422439486
epoch 1 iter 13960: train loss 1.10497. lr 3.000000e-04, running loss 1.11467, it/sec: 4.429491976131924
epoch 1 iter 13980: train loss 1.11073. lr 3.000000e-04, running loss 1.11470, it/sec: 4.490442925782682
epoch 1 iter 14000: train loss 1.11660. lr 3.000000e-04, running loss 1.11461, it/sec: 4.462025778759633
epoch 1 iter 14020: train loss 1.11059. lr 3.000000e-04, running loss 1.11461, it/sec: 4.52509587142478
epoch 1 iter 14040: train loss 1.11526. lr 3.000000e-04, running loss 1.11460, it/sec: 4.449730328502255
epoch 1 iter 14060: train loss 1.12210. lr 3.000000e-04, running loss 1.11460, it/sec: 4.489500694549077
epoch 1 iter 14080: train loss 1.10101. lr 3.000000e-04, running loss 1.11475, it/sec: 4.525492925592595
epoch 1 iter 14100: train loss 1.10024. lr 3.000000e-04, running loss 1.11470, it/sec: 4.462536639000303
epoch 1 iter 14120: train loss 1.09414. lr 3.000000e-04, running loss 1.11476, it/sec: 4.513343276763151
epoch 1 iter 14140: train loss 1.09906. lr 3.000000e-04, running loss 1.11476, it/sec: 4.441412006546333
epoch 1 iter 14160: train loss 1.11032. lr 3.000000e-04, running loss 1.11471, it/sec: 4.525311621916255
epoch 1 iter 14180: train loss 1.11576. lr 3.000000e-04, running loss 1.11462, it/sec: 4.49332826128087
epoch 1 iter 14200: train loss 1.10498. lr 3.000000e-04, running loss 1.11456, it/sec: 4.51902120803218
epoch 1 iter 14220: train loss 1.12907. lr 3.000000e-04, running loss 1.11459, it/sec: 4.520559965479436
epoch 1 iter 14240: train loss 1.11378. lr 3.000000e-04, running loss 1.11485, it/sec: 4.493196161865771
epoch 1 iter 14260: train loss 1.10354. lr 3.000000e-04, running loss 1.11491, it/sec: 4.490764364053795
epoch 1 iter 14280: train loss 1.12675. lr 3.000000e-04, running loss 1.11495, it/sec: 4.46542360023506
epoch 1 iter 14300: train loss 1.11647. lr 3.000000e-04, running loss 1.11484, it/sec: 4.4614358329106665
epoch 1 iter 14320: train loss 1.12485. lr 3.000000e-04, running loss 1.11480, it/sec: 4.502849504804004
epoch 1 iter 14340: train loss 1.13482. lr 3.000000e-04, running loss 1.11485, it/sec: 4.458901431180856
epoch 1 iter 14360: train loss 1.11382. lr 3.000000e-04, running loss 1.11477, it/sec: 4.523381267113906
epoch 1 iter 14380: train loss 1.11452. lr 3.000000e-04, running loss 1.11475, it/sec: 4.461579508382488
epoch 1 iter 14400: train loss 1.11029. lr 3.000000e-04, running loss 1.11459, it/sec: 4.516976977561034
epoch 1 iter 14420: train loss 1.09685. lr 3.000000e-04, running loss 1.11452, it/sec: 4.46433868743089
epoch 1 iter 14440: train loss 1.11607. lr 3.000000e-04, running loss 1.11456, it/sec: 4.513242792820015
epoch 1 iter 14460: train loss 1.10752. lr 3.000000e-04, running loss 1.11456, it/sec: 4.443636472869107
epoch 1 iter 14480: train loss 1.09510. lr 3.000000e-04, running loss 1.11447, it/sec: 4.509657844167902
epoch 1 iter 14500: train loss 1.14884. lr 3.000000e-04, running loss 1.11460, it/sec: 4.47029674658391
epoch 1 iter 14520: train loss 1.11782. lr 3.000000e-04, running loss 1.11455, it/sec: 4.510650529489356
epoch 1 iter 14540: train loss 1.10232. lr 3.000000e-04, running loss 1.11460, it/sec: 4.440670071767216
epoch 1 iter 14560: train loss 1.10862. lr 3.000000e-04, running loss 1.11462, it/sec: 4.5296774727303895
epoch 1 iter 14580: train loss 1.12804. lr 3.000000e-04, running loss 1.11458, it/sec: 4.443730267462908
epoch 1 iter 14600: train loss 1.12280. lr 3.000000e-04, running loss 1.11468, it/sec: 4.536818628728349
epoch 1 iter 14620: train loss 1.11631. lr 3.000000e-04, running loss 1.11460, it/sec: 4.438250559885115
epoch 1 iter 14640: train loss 1.11911. lr 3.000000e-04, running loss 1.11473, it/sec: 4.4916560615192775
epoch 1 iter 14660: train loss 1.13435. lr 3.000000e-04, running loss 1.11474, it/sec: 4.457481409414608
epoch 1 iter 14680: train loss 1.11436. lr 3.000000e-04, running loss 1.11533, it/sec: 4.519237217742104
epoch 1 iter 14700: train loss 1.10733. lr 3.000000e-04, running loss 1.11527, it/sec: 4.469962706343636
epoch 1 iter 14720: train loss 1.12034. lr 3.000000e-04, running loss 1.11526, it/sec: 4.494748086912661
epoch 1 iter 14740: train loss 1.13806. lr 3.000000e-04, running loss 1.11528, it/sec: 4.503365355430193
epoch 1 iter 14760: train loss 1.14159. lr 3.000000e-04, running loss 1.11521, it/sec: 4.450641299990059
epoch 1 iter 14780: train loss 1.12607. lr 3.000000e-04, running loss 1.11526, it/sec: 4.499318634287541
epoch 1 iter 14800: train loss 1.14611. lr 3.000000e-04, running loss 1.12139, it/sec: 4.495568084380527
epoch 1 iter 14820: train loss 1.11949. lr 3.000000e-04, running loss 1.12138, it/sec: 4.447290454042944
epoch 1 iter 14840: train loss 1.14928. lr 3.000000e-04, running loss 1.12122, it/sec: 4.50424690964655
epoch 1 iter 14860: train loss 1.13360. lr 3.000000e-04, running loss 1.12101, it/sec: 4.453866768296903
epoch 1 iter 14880: train loss 1.11371. lr 3.000000e-04, running loss 1.12081, it/sec: 4.496710429862635
epoch 1 iter 14900: train loss 1.10272. lr 3.000000e-04, running loss 1.12056, it/sec: 4.475696121331713
epoch 1 iter 14920: train loss 1.13807. lr 3.000000e-04, running loss 1.12045, it/sec: 4.488614747619708
epoch 1 iter 14940: train loss 1.15171. lr 3.000000e-04, running loss 1.12044, it/sec: 4.5009022009266015
epoch 1 iter 14960: train loss 1.10243. lr 3.000000e-04, running loss 1.12039, it/sec: 4.464040989192598
epoch 1 iter 14980: train loss 1.10284. lr 3.000000e-04, running loss 1.12038, it/sec: 4.488125354481658
epoch 1 iter 15000: train loss 1.15520. lr 3.000000e-04, running loss 1.12026, it/sec: 4.466261436786593
epoch 1 iter 15020: train loss 1.10775. lr 3.000000e-04, running loss 1.12016, it/sec: 4.47133701250636
epoch 1 iter 15040: train loss 1.10568. lr 3.000000e-04, running loss 1.11998, it/sec: 4.478311045232552
epoch 1 iter 15060: train loss 1.12436. lr 3.000000e-04, running loss 1.11983, it/sec: 4.502393469234311
epoch 1 iter 15080: train loss 1.14644. lr 3.000000e-04, running loss 1.11978, it/sec: 4.43359913195641
epoch 1 iter 15100: train loss 1.11119. lr 3.000000e-04, running loss 1.11966, it/sec: 4.5170745256643015
epoch 1 iter 15120: train loss 1.11549. lr 3.000000e-04, running loss 1.11965, it/sec: 4.501333801344258
epoch 1 iter 15140: train loss 1.10644. lr 3.000000e-04, running loss 1.11950, it/sec: 4.465754249511208
epoch 1 iter 15160: train loss 1.09204. lr 3.000000e-04, running loss 1.11936, it/sec: 4.473243084937395
epoch 1 iter 15180: train loss 1.11457. lr 3.000000e-04, running loss 1.11927, it/sec: 4.505384109671269
epoch 1 iter 15200: train loss 1.10168. lr 3.000000e-04, running loss 1.11908, it/sec: 4.4603377178986
epoch 1 iter 15220: train loss 1.10273. lr 3.000000e-04, running loss 1.11894, it/sec: 4.526355342676931
epoch 1 iter 15240: train loss 1.73651. lr 3.000000e-04, running loss 1.11948, it/sec: 4.448494433179389
epoch 1 iter 15260: train loss 1.11454. lr 3.000000e-04, running loss 1.11948, it/sec: 4.527027547084368
epoch 1 iter 15280: train loss 1.11583. lr 3.000000e-04, running loss 1.11941, it/sec: 4.4412596474184465
epoch 1 iter 15300: train loss 1.10305. lr 3.000000e-04, running loss 1.11922, it/sec: 4.534991906713419
epoch 1 iter 15320: train loss 1.11401. lr 3.000000e-04, running loss 1.11915, it/sec: 4.415532382188254
epoch 1 iter 15340: train loss 1.12759. lr 3.000000e-04, running loss 1.11906, it/sec: 4.518130371949657
epoch 1 iter 15360: train loss 1.11079. lr 3.000000e-04, running loss 1.11906, it/sec: 4.444593624792348
epoch 1 iter 15380: train loss 1.12866. lr 3.000000e-04, running loss 1.11894, it/sec: 4.4949822294822654
epoch 1 iter 15400: train loss 1.11344. lr 3.000000e-04, running loss 1.11875, it/sec: 4.448138553426997
epoch 1 iter 15420: train loss 1.10695. lr 3.000000e-04, running loss 1.11871, it/sec: 4.534218710658161
epoch 1 iter 15440: train loss 1.10334. lr 3.000000e-04, running loss 1.11854, it/sec: 4.4440682787638135
epoch 1 iter 15460: train loss 1.11115. lr 3.000000e-04, running loss 1.11848, it/sec: 4.499595790610753
epoch 1 iter 15480: train loss 1.12733. lr 3.000000e-04, running loss 1.11840, it/sec: 4.468038388216531
epoch 1 iter 15500: train loss 1.11406. lr 3.000000e-04, running loss 1.11829, it/sec: 4.523239150067603
epoch 1 iter 15520: train loss 1.13317. lr 3.000000e-04, running loss 1.11837, it/sec: 4.47451535480599
epoch 1 iter 15540: train loss 1.10135. lr 3.000000e-04, running loss 1.11823, it/sec: 4.514084795398463
epoch 1 iter 15560: train loss 1.12304. lr 3.000000e-04, running loss 1.11818, it/sec: 4.445206866648304
epoch 1 iter 15580: train loss 1.11970. lr 3.000000e-04, running loss 1.11808, it/sec: 4.5111267169333615
epoch 1 iter 15600: train loss 1.10906. lr 3.000000e-04, running loss 1.11832, it/sec: 4.474517557398565
epoch 1 iter 15620: train loss 1.10692. lr 3.000000e-04, running loss 1.11822, it/sec: 4.48754479953612
epoch 1 iter 15640: train loss 1.09827. lr 3.000000e-04, running loss 1.11799, it/sec: 4.483366882973742
epoch 1 iter 15660: train loss 1.10777. lr 3.000000e-04, running loss 1.11789, it/sec: 4.538220601410133
epoch 1 iter 15680: train loss 1.09908. lr 3.000000e-04, running loss 1.11802, it/sec: 4.454675785448445
epoch 1 iter 15700: train loss 1.10311. lr 3.000000e-04, running loss 1.11786, it/sec: 4.517548355886315
epoch 1 iter 15720: train loss 1.09269. lr 3.000000e-04, running loss 1.11786, it/sec: 4.48773211147782
epoch 1 iter 15740: train loss 1.10053. lr 3.000000e-04, running loss 1.11769, it/sec: 4.46856013144519
epoch 1 iter 15760: train loss 1.11016. lr 3.000000e-04, running loss 1.11761, it/sec: 4.504031986916056
epoch 1 iter 15780: train loss 1.09667. lr 3.000000e-04, running loss 1.11758, it/sec: 4.467091466378168
epoch 1 iter 15800: train loss 1.11603. lr 3.000000e-04, running loss 1.11750, it/sec: 4.490280367823422
epoch 1 iter 15820: train loss 1.10940. lr 3.000000e-04, running loss 1.11747, it/sec: 4.50026173084987
epoch 1 iter 15840: train loss 1.13575. lr 3.000000e-04, running loss 1.11742, it/sec: 4.466534234564785
epoch 1 iter 15860: train loss 1.12119. lr 3.000000e-04, running loss 1.11729, it/sec: 4.51607179210042
epoch 1 iter 15880: train loss 1.12090. lr 3.000000e-04, running loss 1.11724, it/sec: 4.473813339396961
epoch 1 iter 15900: train loss 1.11657. lr 3.000000e-04, running loss 1.11715, it/sec: 4.512925827241206
epoch 1 iter 15920: train loss 1.11177. lr 3.000000e-04, running loss 1.11718, it/sec: 4.476843460063791
epoch 1 iter 15940: train loss 1.10885. lr 3.000000e-04, running loss 1.11720, it/sec: 4.513943627196275
epoch 1 iter 15960: train loss 1.11593. lr 3.000000e-04, running loss 1.11718, it/sec: 4.443692611912836
epoch 1 iter 15980: train loss 1.11694. lr 3.000000e-04, running loss 1.11723, it/sec: 4.476969007819668
epoch 1 iter 16000: train loss 1.13121. lr 3.000000e-04, running loss 1.11713, it/sec: 4.480369138828098
epoch 1 iter 16020: train loss 1.11810. lr 3.000000e-04, running loss 1.11714, it/sec: 4.495829053380959
epoch 1 iter 16040: train loss 1.10124. lr 3.000000e-04, running loss 1.11714, it/sec: 4.487077584236747
epoch 1 iter 16060: train loss 1.10213. lr 3.000000e-04, running loss 1.11706, it/sec: 4.5119381871821505
epoch 1 iter 16080: train loss 1.15164. lr 3.000000e-04, running loss 1.11731, it/sec: 4.492571910137562
epoch 1 iter 16100: train loss 1.10066. lr 3.000000e-04, running loss 1.11732, it/sec: 4.480968299823398
epoch 1 iter 16120: train loss 1.12128. lr 3.000000e-04, running loss 1.11722, it/sec: 4.464920988367072
epoch 1 iter 16140: train loss 1.10624. lr 3.000000e-04, running loss 1.11713, it/sec: 4.513400213879843
epoch 1 iter 16160: train loss 1.11506. lr 3.000000e-04, running loss 1.11700, it/sec: 4.460209799855684
epoch 1 iter 16180: train loss 1.11270. lr 3.000000e-04, running loss 1.11686, it/sec: 4.485276186491736
epoch 1 iter 16200: train loss 1.11134. lr 3.000000e-04, running loss 1.11680, it/sec: 4.450265867564626
epoch 1 iter 16220: train loss 1.11605. lr 3.000000e-04, running loss 1.11663, it/sec: 4.524391549792658
epoch 1 iter 16240: train loss 1.12002. lr 3.000000e-04, running loss 1.11656, it/sec: 4.4797150321323524
epoch 1 iter 16260: train loss 1.12261. lr 3.000000e-04, running loss 1.11652, it/sec: 4.480962536694482
epoch 1 iter 16280: train loss 1.11530. lr 3.000000e-04, running loss 1.11643, it/sec: 4.445009691612773
epoch 1 iter 16300: train loss 1.12744. lr 3.000000e-04, running loss 1.11641, it/sec: 4.508630957513233
epoch 1 iter 16320: train loss 1.14922. lr 3.000000e-04, running loss 1.11632, it/sec: 4.4556750227172754
epoch 1 iter 16340: train loss 1.10116. lr 3.000000e-04, running loss 1.11619, it/sec: 4.477027654795024
epoch 1 iter 16360: train loss 1.10838. lr 3.000000e-04, running loss 1.11617, it/sec: 4.457722097592134
epoch 1 iter 16380: train loss 1.10961. lr 3.000000e-04, running loss 1.11615, it/sec: 4.527562831750074
epoch 1 iter 16400: train loss 1.10825. lr 3.000000e-04, running loss 1.11609, it/sec: 4.422297167224446
epoch 1 iter 16420: train loss 1.12719. lr 3.000000e-04, running loss 1.11609, it/sec: 4.527759464656605
epoch 1 iter 16440: train loss 1.11091. lr 3.000000e-04, running loss 1.11600, it/sec: 4.479536696858897
epoch 1 iter 16460: train loss 1.11206. lr 3.000000e-04, running loss 1.11591, it/sec: 4.482494628302432
epoch 1 iter 16480: train loss 1.11015. lr 3.000000e-04, running loss 1.11574, it/sec: 4.447498769388689
epoch 1 iter 16500: train loss 1.09851. lr 3.000000e-04, running loss 1.11559, it/sec: 4.449446116246626
epoch 1 iter 16520: train loss 1.11142. lr 3.000000e-04, running loss 1.11547, it/sec: 4.52064644976535
epoch 1 iter 16540: train loss 1.08787. lr 3.000000e-04, running loss 1.11539, it/sec: 4.4726715936528985
epoch 1 iter 16560: train loss 1.09070. lr 3.000000e-04, running loss 1.11535, it/sec: 4.503945224419452
epoch 1 iter 16580: train loss 1.10595. lr 3.000000e-04, running loss 1.11523, it/sec: 4.463938364471339
epoch 1 iter 16600: train loss 1.10806. lr 3.000000e-04, running loss 1.11522, it/sec: 4.496009820176727
epoch 1 iter 16620: train loss 1.10527. lr 3.000000e-04, running loss 1.11516, it/sec: 4.485994985711939
epoch 1 iter 16640: train loss 1.11416. lr 3.000000e-04, running loss 1.11513, it/sec: 4.504358397016801
epoch 1 iter 16660: train loss 1.12821. lr 3.000000e-04, running loss 1.11508, it/sec: 4.488536011852846
epoch 1 iter 16680: train loss 1.10934. lr 3.000000e-04, running loss 1.11507, it/sec: 4.500108708183109
epoch 1 iter 16700: train loss 1.12165. lr 3.000000e-04, running loss 1.11510, it/sec: 4.451001798657564
epoch 1 iter 16720: train loss 1.11353. lr 3.000000e-04, running loss 1.11514, it/sec: 4.520500764816534
epoch 1 iter 16740: train loss 1.11271. lr 3.000000e-04, running loss 1.11508, it/sec: 4.451041204191847
epoch 1 iter 16760: train loss 1.10162. lr 3.000000e-04, running loss 1.11507, it/sec: 4.53667737442645
epoch 1 iter 16780: train loss 1.15524. lr 3.000000e-04, running loss 1.11507, it/sec: 4.465857496395214
epoch 1 iter 16800: train loss 1.11556. lr 3.000000e-04, running loss 1.11503, it/sec: 4.513238779388756
epoch 1 iter 16820: train loss 1.10790. lr 3.000000e-04, running loss 1.11503, it/sec: 4.472560270892636
epoch 1 iter 16840: train loss 1.11282. lr 3.000000e-04, running loss 1.11507, it/sec: 4.525796133360741
epoch 1 iter 16860: train loss 1.11024. lr 3.000000e-04, running loss 1.11507, it/sec: 4.455251815533932
epoch 1 iter 16880: train loss 1.11632. lr 3.000000e-04, running loss 1.11498, it/sec: 4.514763551503477
epoch 1 iter 16900: train loss 1.12667. lr 3.000000e-04, running loss 1.11501, it/sec: 4.466256489358586
epoch 1 iter 16920: train loss 1.11213. lr 3.000000e-04, running loss 1.11493, it/sec: 4.491771586473709
epoch 1 iter 16940: train loss 1.09556. lr 3.000000e-04, running loss 1.11492, it/sec: 4.449644279751697
epoch 1 iter 16960: train loss 1.10368. lr 3.000000e-04, running loss 1.11491, it/sec: 4.506831552143603
epoch 1 iter 16980: train loss 1.10622. lr 3.000000e-04, running loss 1.11487, it/sec: 4.452006801827784
epoch 1 iter 17000: train loss 1.09526. lr 3.000000e-04, running loss 1.11480, it/sec: 4.5061689660585715
epoch 1 iter 17020: train loss 1.08472. lr 3.000000e-04, running loss 1.11531, it/sec: 4.463465313176686
epoch 1 iter 17040: train loss 1.11086. lr 3.000000e-04, running loss 1.11537, it/sec: 4.503893293448829
epoch 1 iter 17060: train loss 1.12783. lr 3.000000e-04, running loss 1.11540, it/sec: 4.436146917504608
epoch 1 iter 17080: train loss 1.17996. lr 3.000000e-04, running loss 1.11546, it/sec: 4.496142207463718
epoch 1 iter 17100: train loss 1.13423. lr 3.000000e-04, running loss 1.11550, it/sec: 4.46392712633261
epoch 1 iter 17120: train loss 1.10207. lr 3.000000e-04, running loss 1.11546, it/sec: 4.504182901733688
epoch 1 iter 17140: train loss 1.12774. lr 3.000000e-04, running loss 1.11556, it/sec: 4.515174573514644
epoch 1 iter 17160: train loss 1.10053. lr 3.000000e-04, running loss 1.11554, it/sec: 4.496422488950116
epoch 1 iter 17180: train loss 1.11389. lr 3.000000e-04, running loss 1.11555, it/sec: 4.480109220783273
epoch 1 iter 17200: train loss 1.12587. lr 3.000000e-04, running loss 1.11559, it/sec: 4.47341083326818
epoch 1 iter 17220: train loss 1.12079. lr 3.000000e-04, running loss 1.11546, it/sec: 4.445381490847195
epoch 1 iter 17240: train loss 1.10024. lr 3.000000e-04, running loss 1.11534, it/sec: 4.471912841030393
epoch 1 iter 17260: train loss 1.10158. lr 3.000000e-04, running loss 1.11524, it/sec: 4.444884112891984
epoch 1 iter 17280: train loss 1.09743. lr 3.000000e-04, running loss 1.11523, it/sec: 4.4612912514657825
epoch 1 iter 17300: train loss 1.12169. lr 3.000000e-04, running loss 1.11510, it/sec: 4.496458153118614
epoch 1 iter 17320: train loss 1.11034. lr 3.000000e-04, running loss 1.11513, it/sec: 4.49801761337252
epoch 1 iter 17340: train loss 1.11748. lr 3.000000e-04, running loss 1.11505, it/sec: 4.487300195723786
epoch 1 iter 17360: train loss 1.11946. lr 3.000000e-04, running loss 1.11506, it/sec: 4.450651262994229
epoch 1 iter 17380: train loss 1.11344. lr 3.000000e-04, running loss 1.11549, it/sec: 4.50590175966193
epoch 1 iter 17400: train loss 1.09722. lr 3.000000e-04, running loss 1.11578, it/sec: 4.478185962659082
epoch 1 iter 17420: train loss 1.12427. lr 3.000000e-04, running loss 1.11587, it/sec: 4.506760381973463
epoch 1 iter 17440: train loss 1.13175. lr 3.000000e-04, running loss 1.11576, it/sec: 4.46350679284837
epoch 1 iter 17460: train loss 1.11002. lr 3.000000e-04, running loss 1.11573, it/sec: 4.4777980494063785
epoch 1 iter 17480: train loss 1.09297. lr 3.000000e-04, running loss 1.11569, it/sec: 4.422241177026908
epoch 1 iter 17500: train loss 1.10650. lr 3.000000e-04, running loss 1.11554, it/sec: 4.504331554467143
epoch 1 iter 17520: train loss 1.09870. lr 3.000000e-04, running loss 1.11552, it/sec: 4.433744400210444
epoch 1 iter 17540: train loss 1.10390. lr 3.000000e-04, running loss 1.11552, it/sec: 4.503040853776505
epoch 1 iter 17560: train loss 1.09495. lr 3.000000e-04, running loss 1.11566, it/sec: 4.4724895374467755
epoch 1 iter 17580: train loss 1.10886. lr 3.000000e-04, running loss 1.11572, it/sec: 4.498749873911951
epoch 1 iter 17600: train loss 1.11989. lr 3.000000e-04, running loss 1.11574, it/sec: 4.454678126798376
epoch 1 iter 17620: train loss 1.12273. lr 3.000000e-04, running loss 1.11566, it/sec: 4.46634653519284
epoch 1 iter 17640: train loss 1.12299. lr 3.000000e-04, running loss 1.11581, it/sec: 4.454093653469392
epoch 1 iter 17660: train loss 1.11393. lr 3.000000e-04, running loss 1.11579, it/sec: 4.496617214784177
epoch 1 iter 17680: train loss 1.09462. lr 3.000000e-04, running loss 1.11569, it/sec: 4.4872089420441315
epoch 1 iter 17700: train loss 1.12997. lr 3.000000e-04, running loss 1.11576, it/sec: 4.471336692478624
epoch 1 iter 17720: train loss 1.13188. lr 3.000000e-04, running loss 1.11574, it/sec: 4.477130681562132
epoch 1 iter 17740: train loss 1.11650. lr 3.000000e-04, running loss 1.11567, it/sec: 4.469046445620416
epoch 1 iter 17760: train loss 1.11417. lr 3.000000e-04, running loss 1.11578, it/sec: 4.513076259797377
epoch 1 iter 17780: train loss 1.11036. lr 3.000000e-04, running loss 1.11563, it/sec: 4.463206713757131
epoch 1 iter 17800: train loss 1.10498. lr 3.000000e-04, running loss 1.11557, it/sec: 4.489233607771436
epoch 1 iter 17820: train loss 1.12586. lr 3.000000e-04, running loss 1.11561, it/sec: 4.47681884832595
epoch 1 iter 17840: train loss 1.10254. lr 3.000000e-04, running loss 1.11568, it/sec: 4.502919740684922
epoch 1 iter 17860: train loss 1.10962. lr 3.000000e-04, running loss 1.11566, it/sec: 4.504563408404076
epoch 1 iter 17880: train loss 1.10268. lr 3.000000e-04, running loss 1.11558, it/sec: 4.506635554086881
epoch 1 iter 17900: train loss 1.13378. lr 3.000000e-04, running loss 1.11551, it/sec: 4.464162670486975
epoch 1 iter 17920: train loss 1.11104. lr 3.000000e-04, running loss 1.11548, it/sec: 4.503494707833606
epoch 1 iter 17940: train loss 1.08430. lr 3.000000e-04, running loss 1.11539, it/sec: 4.462545042932921
epoch 1 iter 17960: train loss 1.12131. lr 3.000000e-04, running loss 1.11575, it/sec: 4.52238612100825
epoch 1 iter 17980: train loss 1.13795. lr 3.000000e-04, running loss 1.11559, it/sec: 4.477289402045087
epoch 1 iter 18000: train loss 1.10679. lr 3.000000e-04, running loss 1.11546, it/sec: 4.502856924723403
epoch 1 iter 18020: train loss 1.10289. lr 3.000000e-04, running loss 1.11541, it/sec: 4.45806420636412
epoch 1 iter 18040: train loss 1.09574. lr 3.000000e-04, running loss 1.11529, it/sec: 4.51273524584315
epoch 1 iter 18060: train loss 1.14670. lr 3.000000e-04, running loss 1.11521, it/sec: 4.449684609755026
epoch 1 iter 18080: train loss 1.13255. lr 3.000000e-04, running loss 1.11526, it/sec: 4.516395972169588
epoch 1 iter 18100: train loss 1.09253. lr 3.000000e-04, running loss 1.11514, it/sec: 4.447053323338604
epoch 1 iter 18120: train loss 1.10027. lr 3.000000e-04, running loss 1.11536, it/sec: 4.514576341888945
epoch 1 iter 18140: train loss 1.09710. lr 3.000000e-04, running loss 1.11524, it/sec: 4.494930383860143
epoch 1 iter 18160: train loss 1.11473. lr 3.000000e-04, running loss 1.11516, it/sec: 4.497013250177428
epoch 1 iter 18180: train loss 1.10940. lr 3.000000e-04, running loss 1.11511, it/sec: 4.457687440825192
epoch 1 iter 18200: train loss 1.11181. lr 3.000000e-04, running loss 1.11510, it/sec: 4.518588354453817
epoch 1 iter 18220: train loss 1.09933. lr 3.000000e-04, running loss 1.11518, it/sec: 4.469550425145781
epoch 1 iter 18240: train loss 1.10961. lr 3.000000e-04, running loss 1.11510, it/sec: 4.51798244075226
epoch 1 iter 18260: train loss 1.10442. lr 3.000000e-04, running loss 1.11512, it/sec: 4.444217355222942
epoch 1 iter 18280: train loss 1.11528. lr 3.000000e-04, running loss 1.11503, it/sec: 4.5355180905532535
epoch 1 iter 18300: train loss 1.12697. lr 3.000000e-04, running loss 1.11515, it/sec: 4.434233960433119
epoch 1 iter 18320: train loss 1.09040. lr 3.000000e-04, running loss 1.11512, it/sec: 4.500721344721511
epoch 1 iter 18340: train loss 1.11706. lr 3.000000e-04, running loss 1.11503, it/sec: 4.48782419183186
epoch 1 iter 18360: train loss 1.12593. lr 3.000000e-04, running loss 1.11496, it/sec: 4.502988863400191
epoch 1 iter 18380: train loss 1.09855. lr 3.000000e-04, running loss 1.11482, it/sec: 4.484142920729204
epoch 1 iter 18400: train loss 1.11586. lr 3.000000e-04, running loss 1.11470, it/sec: 4.492012622378609
epoch 1 iter 18420: train loss 1.10277. lr 3.000000e-04, running loss 1.11460, it/sec: 4.468674051779877
epoch 1 iter 18440: train loss 1.12438. lr 3.000000e-04, running loss 1.11449, it/sec: 4.466610026413718
epoch 1 iter 18460: train loss 1.11625. lr 3.000000e-04, running loss 1.11458, it/sec: 4.422403069366737
epoch 1 iter 18480: train loss 1.11101. lr 3.000000e-04, running loss 1.11470, it/sec: 4.5067500834229275
epoch 1 iter 18500: train loss 1.12073. lr 3.000000e-04, running loss 1.11469, it/sec: 4.493372700034697
epoch 1 iter 18520: train loss 1.11537. lr 3.000000e-04, running loss 1.11468, it/sec: 4.517336853764047
epoch 1 iter 18540: train loss 1.12891. lr 3.000000e-04, running loss 1.11474, it/sec: 4.471434860516499
epoch 1 iter 18560: train loss 1.14656. lr 3.000000e-04, running loss 1.11465, it/sec: 4.480478283463519
epoch 1 iter 18580: train loss 1.10348. lr 3.000000e-04, running loss 1.11464, it/sec: 4.488967621312082
epoch 1 iter 18600: train loss 1.10768. lr 3.000000e-04, running loss 1.11469, it/sec: 4.510025811216014
epoch 1 iter 18620: train loss 1.10325. lr 3.000000e-04, running loss 1.11463, it/sec: 4.494154507734326
epoch 1 iter 18640: train loss 1.19596. lr 3.000000e-04, running loss 1.11466, it/sec: 4.48992523186547
epoch 1 iter 18660: train loss 1.11823. lr 3.000000e-04, running loss 1.11481, it/sec: 4.494041788557623
epoch 1 iter 18680: train loss 1.10070. lr 3.000000e-04, running loss 1.11474, it/sec: 4.452021686877015
epoch 1 iter 18700: train loss 1.11931. lr 3.000000e-04, running loss 1.11464, it/sec: 4.492546338041492
epoch 1 iter 18720: train loss 1.09568. lr 3.000000e-04, running loss 1.11463, it/sec: 4.515328132392658
epoch 1 iter 18740: train loss 1.09807. lr 3.000000e-04, running loss 1.11452, it/sec: 4.434996325493667
epoch 1 iter 18760: train loss 1.11422. lr 3.000000e-04, running loss 1.11455, it/sec: 4.525917271173601
epoch 1 iter 18780: train loss 1.11409. lr 3.000000e-04, running loss 1.11456, it/sec: 4.463827056154451
epoch 1 iter 18800: train loss 1.13022. lr 3.000000e-04, running loss 1.11451, it/sec: 4.5061789971978605
epoch 1 iter 18820: train loss 1.12437. lr 3.000000e-04, running loss 1.11453, it/sec: 4.472601338745582
epoch 1 iter 18840: train loss 1.09598. lr 3.000000e-04, running loss 1.11456, it/sec: 4.455398269321701
epoch 1 iter 18860: train loss 1.12473. lr 3.000000e-04, running loss 1.11458, it/sec: 4.481337845394614
epoch 1 iter 18880: train loss 1.11193. lr 3.000000e-04, running loss 1.11460, it/sec: 4.4582797346649095
epoch 1 iter 18900: train loss 1.10789. lr 3.000000e-04, running loss 1.11454, it/sec: 4.497226007287562
epoch 1 iter 18920: train loss 1.15588. lr 3.000000e-04, running loss 1.11455, it/sec: 4.469159751320281
epoch 1 iter 18940: train loss 1.13031. lr 3.000000e-04, running loss 1.11448, it/sec: 4.484647675898796
epoch 1 iter 18960: train loss 1.10422. lr 3.000000e-04, running loss 1.11446, it/sec: 4.460529072512267
epoch 1 iter 18980: train loss 1.09704. lr 3.000000e-04, running loss 1.11441, it/sec: 4.511950727727792
epoch 1 iter 19000: train loss 1.12200. lr 3.000000e-04, running loss 1.11442, it/sec: 4.4422120448358235
epoch 1 iter 19020: train loss 1.12260. lr 3.000000e-04, running loss 1.11441, it/sec: 4.498552170415095
epoch 1 iter 19040: train loss 1.11965. lr 3.000000e-04, running loss 1.11439, it/sec: 4.442564922793467
epoch 1 iter 19060: train loss 1.09084. lr 3.000000e-04, running loss 1.11435, it/sec: 4.5237191654611255
epoch 1 iter 19080: train loss 1.12722. lr 3.000000e-04, running loss 1.11451, it/sec: 4.435604088401363
epoch 1 iter 19100: train loss 1.12367. lr 3.000000e-04, running loss 1.11444, it/sec: 4.513339284495842
epoch 1 iter 19120: train loss 1.10811. lr 3.000000e-04, running loss 1.11450, it/sec: 4.415977736656612
epoch 1 iter 19140: train loss 1.10531. lr 3.000000e-04, running loss 1.11445, it/sec: 4.467617180245857
epoch 1 iter 19160: train loss 1.11490. lr 3.000000e-04, running loss 1.11446, it/sec: 4.515154002718155
epoch 1 iter 19180: train loss 1.09770. lr 3.000000e-04, running loss 1.11440, it/sec: 4.480875434725821
epoch 1 iter 19200: train loss 1.11793. lr 3.000000e-04, running loss 1.11433, it/sec: 4.5064430259383395
epoch 1 iter 19220: train loss 1.10910. lr 3.000000e-04, running loss 1.11427, it/sec: 4.478483045180778
epoch 1 iter 19240: train loss 1.09171. lr 3.000000e-04, running loss 1.11413, it/sec: 4.506901160347449
epoch 1 iter 19260: train loss 1.12698. lr 3.000000e-04, running loss 1.11415, it/sec: 4.482082201359701
epoch 1 iter 19280: train loss 1.11115. lr 3.000000e-04, running loss 1.11405, it/sec: 4.505490516996365
epoch 1 iter 19300: train loss 1.10520. lr 3.000000e-04, running loss 1.11410, it/sec: 4.46651478390213
epoch 1 iter 19320: train loss 1.14403. lr 3.000000e-04, running loss 1.11402, it/sec: 4.526601065916221
epoch 1 iter 19340: train loss 1.08876. lr 3.000000e-04, running loss 1.11407, it/sec: 4.478342772248562
epoch 1 iter 19360: train loss 1.13128. lr 3.000000e-04, running loss 1.11409, it/sec: 4.491365176858365
epoch 1 iter 19380: train loss 1.11023. lr 3.000000e-04, running loss 1.11406, it/sec: 4.457616285276907
epoch 1 iter 19400: train loss 1.12291. lr 3.000000e-04, running loss 1.11411, it/sec: 4.519437172798955
epoch 1 iter 19420: train loss 1.10824. lr 3.000000e-04, running loss 1.11408, it/sec: 4.430877315065399
epoch 1 iter 19440: train loss 1.10892. lr 3.000000e-04, running loss 1.11416, it/sec: 4.51871941840139
epoch 1 iter 19460: train loss 1.11676. lr 3.000000e-04, running loss 1.11430, it/sec: 4.463185558614391
epoch 1 iter 19480: train loss 1.11442. lr 3.000000e-04, running loss 1.11415, it/sec: 4.5003408372707465
epoch 1 iter 19500: train loss 1.11694. lr 3.000000e-04, running loss 1.11434, it/sec: 4.447383789311296
epoch 1 iter 19520: train loss 1.10892. lr 3.000000e-04, running loss 1.11442, it/sec: 4.507995256932565
epoch 1 iter 19540: train loss 1.10168. lr 3.000000e-04, running loss 1.11432, it/sec: 4.4552932420512485
epoch 1 iter 19560: train loss 1.12266. lr 3.000000e-04, running loss 1.11438, it/sec: 4.491616720364654
epoch 1 iter 19580: train loss 1.12287. lr 3.000000e-04, running loss 1.11439, it/sec: 4.492511219911241
epoch 1 iter 19600: train loss 1.09976. lr 3.000000e-04, running loss 1.11438, it/sec: 4.488655164408237
epoch 1 iter 19620: train loss 1.12015. lr 3.000000e-04, running loss 1.11432, it/sec: 4.433834770451121
epoch 1 iter 19640: train loss 1.17019. lr 3.000000e-04, running loss 1.11444, it/sec: 4.498069530237225
epoch 1 iter 19660: train loss 1.11534. lr 3.000000e-04, running loss 1.11434, it/sec: 4.4683574054838955
epoch 1 iter 19680: train loss 1.09566. lr 3.000000e-04, running loss 1.11430, it/sec: 4.510155505089961
epoch 1 iter 19700: train loss 1.10118. lr 3.000000e-04, running loss 1.11423, it/sec: 4.483791709717311
epoch 1 iter 19720: train loss 1.11887. lr 3.000000e-04, running loss 1.11430, it/sec: 4.500322489199734
epoch 1 iter 19740: train loss 1.13023. lr 3.000000e-04, running loss 1.11428, it/sec: 4.467075482642885
epoch 1 iter 19760: train loss 1.10282. lr 3.000000e-04, running loss 1.11424, it/sec: 4.501168530457241
epoch 1 iter 19780: train loss 1.11440. lr 3.000000e-04, running loss 1.11420, it/sec: 4.456197596182757
epoch 1 iter 19800: train loss 1.11708. lr 3.000000e-04, running loss 1.11446, it/sec: 4.4584931166296755
epoch 1 iter 19820: train loss 1.13530. lr 3.000000e-04, running loss 1.11456, it/sec: 4.507374788494747
epoch 1 iter 19840: train loss 1.12899. lr 3.000000e-04, running loss 1.11450, it/sec: 4.454892950062459
epoch 1 iter 19860: train loss 1.12653. lr 3.000000e-04, running loss 1.11444, it/sec: 4.492582465893105
epoch 1 iter 19880: train loss 1.09521. lr 3.000000e-04, running loss 1.11447, it/sec: 4.437214014181323
epoch 1 iter 19900: train loss 1.10378. lr 3.000000e-04, running loss 1.11441, it/sec: 4.489952668074384
epoch 1 iter 19920: train loss 1.09941. lr 3.000000e-04, running loss 1.11433, it/sec: 4.463161336830956
epoch 1 iter 19940: train loss 1.10269. lr 3.000000e-04, running loss 1.11431, it/sec: 4.4844414567128945
epoch 1 iter 19960: train loss 1.15641. lr 3.000000e-04, running loss 1.11435, it/sec: 4.463594275051406
epoch 1 iter 19980: train loss 1.12494. lr 3.000000e-04, running loss 1.11431, it/sec: 4.490265831427789
epoch 1 iter 20000: train loss 1.11938. lr 3.000000e-04, running loss 1.11462, it/sec: 4.44161801575847
epoch 1 iter 20020: train loss 1.10819. lr 3.000000e-04, running loss 1.11461, it/sec: 4.523690228708518
epoch 1 iter 20040: train loss 1.11436. lr 3.000000e-04, running loss 1.11471, it/sec: 4.452572505988143
epoch 1 iter 20060: train loss 1.11875. lr 3.000000e-04, running loss 1.11471, it/sec: 4.4675608753921106
epoch 1 iter 20080: train loss 1.09827. lr 3.000000e-04, running loss 1.11466, it/sec: 4.450500546336131
epoch 1 iter 20100: train loss 1.11712. lr 3.000000e-04, running loss 1.11473, it/sec: 4.519045878256005
epoch 1 iter 20120: train loss 1.10405. lr 3.000000e-04, running loss 1.11473, it/sec: 4.478941070239897
epoch 1 iter 20140: train loss 1.11143. lr 3.000000e-04, running loss 1.11465, it/sec: 4.500151276042109
epoch 1 iter 20160: train loss 1.12761. lr 3.000000e-04, running loss 1.11457, it/sec: 4.456099024605694
epoch 1 iter 20180: train loss 1.12694. lr 3.000000e-04, running loss 1.11466, it/sec: 4.49806639404019
epoch 1 iter 20200: train loss 1.10829. lr 3.000000e-04, running loss 1.11468, it/sec: 4.475148077486676
epoch 1 iter 20220: train loss 1.10283. lr 3.000000e-04, running loss 1.11467, it/sec: 4.476479224178365
epoch 1 iter 20240: train loss 1.12843. lr 3.000000e-04, running loss 1.11468, it/sec: 4.464785551753084
epoch 1 iter 20260: train loss 1.10759. lr 3.000000e-04, running loss 1.11465, it/sec: 4.5267612642553905
epoch 1 iter 20280: train loss 1.11799. lr 3.000000e-04, running loss 1.11463, it/sec: 4.476440228914014
epoch 1 iter 20300: train loss 1.12778. lr 3.000000e-04, running loss 1.11462, it/sec: 4.508488118847461
epoch 1 iter 20320: train loss 1.12268. lr 3.000000e-04, running loss 1.11453, it/sec: 4.440355409880141
epoch 1 iter 20340: train loss 1.13792. lr 3.000000e-04, running loss 1.11460, it/sec: 4.534418738214793
epoch 1 iter 20360: train loss 1.14687. lr 3.000000e-04, running loss 1.11480, it/sec: 4.4687003303785255
epoch 1 iter 20380: train loss 1.10123. lr 3.000000e-04, running loss 1.11511, it/sec: 4.521382315613341
epoch 1 iter 20400: train loss 1.11384. lr 3.000000e-04, running loss 1.11511, it/sec: 4.451603494337697
epoch 1 iter 20420: train loss 1.11280. lr 3.000000e-04, running loss 1.11526, it/sec: 4.505954955422458
epoch 1 iter 20440: train loss 1.09850. lr 3.000000e-04, running loss 1.11529, it/sec: 4.4664944357872205
epoch 1 iter 20460: train loss 1.09656. lr 3.000000e-04, running loss 1.11521, it/sec: 4.512849332663457
epoch 1 iter 20480: train loss 1.10250. lr 3.000000e-04, running loss 1.11520, it/sec: 4.4854868893567
epoch 1 iter 20500: train loss 1.11988. lr 3.000000e-04, running loss 1.11514, it/sec: 4.519206991402186
epoch 1 iter 20520: train loss 1.11369. lr 3.000000e-04, running loss 1.11508, it/sec: 4.436661101312414
epoch 1 iter 20540: train loss 1.12884. lr 3.000000e-04, running loss 1.11510, it/sec: 4.5263684142805545
epoch 1 iter 20560: train loss 1.13403. lr 3.000000e-04, running loss 1.11513, it/sec: 4.5006512575789825
epoch 1 iter 20580: train loss 1.10198. lr 3.000000e-04, running loss 1.11517, it/sec: 4.514627560913735
epoch 1 iter 20600: train loss 1.10483. lr 3.000000e-04, running loss 1.11512, it/sec: 4.473617360205694
epoch 1 iter 20620: train loss 1.09370. lr 3.000000e-04, running loss 1.11508, it/sec: 4.5103779102859125
epoch 1 iter 20640: train loss 1.10384. lr 3.000000e-04, running loss 1.11504, it/sec: 4.456181134341684
epoch 1 iter 20660: train loss 1.09317. lr 3.000000e-04, running loss 1.11490, it/sec: 4.52484388203791
epoch 1 iter 20680: train loss 1.24907. lr 3.000000e-04, running loss 1.11507, it/sec: 4.459027923571286
epoch 1 iter 20700: train loss 1.11908. lr 3.000000e-04, running loss 1.11506, it/sec: 4.498767198386705
epoch 1 iter 20720: train loss 1.12420. lr 3.000000e-04, running loss 1.11496, it/sec: 4.460529152422425
epoch 1 iter 20740: train loss 1.10565. lr 3.000000e-04, running loss 1.11495, it/sec: 4.5409116522750095
epoch 1 iter 20760: train loss 1.13358. lr 3.000000e-04, running loss 1.11504, it/sec: 4.4489990518714615
epoch 1 iter 20780: train loss 1.18382. lr 3.000000e-04, running loss 1.11502, it/sec: 4.514325377992551
epoch 1 iter 20800: train loss 1.12298. lr 3.000000e-04, running loss 1.11499, it/sec: 4.44356353319494
epoch 1 iter 20820: train loss 1.11437. lr 3.000000e-04, running loss 1.11530, it/sec: 4.505784817785914
epoch 1 iter 20840: train loss 1.11803. lr 3.000000e-04, running loss 1.11520, it/sec: 4.461604688605284
epoch 1 iter 20860: train loss 1.12833. lr 3.000000e-04, running loss 1.11513, it/sec: 4.5149568743357005
epoch 1 iter 20880: train loss 1.08601. lr 3.000000e-04, running loss 1.11528, it/sec: 4.441698230735834
epoch 1 iter 20900: train loss 1.18337. lr 3.000000e-04, running loss 1.11533, it/sec: 4.501131352784232
epoch 1 iter 20920: train loss 1.10292. lr 3.000000e-04, running loss 1.11534, it/sec: 4.469045227273276
epoch 1 iter 20940: train loss 1.10877. lr 3.000000e-04, running loss 1.11532, it/sec: 4.4896004666544025
epoch 1 iter 20960: train loss 1.10569. lr 3.000000e-04, running loss 1.11522, it/sec: 4.443285339204068
epoch 1 iter 20980: train loss 1.11987. lr 3.000000e-04, running loss 1.11521, it/sec: 4.537896452409966
epoch 1 iter 21000: train loss 1.11793. lr 3.000000e-04, running loss 1.11535, it/sec: 4.447758934807991
epoch 1 iter 21020: train loss 1.09935. lr 3.000000e-04, running loss 1.11544, it/sec: 4.52115324364496
epoch 1 iter 21040: train loss 1.10303. lr 3.000000e-04, running loss 1.11538, it/sec: 4.458506137077919
epoch 1 iter 21060: train loss 1.11096. lr 3.000000e-04, running loss 1.11548, it/sec: 4.496528008513044
epoch 1 iter 21080: train loss 1.09451. lr 3.000000e-04, running loss 1.11546, it/sec: 4.465992920267468
epoch 1 iter 21100: train loss 1.11893. lr 3.000000e-04, running loss 1.11562, it/sec: 4.493513531785597
epoch 1 iter 21120: train loss 1.11429. lr 3.000000e-04, running loss 1.11554, it/sec: 4.456803041119547
epoch 1 iter 21140: train loss 1.08652. lr 3.000000e-04, running loss 1.11549, it/sec: 4.514287189094223
epoch 1 iter 21160: train loss 1.11354. lr 3.000000e-04, running loss 1.11553, it/sec: 4.4581699207856165
epoch 1 iter 21180: train loss 1.12419. lr 3.000000e-04, running loss 1.11548, it/sec: 4.520209933303268
epoch 1 iter 21200: train loss 1.39568. lr 3.000000e-04, running loss 1.11566, it/sec: 4.440945886934522
epoch 1 iter 21220: train loss 1.07417. lr 3.000000e-04, running loss 1.11556, it/sec: 4.5148592930903835
epoch 1 iter 21240: train loss 1.13085. lr 3.000000e-04, running loss 1.11560, it/sec: 4.4815650479732
epoch 1 iter 21260: train loss 1.11368. lr 3.000000e-04, running loss 1.11556, it/sec: 4.505143707669517
epoch 1 iter 21280: train loss 1.10256. lr 3.000000e-04, running loss 1.11555, it/sec: 4.51428146212002
epoch 1 iter 21300: train loss 1.12007. lr 3.000000e-04, running loss 1.11555, it/sec: 4.508749715616582
epoch 1 iter 21320: train loss 1.09853. lr 3.000000e-04, running loss 1.11541, it/sec: 4.484453945543919
epoch 1 iter 21340: train loss 1.11346. lr 3.000000e-04, running loss 1.11562, it/sec: 4.467651151589359
epoch 1 iter 21360: train loss 1.11269. lr 3.000000e-04, running loss 1.11559, it/sec: 4.515345176542429
epoch 1 iter 21380: train loss 1.10756. lr 3.000000e-04, running loss 1.11553, it/sec: 4.464929560257998
epoch 1 iter 21400: train loss 1.12939. lr 3.000000e-04, running loss 1.11557, it/sec: 4.497387672217411
epoch 1 iter 21420: train loss 1.12906. lr 3.000000e-04, running loss 1.11544, it/sec: 4.47121697971683
epoch 1 iter 21440: train loss 1.10321. lr 3.000000e-04, running loss 1.11530, it/sec: 4.473090433685401
epoch 1 iter 21460: train loss 1.12167. lr 3.000000e-04, running loss 1.11518, it/sec: 4.517420378183275
epoch 1 iter 21480: train loss 1.10401. lr 3.000000e-04, running loss 1.11518, it/sec: 4.48676512907497
epoch 1 iter 21500: train loss 1.10673. lr 3.000000e-04, running loss 1.11518, it/sec: 4.460294426941648
epoch 1 iter 21520: train loss 1.10920. lr 3.000000e-04, running loss 1.11512, it/sec: 4.464956893751224
epoch 1 iter 21540: train loss 1.11322. lr 3.000000e-04, running loss 1.11504, it/sec: 4.495214395731141
epoch 1 iter 21560: train loss 1.12474. lr 3.000000e-04, running loss 1.11505, it/sec: 4.513065749785177
epoch 1 iter 21580: train loss 1.11905. lr 3.000000e-04, running loss 1.11504, it/sec: 4.452700801091268
epoch 1 iter 21600: train loss 1.10591. lr 3.000000e-04, running loss 1.11512, it/sec: 4.5222692825922515
epoch 1 iter 21620: train loss 1.12039. lr 3.000000e-04, running loss 1.11511, it/sec: 4.4626878126173795
epoch 1 iter 21640: train loss 1.10887. lr 3.000000e-04, running loss 1.11505, it/sec: 4.529540602518568
epoch 1 iter 21660: train loss 1.13940. lr 3.000000e-04, running loss 1.11497, it/sec: 4.459174823644042
epoch 1 iter 21680: train loss 1.13032. lr 3.000000e-04, running loss 1.11492, it/sec: 4.515948120462354
epoch 1 iter 21700: train loss 1.12101. lr 3.000000e-04, running loss 1.11489, it/sec: 4.470397865488775
epoch 1 iter 21720: train loss 1.10224. lr 3.000000e-04, running loss 1.11485, it/sec: 4.520072100508491
epoch 1 iter 21740: train loss 1.14049. lr 3.000000e-04, running loss 1.11489, it/sec: 4.468757524129436
epoch 1 iter 21760: train loss 1.10013. lr 3.000000e-04, running loss 1.11483, it/sec: 4.484632512180644
epoch 1 iter 21780: train loss 1.13433. lr 3.000000e-04, running loss 1.11486, it/sec: 4.439992158469623
epoch 1 iter 21800: train loss 1.10144. lr 3.000000e-04, running loss 1.11479, it/sec: 4.519537708046908
epoch 1 iter 21820: train loss 1.11332. lr 3.000000e-04, running loss 1.11487, it/sec: 4.475784644037494
epoch 1 iter 21840: train loss 1.12405. lr 3.000000e-04, running loss 1.11492, it/sec: 4.510359580932464
epoch 1 iter 21860: train loss 1.11140. lr 3.000000e-04, running loss 1.11497, it/sec: 4.436156107804451
epoch 1 iter 21880: train loss 1.10916. lr 3.000000e-04, running loss 1.11490, it/sec: 4.510251092777841
epoch 1 iter 21900: train loss 1.11321. lr 3.000000e-04, running loss 1.11493, it/sec: 4.465522982903192
epoch 1 iter 21920: train loss 1.08905. lr 3.000000e-04, running loss 1.11490, it/sec: 4.525074863291371
epoch 1 iter 21940: train loss 1.10389. lr 3.000000e-04, running loss 1.11506, it/sec: 4.4528388571051485
epoch 1 iter 21960: train loss 1.11336. lr 3.000000e-04, running loss 1.11508, it/sec: 4.497934966231481
epoch 1 iter 21980: train loss 1.10733. lr 3.000000e-04, running loss 1.11515, it/sec: 4.48496826626228
epoch 1 iter 22000: train loss 1.11414. lr 3.000000e-04, running loss 1.11515, it/sec: 4.49274012320919
epoch 1 iter 22020: train loss 1.11951. lr 3.000000e-04, running loss 1.11657, it/sec: 4.435001085983338
epoch 1 iter 22040: train loss 1.09410. lr 3.000000e-04, running loss 1.11649, it/sec: 4.5272710705826675
epoch 1 iter 22060: train loss 1.10001. lr 3.000000e-04, running loss 1.11654, it/sec: 4.456115467346879
epoch 1 iter 22080: train loss 1.11356. lr 3.000000e-04, running loss 1.11654, it/sec: 4.527688205941279
epoch 1 iter 22100: train loss 1.14903. lr 3.000000e-04, running loss 1.11649, it/sec: 4.462645692048652
epoch 1 iter 22120: train loss 1.10202. lr 3.000000e-04, running loss 1.11646, it/sec: 4.515932192350702
epoch 1 iter 22140: train loss 1.10657. lr 3.000000e-04, running loss 1.11640, it/sec: 4.455964300642046
epoch 1 iter 22160: train loss 1.11306. lr 3.000000e-04, running loss 1.11634, it/sec: 4.515881168069831
epoch 1 iter 22180: train loss 1.09185. lr 3.000000e-04, running loss 1.11626, it/sec: 4.470178387558096
epoch 1 iter 22200: train loss 1.12020. lr 3.000000e-04, running loss 1.11626, it/sec: 4.488961374281229
epoch 1 iter 22220: train loss 1.11694. lr 3.000000e-04, running loss 1.11636, it/sec: 4.492805219762649
epoch 1 iter 22240: train loss 1.13050. lr 3.000000e-04, running loss 1.11639, it/sec: 4.530476952789053
epoch 1 iter 22260: train loss 1.12036. lr 3.000000e-04, running loss 1.11646, it/sec: 4.4757987276771
epoch 1 iter 22280: train loss 1.10863. lr 3.000000e-04, running loss 1.11646, it/sec: 4.499456276639648
epoch 1 iter 22300: train loss 1.10493. lr 3.000000e-04, running loss 1.11635, it/sec: 4.501348755056923
epoch 1 iter 22320: train loss 1.10851. lr 3.000000e-04, running loss 1.11630, it/sec: 4.486027526877675
epoch 1 iter 22340: train loss 1.10078. lr 3.000000e-04, running loss 1.11619, it/sec: 4.449531069453617
epoch 1 iter 22360: train loss 1.10435. lr 3.000000e-04, running loss 1.11606, it/sec: 4.522229934256492
epoch 1 iter 22380: train loss 1.10453. lr 3.000000e-04, running loss 1.11601, it/sec: 4.420666875361643
epoch 1 iter 22400: train loss 1.08955. lr 3.000000e-04, running loss 1.11599, it/sec: 4.519978202152321
epoch 1 iter 22420: train loss 1.15387. lr 3.000000e-04, running loss 1.11607, it/sec: 4.469244760234584
epoch 1 iter 22440: train loss 1.11965. lr 3.000000e-04, running loss 1.11594, it/sec: 4.486320819285382
epoch 1 iter 22460: train loss 1.10639. lr 3.000000e-04, running loss 1.11584, it/sec: 4.443106516551344
epoch 1 iter 22480: train loss 1.10381. lr 3.000000e-04, running loss 1.11568, it/sec: 4.526094587055972
epoch 1 iter 22500: train loss 1.08950. lr 3.000000e-04, running loss 1.11570, it/sec: 4.49273499592972
epoch 1 iter 22520: train loss 1.12456. lr 3.000000e-04, running loss 1.11579, it/sec: 4.476414279103816
epoch 1 iter 22540: train loss 1.09534. lr 3.000000e-04, running loss 1.11580, it/sec: 4.494863730992525
epoch 1 iter 22560: train loss 1.11585. lr 3.000000e-04, running loss 1.11569, it/sec: 4.465776406442758
epoch 1 iter 22580: train loss 1.11134. lr 3.000000e-04, running loss 1.11568, it/sec: 4.498520721811747
epoch 1 iter 22600: train loss 1.10464. lr 3.000000e-04, running loss 1.11565, it/sec: 4.480155163798255
epoch 1 iter 22620: train loss 1.10711. lr 3.000000e-04, running loss 1.11555, it/sec: 4.467361032735046
epoch 1 iter 22640: train loss 1.14311. lr 3.000000e-04, running loss 1.11552, it/sec: 4.507842968778789
epoch 1 iter 22660: train loss 1.12134. lr 3.000000e-04, running loss 1.11559, it/sec: 4.458116516907454
epoch 1 iter 22680: train loss 1.11704. lr 3.000000e-04, running loss 1.11630, it/sec: 4.4968366082767695
epoch 1 iter 22700: train loss 1.11011. lr 3.000000e-04, running loss 1.11630, it/sec: 4.457547971379562
epoch 1 iter 22720: train loss 1.10349. lr 3.000000e-04, running loss 1.11617, it/sec: 4.488757699635096
epoch 1 iter 22740: train loss 1.10074. lr 3.000000e-04, running loss 1.11620, it/sec: 4.484462713168604
epoch 1 iter 22760: train loss 1.11523. lr 3.000000e-04, running loss 1.11621, it/sec: 4.478264216660838
epoch 1 iter 22780: train loss 1.11081. lr 3.000000e-04, running loss 1.11618, it/sec: 4.510530939380057
epoch 1 iter 22800: train loss 1.12749. lr 3.000000e-04, running loss 1.11610, it/sec: 4.460175065026816
epoch 1 iter 22820: train loss 1.10415. lr 3.000000e-04, running loss 1.11592, it/sec: 4.510676715504494
epoch 1 iter 22840: train loss 1.12332. lr 3.000000e-04, running loss 1.11593, it/sec: 4.462988041060496
epoch 1 iter 22860: train loss 1.09419. lr 3.000000e-04, running loss 1.11579, it/sec: 4.4940539864698
epoch 1 iter 22880: train loss 1.12252. lr 3.000000e-04, running loss 1.11569, it/sec: 4.465289925080793
epoch 1 iter 22900: train loss 1.13092. lr 3.000000e-04, running loss 1.11568, it/sec: 4.5020568048145675
epoch 1 iter 22920: train loss 1.10402. lr 3.000000e-04, running loss 1.11552, it/sec: 4.509582842931386
epoch 1 iter 22940: train loss 1.13506. lr 3.000000e-04, running loss 1.11546, it/sec: 4.45339973601637
epoch 1 iter 22960: train loss 1.11862. lr 3.000000e-04, running loss 1.11531, it/sec: 4.50735902368626
epoch 1 iter 22980: train loss 1.11224. lr 3.000000e-04, running loss 1.11523, it/sec: 4.463627728309817
epoch 1 iter 23000: train loss 1.11478. lr 3.000000e-04, running loss 1.11524, it/sec: 4.510413797362781
epoch 1 iter 23020: train loss 1.11780. lr 3.000000e-04, running loss 1.11512, it/sec: 4.456791043428377
epoch 1 iter 23040: train loss 1.12372. lr 3.000000e-04, running loss 1.11504, it/sec: 4.544043434030847
epoch 1 iter 23060: train loss 1.12720. lr 3.000000e-04, running loss 1.11512, it/sec: 4.445530694535993
epoch 1 iter 23080: train loss 1.11866. lr 3.000000e-04, running loss 1.11516, it/sec: 4.515913553112758
epoch 1 iter 23100: train loss 1.11338. lr 3.000000e-04, running loss 1.11511, it/sec: 4.473231278103698
epoch 1 iter 23120: train loss 1.09854. lr 3.000000e-04, running loss 1.11503, it/sec: 4.5260031008710575
epoch 1 iter 23140: train loss 1.09547. lr 3.000000e-04, running loss 1.11516, it/sec: 4.444943996938912
epoch 1 iter 23160: train loss 1.12655. lr 3.000000e-04, running loss 1.11513, it/sec: 4.511527326462259
epoch 1 iter 23180: train loss 1.11291. lr 3.000000e-04, running loss 1.11507, it/sec: 4.444488356622718
epoch 1 iter 23200: train loss 1.12678. lr 3.000000e-04, running loss 1.11503, it/sec: 4.526324242318918
epoch 1 iter 23220: train loss 1.11427. lr 3.000000e-04, running loss 1.11497, it/sec: 4.448541788514505
epoch 1 iter 23240: train loss 1.11999. lr 3.000000e-04, running loss 1.11499, it/sec: 4.510945057075358
epoch 1 iter 23260: train loss 1.11227. lr 3.000000e-04, running loss 1.11492, it/sec: 4.420035104289009
epoch 1 iter 23280: train loss 1.19436. lr 3.000000e-04, running loss 1.11485, it/sec: 4.525973890459959
epoch 1 iter 23300: train loss 1.11810. lr 3.000000e-04, running loss 1.11488, it/sec: 4.465596825195789
epoch 1 iter 23320: train loss 1.18976. lr 3.000000e-04, running loss 1.11502, it/sec: 4.503499473664958
epoch 1 iter 23340: train loss 1.12192. lr 3.000000e-04, running loss 1.11488, it/sec: 4.4706083919737
epoch 1 iter 23360: train loss 1.11990. lr 3.000000e-04, running loss 1.11484, it/sec: 4.4999925123535816
epoch 1 iter 23380: train loss 1.17104. lr 3.000000e-04, running loss 1.11492, it/sec: 4.472856445573087
epoch 1 iter 23400: train loss 1.13463. lr 3.000000e-04, running loss 1.11491, it/sec: 4.52160672793843
epoch 1 iter 23420: train loss 1.12037. lr 3.000000e-04, running loss 1.11489, it/sec: 4.459960030764374
epoch 1 iter 23440: train loss 1.11817. lr 3.000000e-04, running loss 1.11484, it/sec: 4.507660313688806
epoch 1 iter 23460: train loss 1.11715. lr 3.000000e-04, running loss 1.11507, it/sec: 4.478608322982336
epoch 1 iter 23480: train loss 1.10893. lr 3.000000e-04, running loss 1.11492, it/sec: 4.51355039109722
epoch 1 iter 23500: train loss 1.10339. lr 3.000000e-04, running loss 1.11487, it/sec: 4.462204036765126
epoch 1 iter 23520: train loss 1.10721. lr 3.000000e-04, running loss 1.11481, it/sec: 4.527670779715995
epoch 1 iter 23540: train loss 1.12015. lr 3.000000e-04, running loss 1.11479, it/sec: 4.426493811416424
epoch 1 iter 23560: train loss 1.11291. lr 3.000000e-04, running loss 1.11467, it/sec: 4.498218527600601
epoch 1 iter 23580: train loss 1.11626. lr 3.000000e-04, running loss 1.11460, it/sec: 4.46644577896257
epoch 1 iter 23600: train loss 1.09421. lr 3.000000e-04, running loss 1.11455, it/sec: 4.509738970569676
epoch 1 iter 23620: train loss 1.12803. lr 3.000000e-04, running loss 1.11458, it/sec: 4.449678513071052
epoch 1 iter 23640: train loss 1.12493. lr 3.000000e-04, running loss 1.11494, it/sec: 4.503966361532901
epoch 1 iter 23660: train loss 1.11277. lr 3.000000e-04, running loss 1.11497, it/sec: 4.445761752946796
epoch 1 iter 23680: train loss 1.11476. lr 3.000000e-04, running loss 1.11495, it/sec: 4.5203180435480075
epoch 1 iter 23700: train loss 1.09190. lr 3.000000e-04, running loss 1.11489, it/sec: 4.481439162822768
epoch 1 iter 23720: train loss 1.11096. lr 3.000000e-04, running loss 1.11490, it/sec: 4.520848450835855
epoch 1 iter 23740: train loss 1.10754. lr 3.000000e-04, running loss 1.11491, it/sec: 4.407465360203123
epoch 1 iter 23760: train loss 1.13651. lr 3.000000e-04, running loss 1.11486, it/sec: 4.499084790276862
epoch 1 iter 23780: train loss 1.11830. lr 3.000000e-04, running loss 1.11496, it/sec: 4.499333311840778
epoch 1 iter 23800: train loss 1.10276. lr 3.000000e-04, running loss 1.11482, it/sec: 4.494213362461932
epoch 1 iter 23820: train loss 1.12278. lr 3.000000e-04, running loss 1.11486, it/sec: 4.4430548943361226
epoch 1 iter 23840: train loss 1.11489. lr 3.000000e-04, running loss 1.11494, it/sec: 4.498119302493364
epoch 1 iter 23860: train loss 1.09076. lr 3.000000e-04, running loss 1.11482, it/sec: 4.470487397866006
epoch 1 iter 23880: train loss 1.11301. lr 3.000000e-04, running loss 1.11477, it/sec: 4.525499436840771
epoch 1 iter 23900: train loss 1.11128. lr 3.000000e-04, running loss 1.11473, it/sec: 4.488694010385851
epoch 1 iter 23920: train loss 1.07821. lr 3.000000e-04, running loss 1.11468, it/sec: 4.507848678245619
epoch 1 iter 23940: train loss 1.12972. lr 3.000000e-04, running loss 1.11461, it/sec: 4.492694909795963
epoch 1 iter 23960: train loss 1.12551. lr 3.000000e-04, running loss 1.11471, it/sec: 4.511401746280225
epoch 1 iter 23980: train loss 1.13466. lr 3.000000e-04, running loss 1.11461, it/sec: 4.4691880341727925
epoch 1 iter 24000: train loss 1.13015. lr 3.000000e-04, running loss 1.11458, it/sec: 4.520605352514907
epoch 1 iter 24020: train loss 1.12939. lr 3.000000e-04, running loss 1.11445, it/sec: 4.48728988629462
epoch 1 iter 24040: train loss 1.12420. lr 3.000000e-04, running loss 1.11459, it/sec: 4.539256871140638
epoch 1 iter 24060: train loss 1.12024. lr 3.000000e-04, running loss 1.11473, it/sec: 4.47450274180043
epoch 1 iter 24080: train loss 1.10280. lr 3.000000e-04, running loss 1.11471, it/sec: 4.52039908294076
epoch 1 iter 24100: train loss 1.10346. lr 3.000000e-04, running loss 1.11460, it/sec: 4.465462962085984
epoch 1 iter 24120: train loss 1.11258. lr 3.000000e-04, running loss 1.11458, it/sec: 4.496313052146851
epoch 1 iter 24140: train loss 1.11313. lr 3.000000e-04, running loss 1.11449, it/sec: 4.466747211242753
epoch 1 iter 24160: train loss 1.12576. lr 3.000000e-04, running loss 1.11461, it/sec: 4.4886188381695
epoch 1 iter 24180: train loss 1.10809. lr 3.000000e-04, running loss 1.11455, it/sec: 4.497559117859399
epoch 1 iter 24200: train loss 1.11223. lr 3.000000e-04, running loss 1.11461, it/sec: 4.509877902360947
epoch 1 iter 24220: train loss 1.10789. lr 3.000000e-04, running loss 1.11488, it/sec: 4.498387163792716
epoch 1 iter 24240: train loss 1.14811. lr 3.000000e-04, running loss 1.11485, it/sec: 4.5094625163720785
epoch 1 iter 24260: train loss 1.11008. lr 3.000000e-04, running loss 1.11484, it/sec: 4.451701965176067
epoch 1 iter 24280: train loss 1.11465. lr 3.000000e-04, running loss 1.11474, it/sec: 4.502835656398884
epoch 1 iter 24300: train loss 1.11666. lr 3.000000e-04, running loss 1.11466, it/sec: 4.4855978305990245
epoch 1 iter 24320: train loss 1.10228. lr 3.000000e-04, running loss 1.11471, it/sec: 4.515489693839732
epoch 1 iter 24340: train loss 1.09604. lr 3.000000e-04, running loss 1.11471, it/sec: 4.5027548607848
epoch 1 iter 24360: train loss 1.09878. lr 3.000000e-04, running loss 1.11468, it/sec: 4.491972609034102
epoch 1 iter 24380: train loss 1.10916. lr 3.000000e-04, running loss 1.11472, it/sec: 4.48087657889091
epoch 1 iter 24400: train loss 1.12104. lr 3.000000e-04, running loss 1.11468, it/sec: 4.523165864983824
epoch 1 iter 24420: train loss 1.11485. lr 3.000000e-04, running loss 1.11491, it/sec: 4.491235775900618
epoch 1 iter 24440: train loss 1.11434. lr 3.000000e-04, running loss 1.11528, it/sec: 4.474221082298684
epoch 1 iter 24460: train loss 1.10732. lr 3.000000e-04, running loss 1.11522, it/sec: 4.519735401930569
epoch 1 iter 24480: train loss 1.09847. lr 3.000000e-04, running loss 1.11512, it/sec: 4.463362675496012
epoch 1 iter 24500: train loss 1.11661. lr 3.000000e-04, running loss 1.11512, it/sec: 4.499520414257729
epoch 1 iter 24520: train loss 1.10061. lr 3.000000e-04, running loss 1.11512, it/sec: 4.480009768435797
epoch 1 iter 24540: train loss 1.10245. lr 3.000000e-04, running loss 1.11516, it/sec: 4.530801932970757
epoch 1 iter 24560: train loss 1.11352. lr 3.000000e-04, running loss 1.11538, it/sec: 4.423289150220564
epoch 1 iter 24580: train loss 1.12055. lr 3.000000e-04, running loss 1.11538, it/sec: 4.516885164863582
epoch 1 iter 24600: train loss 1.10159. lr 3.000000e-04, running loss 1.11538, it/sec: 4.463475135378238
epoch 1 iter 24620: train loss 1.13592. lr 3.000000e-04, running loss 1.11537, it/sec: 4.4859728092599855
epoch 1 iter 24640: train loss 1.10732. lr 3.000000e-04, running loss 1.11534, it/sec: 4.438327088437717
epoch 1 iter 24660: train loss 1.10599. lr 3.000000e-04, running loss 1.11560, it/sec: 4.507400651486705
epoch 1 iter 24680: train loss 1.10307. lr 3.000000e-04, running loss 1.11556, it/sec: 4.482428664175893
epoch 1 iter 24700: train loss 1.09320. lr 3.000000e-04, running loss 1.11551, it/sec: 4.436050411918949
epoch 1 iter 24720: train loss 1.25008. lr 3.000000e-04, running loss 1.11562, it/sec: 4.483993265047209
epoch 1 iter 24740: train loss 1.12719. lr 3.000000e-04, running loss 1.11556, it/sec: 4.504388283334572
epoch 1 iter 24760: train loss 1.10434. lr 3.000000e-04, running loss 1.11555, it/sec: 4.470364411875481
epoch 1 iter 24780: train loss 1.09931. lr 3.000000e-04, running loss 1.11558, it/sec: 4.513585065763942
epoch 1 iter 24800: train loss 1.12047. lr 3.000000e-04, running loss 1.11545, it/sec: 4.479154908290355
epoch 1 iter 24820: train loss 1.09796. lr 3.000000e-04, running loss 1.11543, it/sec: 4.500473842859945
epoch 1 iter 24840: train loss 1.11404. lr 3.000000e-04, running loss 1.11542, it/sec: 4.499667929704698
epoch 1 iter 24860: train loss 1.10765. lr 3.000000e-04, running loss 1.11543, it/sec: 4.489764365806939
epoch 1 iter 24880: train loss 1.20918. lr 3.000000e-04, running loss 1.11546, it/sec: 4.517506049838629
epoch 1 iter 24900: train loss 1.11430. lr 3.000000e-04, running loss 1.11540, it/sec: 4.465180584929481
epoch 1 iter 24920: train loss 1.10599. lr 3.000000e-04, running loss 1.11534, it/sec: 4.5049597686342455
epoch 1 iter 24940: train loss 1.12634. lr 3.000000e-04, running loss 1.11529, it/sec: 4.4269923157875155
epoch 1 iter 24960: train loss 1.11421. lr 3.000000e-04, running loss 1.11524, it/sec: 4.516195979823174
epoch 1 iter 24980: train loss 1.13319. lr 3.000000e-04, running loss 1.11524, it/sec: 4.462136498911697
epoch 1 iter 25000: train loss 1.11049. lr 3.000000e-04, running loss 1.11521, it/sec: 1.1216562262837027
epoch 1 iter 25020: train loss 1.11558. lr 3.000000e-04, running loss 1.11509, it/sec: 4.468234197318899
epoch 1 iter 25040: train loss 1.13776. lr 3.000000e-04, running loss 1.11507, it/sec: 4.500723531921639
epoch 1 iter 25060: train loss 1.15390. lr 3.000000e-04, running loss 1.11510, it/sec: 4.499653735519958
epoch 1 iter 25080: train loss 1.12235. lr 3.000000e-04, running loss 1.11506, it/sec: 4.495217042181433
epoch 1 iter 25100: train loss 1.12127. lr 3.000000e-04, running loss 1.11507, it/sec: 4.461956851034539
epoch 1 iter 25120: train loss 1.10346. lr 3.000000e-04, running loss 1.11504, it/sec: 4.513984278435012
epoch 1 iter 25140: train loss 1.14319. lr 3.000000e-04, running loss 1.11501, it/sec: 4.463020486978192
epoch 1 iter 25160: train loss 1.11699. lr 3.000000e-04, running loss 1.11494, it/sec: 4.514094351788266
epoch 1 iter 25180: train loss 1.12116. lr 3.000000e-04, running loss 1.11497, it/sec: 4.4622761362651415
epoch 1 iter 25200: train loss 1.14888. lr 3.000000e-04, running loss 1.11493, it/sec: 4.474654227265177
epoch 1 iter 25220: train loss 1.13277. lr 3.000000e-04, running loss 1.11505, it/sec: 4.45851297536032
epoch 1 iter 25240: train loss 1.12307. lr 3.000000e-04, running loss 1.11509, it/sec: 4.508574061387283
epoch 1 iter 25260: train loss 1.10345. lr 3.000000e-04, running loss 1.11502, it/sec: 4.448447116863616
epoch 1 iter 25280: train loss 1.11097. lr 3.000000e-04, running loss 1.11496, it/sec: 4.507346244941851
epoch 1 iter 25300: train loss 1.09858. lr 3.000000e-04, running loss 1.11498, it/sec: 4.45457722112284
epoch 1 iter 25320: train loss 1.11223. lr 3.000000e-04, running loss 1.11494, it/sec: 4.483577728269986
epoch 1 iter 25340: train loss 1.11309. lr 3.000000e-04, running loss 1.11487, it/sec: 4.4583113786425566
epoch 1 iter 25360: train loss 1.11664. lr 3.000000e-04, running loss 1.11484, it/sec: 4.53009802723176
epoch 1 iter 25380: train loss 1.12929. lr 3.000000e-04, running loss 1.11488, it/sec: 4.449277031923304
epoch 1 iter 25400: train loss 1.11679. lr 3.000000e-04, running loss 1.11477, it/sec: 4.525926919459629
epoch 1 iter 25420: train loss 1.08973. lr 3.000000e-04, running loss 1.11468, it/sec: 4.479697433570429
epoch 1 iter 25440: train loss 1.13426. lr 3.000000e-04, running loss 1.11475, it/sec: 4.5045290352011484
epoch 1 iter 25460: train loss 1.15046. lr 3.000000e-04, running loss 1.11483, it/sec: 4.4622183734946494
epoch 1 iter 25480: train loss 1.10958. lr 3.000000e-04, running loss 1.11480, it/sec: 4.520467476573522
epoch 1 iter 25500: train loss 1.09954. lr 3.000000e-04, running loss 1.11478, it/sec: 4.4385926633512796
epoch 1 iter 25520: train loss 1.14008. lr 3.000000e-04, running loss 1.11473, it/sec: 4.508722842002757
epoch 1 iter 25540: train loss 1.11504. lr 3.000000e-04, running loss 1.11466, it/sec: 4.4473654146028165
epoch 1 iter 25560: train loss 1.12179. lr 3.000000e-04, running loss 1.11472, it/sec: 4.505843307464339
epoch 1 iter 25580: train loss 1.10661. lr 3.000000e-04, running loss 1.11474, it/sec: 4.489266075111746
epoch 1 iter 25600: train loss 1.13931. lr 3.000000e-04, running loss 1.11471, it/sec: 4.494320353843811
epoch 1 iter 25620: train loss 1.13195. lr 3.000000e-04, running loss 1.11467, it/sec: 4.472666232613927
epoch 1 iter 25640: train loss 1.12098. lr 3.000000e-04, running loss 1.11468, it/sec: 4.507577068187184
epoch 1 iter 25660: train loss 1.12456. lr 3.000000e-04, running loss 1.11468, it/sec: 4.468361098916731
epoch 1 iter 25680: train loss 1.11515. lr 3.000000e-04, running loss 1.11463, it/sec: 4.519541221433978
epoch 1 iter 25700: train loss 1.07596. lr 3.000000e-04, running loss 1.11462, it/sec: 4.447269132921874
epoch 1 iter 25720: train loss 1.10795. lr 3.000000e-04, running loss 1.11466, it/sec: 4.520811274507509
epoch 1 iter 25740: train loss 1.11191. lr 3.000000e-04, running loss 1.11473, it/sec: 4.433730757342954
epoch 1 iter 25760: train loss 1.16881. lr 3.000000e-04, running loss 1.11479, it/sec: 4.512559809582983
epoch 1 iter 25780: train loss 1.11395. lr 3.000000e-04, running loss 1.11477, it/sec: 4.462560636079056
epoch 1 iter 25800: train loss 1.12162. lr 3.000000e-04, running loss 1.11481, it/sec: 4.512997070164949
epoch 1 iter 25820: train loss 1.11495. lr 3.000000e-04, running loss 1.11481, it/sec: 4.4711099847043725
epoch 1 iter 25840: train loss 1.09866. lr 3.000000e-04, running loss 1.11473, it/sec: 4.515287601726686
epoch 1 iter 25860: train loss 1.10913. lr 3.000000e-04, running loss 1.11461, it/sec: 4.509306936937405
epoch 1 iter 25880: train loss 1.12162. lr 3.000000e-04, running loss 1.11454, it/sec: 4.463356599252115
epoch 1 iter 25900: train loss 1.13645. lr 3.000000e-04, running loss 1.11480, it/sec: 4.492731847193774
epoch 1 iter 25920: train loss 1.11256. lr 3.000000e-04, running loss 1.11488, it/sec: 4.454366893225495
epoch 1 iter 25940: train loss 1.11342. lr 3.000000e-04, running loss 1.11487, it/sec: 4.470042730671108
epoch 1 iter 25960: train loss 1.15308. lr 3.000000e-04, running loss 1.11499, it/sec: 4.508233016455654
epoch 1 iter 25980: train loss 1.11034. lr 3.000000e-04, running loss 1.11498, it/sec: 4.445884555569963
epoch 1 iter 26000: train loss 1.10787. lr 3.000000e-04, running loss 1.11494, it/sec: 4.519852437649027
epoch 1 iter 26020: train loss 1.11936. lr 3.000000e-04, running loss 1.11498, it/sec: 4.439724464435894
epoch 1 iter 26040: train loss 1.09530. lr 3.000000e-04, running loss 1.11496, it/sec: 4.4825499441377135
epoch 1 iter 26060: train loss 1.11372. lr 3.000000e-04, running loss 1.11491, it/sec: 4.502102835693888
epoch 1 iter 26080: train loss 1.12238. lr 3.000000e-04, running loss 1.11480, it/sec: 4.501233122596474
epoch 1 iter 26100: train loss 1.11377. lr 3.000000e-04, running loss 1.11487, it/sec: 4.500857937385056
epoch 1 iter 26120: train loss 1.10178. lr 3.000000e-04, running loss 1.11484, it/sec: 4.46669378168034
epoch 1 iter 26140: train loss 1.11770. lr 3.000000e-04, running loss 1.11491, it/sec: 4.507320666384625
epoch 1 iter 26160: train loss 1.10324. lr 3.000000e-04, running loss 1.11484, it/sec: 4.430253338578144
epoch 1 iter 26180: train loss 1.12750. lr 3.000000e-04, running loss 1.11494, it/sec: 4.51423279884622
epoch 1 iter 26200: train loss 1.13203. lr 3.000000e-04, running loss 1.11500, it/sec: 4.43650448180903
epoch 1 iter 26220: train loss 1.10644. lr 3.000000e-04, running loss 1.11493, it/sec: 4.52851610379686
epoch 1 iter 26240: train loss 1.11666. lr 3.000000e-04, running loss 1.11491, it/sec: 4.4671402569120024
epoch 1 iter 26260: train loss 1.10184. lr 3.000000e-04, running loss 1.11487, it/sec: 4.492209549441342
epoch 1 iter 26280: train loss 1.12419. lr 3.000000e-04, running loss 1.11477, it/sec: 4.4947859882755985
epoch 1 iter 26300: train loss 1.11606. lr 3.000000e-04, running loss 1.11479, it/sec: 4.500259908360489
epoch 1 iter 26320: train loss 1.09657. lr 3.000000e-04, running loss 1.11474, it/sec: 4.505655821824405
epoch 1 iter 26340: train loss 1.09831. lr 3.000000e-04, running loss 1.11474, it/sec: 4.4963313086997525
epoch 1 iter 26360: train loss 1.09826. lr 3.000000e-04, running loss 1.11472, it/sec: 4.458739897897796
epoch 1 iter 26380: train loss 1.12241. lr 3.000000e-04, running loss 1.11472, it/sec: 4.506611020358854
epoch 1 iter 26400: train loss 1.13021. lr 3.000000e-04, running loss 1.11476, it/sec: 4.482336603946538
epoch 1 iter 26420: train loss 1.10046. lr 3.000000e-04, running loss 1.11472, it/sec: 4.521860402815814
epoch 1 iter 26440: train loss 1.11744. lr 3.000000e-04, running loss 1.11479, it/sec: 4.458347891660644
epoch 1 iter 26460: train loss 1.10987. lr 3.000000e-04, running loss 1.11479, it/sec: 4.511389799941045
epoch 1 iter 26480: train loss 1.10247. lr 3.000000e-04, running loss 1.11477, it/sec: 4.425901569165243
epoch 1 iter 26500: train loss 1.18543. lr 3.000000e-04, running loss 1.11491, it/sec: 4.544746866704576
epoch 1 iter 26520: train loss 1.10413. lr 3.000000e-04, running loss 1.11479, it/sec: 4.462695420719016
epoch 1 iter 26540: train loss 1.12250. lr 3.000000e-04, running loss 1.11478, it/sec: 4.523396449850245
epoch 1 iter 26560: train loss 1.13364. lr 3.000000e-04, running loss 1.11472, it/sec: 4.4826232862732525
epoch 1 iter 26580: train loss 1.10966. lr 3.000000e-04, running loss 1.11471, it/sec: 4.49855597518642
epoch 1 iter 26600: train loss 1.11747. lr 3.000000e-04, running loss 1.11461, it/sec: 4.479999612808261
epoch 1 iter 26620: train loss 1.14105. lr 3.000000e-04, running loss 1.11458, it/sec: 4.4993092617253705
epoch 1 iter 26640: train loss 1.12699. lr 3.000000e-04, running loss 1.11456, it/sec: 4.489992908297528
epoch 1 iter 26660: train loss 1.09691. lr 3.000000e-04, running loss 1.11453, it/sec: 4.517191973536403
epoch 1 iter 26680: train loss 1.13711. lr 3.000000e-04, running loss 1.11455, it/sec: 4.474294951516771
epoch 1 iter 26700: train loss 1.10967. lr 3.000000e-04, running loss 1.11459, it/sec: 4.50556696554494
epoch 1 iter 26720: train loss 1.11371. lr 3.000000e-04, running loss 1.11455, it/sec: 4.459327082170229
epoch 1 iter 26740: train loss 1.10756. lr 3.000000e-04, running loss 1.11462, it/sec: 4.529591464207907
epoch 1 iter 26760: train loss 1.09059. lr 3.000000e-04, running loss 1.11454, it/sec: 4.471357465283065
epoch 1 iter 26780: train loss 1.11706. lr 3.000000e-04, running loss 1.11458, it/sec: 4.516702674013059
epoch 1 iter 26800: train loss 1.17216. lr 3.000000e-04, running loss 1.11462, it/sec: 4.459294509365367
epoch 1 iter 26820: train loss 1.11087. lr 3.000000e-04, running loss 1.11460, it/sec: 4.498096034397704
epoch 1 iter 26840: train loss 1.12129. lr 3.000000e-04, running loss 1.11452, it/sec: 4.450309379082321
epoch 1 iter 26860: train loss 1.11949. lr 3.000000e-04, running loss 1.11442, it/sec: 4.485324126753416
epoch 1 iter 26880: train loss 1.10604. lr 3.000000e-04, running loss 1.11439, it/sec: 4.463726632091031
epoch 1 iter 26900: train loss 1.10121. lr 3.000000e-04, running loss 1.11443, it/sec: 4.502883831135366
epoch 1 iter 26920: train loss 1.10499. lr 3.000000e-04, running loss 1.11437, it/sec: 4.452746045192795
epoch 1 iter 26940: train loss 1.10463. lr 3.000000e-04, running loss 1.11440, it/sec: 4.5115206314404706
epoch 1 iter 26960: train loss 1.15064. lr 3.000000e-04, running loss 1.11461, it/sec: 4.474400976804071
epoch 1 iter 26980: train loss 1.10194. lr 3.000000e-04, running loss 1.11500, it/sec: 4.516529072369365
epoch 1 iter 27000: train loss 1.10568. lr 3.000000e-04, running loss 1.11496, it/sec: 4.453940225593012
epoch 1 iter 27020: train loss 1.09179. lr 3.000000e-04, running loss 1.11513, it/sec: 4.516520462692641
epoch 1 iter 27040: train loss 1.12750. lr 3.000000e-04, running loss 1.11507, it/sec: 4.43728383221726
epoch 1 iter 27060: train loss 1.11748. lr 3.000000e-04, running loss 1.11511, it/sec: 4.471810154366826
epoch 1 iter 27080: train loss 1.12349. lr 3.000000e-04, running loss 1.11539, it/sec: 4.503602363540813
epoch 1 iter 27100: train loss 1.10195. lr 3.000000e-04, running loss 1.11525, it/sec: 4.445551070655873
epoch 1 iter 27120: train loss 1.08825. lr 3.000000e-04, running loss 1.11516, it/sec: 4.509004490691918
epoch 1 iter 27140: train loss 1.11070. lr 3.000000e-04, running loss 1.11510, it/sec: 4.499316872657651
epoch 1 iter 27160: train loss 1.09487. lr 3.000000e-04, running loss 1.11508, it/sec: 4.454845756291966
epoch 1 iter 27180: train loss 1.11906. lr 3.000000e-04, running loss 1.11498, it/sec: 4.51134673488488
epoch 1 iter 27200: train loss 1.12051. lr 3.000000e-04, running loss 1.11495, it/sec: 4.487938874939785
epoch 1 iter 27220: train loss 1.13137. lr 3.000000e-04, running loss 1.11485, it/sec: 4.501894562201359
epoch 1 iter 27240: train loss 1.09906. lr 3.000000e-04, running loss 1.11480, it/sec: 4.483121890907157
epoch 1 iter 27260: train loss 1.09971. lr 3.000000e-04, running loss 1.11479, it/sec: 4.496391151316345
epoch 1 iter 27280: train loss 1.11197. lr 3.000000e-04, running loss 1.11484, it/sec: 4.522838686974359
epoch 1 iter 27300: train loss 1.11582. lr 3.000000e-04, running loss 1.11488, it/sec: 4.4502430123694925
epoch 1 iter 27320: train loss 1.10668. lr 3.000000e-04, running loss 1.11481, it/sec: 4.523557402888654
epoch 1 iter 27340: train loss 1.08949. lr 3.000000e-04, running loss 1.11472, it/sec: 4.436433015661583
epoch 1 iter 27360: train loss 1.11892. lr 3.000000e-04, running loss 1.11471, it/sec: 4.538145758366815
epoch 1 iter 27380: train loss 1.12081. lr 3.000000e-04, running loss 1.11484, it/sec: 4.449781333749155
epoch 1 iter 27400: train loss 1.10865. lr 3.000000e-04, running loss 1.11489, it/sec: 4.522935670376105
epoch 1 iter 27420: train loss 1.10609. lr 3.000000e-04, running loss 1.11490, it/sec: 4.456482632329136
epoch 1 iter 27440: train loss 1.09420. lr 3.000000e-04, running loss 1.11504, it/sec: 4.500290408728037
epoch 1 iter 27460: train loss 1.10291. lr 3.000000e-04, running loss 1.11491, it/sec: 4.455161841016749
epoch 1 iter 27480: train loss 1.10052. lr 3.000000e-04, running loss 1.11481, it/sec: 4.48554463306793
epoch 1 iter 27500: train loss 1.11152. lr 3.000000e-04, running loss 1.11485, it/sec: 4.451820458710118
epoch 1 iter 27520: train loss 1.11790. lr 3.000000e-04, running loss 1.11483, it/sec: 4.517311611863842
epoch 1 iter 27540: train loss 1.11915. lr 3.000000e-04, running loss 1.11486, it/sec: 4.462929661625399
epoch 1 iter 27560: train loss 1.11954. lr 3.000000e-04, running loss 1.11489, it/sec: 4.5329928321607955
epoch 1 iter 27580: train loss 1.11099. lr 3.000000e-04, running loss 1.11485, it/sec: 4.4377993629246655
epoch 1 iter 27600: train loss 1.10853. lr 3.000000e-04, running loss 1.11476, it/sec: 4.482166818538706
epoch 1 iter 27620: train loss 1.12441. lr 3.000000e-04, running loss 1.11470, it/sec: 4.4830327162536285
epoch 1 iter 27640: train loss 1.14623. lr 3.000000e-04, running loss 1.11471, it/sec: 4.490076998282285
epoch 1 iter 27660: train loss 1.09287. lr 3.000000e-04, running loss 1.11478, it/sec: 4.482552294997391
epoch 1 iter 27680: train loss 1.12446. lr 3.000000e-04, running loss 1.11473, it/sec: 4.435657978128071
epoch 1 iter 27700: train loss 1.14289. lr 3.000000e-04, running loss 1.11467, it/sec: 4.511844767465698
epoch 1 iter 27720: train loss 1.10493. lr 3.000000e-04, running loss 1.11465, it/sec: 4.454445407265084
epoch 1 iter 27740: train loss 1.11657. lr 3.000000e-04, running loss 1.11481, it/sec: 4.501363120338617
epoch 1 iter 27760: train loss 1.11859. lr 3.000000e-04, running loss 1.11490, it/sec: 4.455869194925177
epoch 1 iter 27780: train loss 1.12105. lr 3.000000e-04, running loss 1.11485, it/sec: 4.51612712417177
epoch 1 iter 27800: train loss 1.11266. lr 3.000000e-04, running loss 1.11482, it/sec: 4.457871156864923
epoch 1 iter 27820: train loss 1.08851. lr 3.000000e-04, running loss 1.11485, it/sec: 4.514962459450391
epoch 1 iter 27840: train loss 1.09529. lr 3.000000e-04, running loss 1.11486, it/sec: 4.460287086414657
epoch 1 iter 27860: train loss 1.12503. lr 3.000000e-04, running loss 1.11486, it/sec: 4.5130837146445035
epoch 1 iter 27880: train loss 1.12678. lr 3.000000e-04, running loss 1.11475, it/sec: 4.451375394537856
epoch 1 iter 27900: train loss 1.13437. lr 3.000000e-04, running loss 1.11473, it/sec: 4.532562434099011
epoch 1 iter 27920: train loss 1.11853. lr 3.000000e-04, running loss 1.11487, it/sec: 4.437526853247318
epoch 1 iter 27940: train loss 1.12528. lr 3.000000e-04, running loss 1.11508, it/sec: 4.521470794833606
epoch 1 iter 27960: train loss 1.11144. lr 3.000000e-04, running loss 1.11516, it/sec: 4.48700794247406
epoch 1 iter 27980: train loss 1.09241. lr 3.000000e-04, running loss 1.11507, it/sec: 4.492385728156309
epoch 1 iter 28000: train loss 1.10547. lr 3.000000e-04, running loss 1.11503, it/sec: 4.476145321815712
epoch 1 iter 28020: train loss 1.10857. lr 3.000000e-04, running loss 1.11493, it/sec: 4.486526186157981
epoch 1 iter 28040: train loss 1.09482. lr 3.000000e-04, running loss 1.11489, it/sec: 4.4981679228378
epoch 1 iter 28060: train loss 1.13249. lr 3.000000e-04, running loss 1.11483, it/sec: 4.473953268136654
epoch 1 iter 28080: train loss 1.11793. lr 3.000000e-04, running loss 1.11483, it/sec: 4.475312304590516
epoch 1 iter 28100: train loss 1.11550. lr 3.000000e-04, running loss 1.11484, it/sec: 4.48897898588576
epoch 1 iter 28120: train loss 1.11879. lr 3.000000e-04, running loss 1.11472, it/sec: 4.48044704732605
epoch 1 iter 28140: train loss 1.09696. lr 3.000000e-04, running loss 1.11470, it/sec: 4.487917524553241
epoch 1 iter 28160: train loss 1.09711. lr 3.000000e-04, running loss 1.11459, it/sec: 4.513714433143545
epoch 1 iter 28180: train loss 1.10786. lr 3.000000e-04, running loss 1.11453, it/sec: 4.474212273108878
epoch 1 iter 28200: train loss 1.11310. lr 3.000000e-04, running loss 1.11446, it/sec: 4.491674602045032
epoch 1 iter 28220: train loss 1.11736. lr 3.000000e-04, running loss 1.11436, it/sec: 4.48406570875998
epoch 1 iter 28240: train loss 1.09610. lr 3.000000e-04, running loss 1.11425, it/sec: 4.517406461389691
epoch 1 iter 28260: train loss 1.10162. lr 3.000000e-04, running loss 1.11440, it/sec: 4.455819736148254
epoch 1 iter 28280: train loss 1.11809. lr 3.000000e-04, running loss 1.11441, it/sec: 4.5373702307085795
epoch 1 iter 28300: train loss 1.10815. lr 3.000000e-04, running loss 1.11428, it/sec: 4.452802493126408
epoch 1 iter 28320: train loss 1.09912. lr 3.000000e-04, running loss 1.11418, it/sec: 4.4934319385788495
epoch 1 iter 28340: train loss 1.12021. lr 3.000000e-04, running loss 1.11416, it/sec: 4.474353148563506
epoch 1 iter 28360: train loss 1.12372. lr 3.000000e-04, running loss 1.11422, it/sec: 4.501999669834771
epoch 1 iter 28380: train loss 1.12730. lr 3.000000e-04, running loss 1.11426, it/sec: 4.466220804573556
epoch 1 iter 28400: train loss 1.12515. lr 3.000000e-04, running loss 1.11424, it/sec: 4.520788160061859
epoch 1 iter 28420: train loss 1.11792. lr 3.000000e-04, running loss 1.11418, it/sec: 4.446693701051592
epoch 1 iter 28440: train loss 1.11878. lr 3.000000e-04, running loss 1.11416, it/sec: 4.516239445002753
epoch 1 iter 28460: train loss 1.11036. lr 3.000000e-04, running loss 1.11423, it/sec: 4.454134919875363
epoch 1 iter 28480: train loss 1.12050. lr 3.000000e-04, running loss 1.11426, it/sec: 4.527474442059317
epoch 1 iter 28500: train loss 1.11300. lr 3.000000e-04, running loss 1.11418, it/sec: 4.4574364055760745
epoch 1 iter 28520: train loss 1.11118. lr 3.000000e-04, running loss 1.11414, it/sec: 4.509273386080629
epoch 1 iter 28540: train loss 1.11743. lr 3.000000e-04, running loss 1.11406, it/sec: 4.44426675336202
epoch 1 iter 28560: train loss 1.10901. lr 3.000000e-04, running loss 1.11398, it/sec: 4.510134065937297
epoch 1 iter 28580: train loss 1.10598. lr 3.000000e-04, running loss 1.11402, it/sec: 4.473588682275661
epoch 1 iter 28600: train loss 1.09499. lr 3.000000e-04, running loss 1.11392, it/sec: 4.5074356160747024
epoch 1 iter 28620: train loss 1.11284. lr 3.000000e-04, running loss 1.11405, it/sec: 4.466988642119333
epoch 1 iter 28640: train loss 1.11181. lr 3.000000e-04, running loss 1.11399, it/sec: 4.512915725735071
epoch 1 iter 28660: train loss 1.11723. lr 3.000000e-04, running loss 1.11397, it/sec: 4.45187543537367
epoch 1 iter 28680: train loss 1.12091. lr 3.000000e-04, running loss 1.11392, it/sec: 4.496460923426163
epoch 1 iter 28700: train loss 1.12111. lr 3.000000e-04, running loss 1.11398, it/sec: 4.43430057729538
epoch 1 iter 28720: train loss 1.09832. lr 3.000000e-04, running loss 1.11386, it/sec: 4.518702736227089
epoch 1 iter 28740: train loss 1.13312. lr 3.000000e-04, running loss 1.11395, it/sec: 4.42706679099913
epoch 1 iter 28760: train loss 1.11861. lr 3.000000e-04, running loss 1.11405, it/sec: 4.50214910937989
epoch 1 iter 28780: train loss 1.11060. lr 3.000000e-04, running loss 1.11402, it/sec: 4.492097271955042
epoch 1 iter 28800: train loss 1.13350. lr 3.000000e-04, running loss 1.11402, it/sec: 4.490180828376288
epoch 1 iter 28820: train loss 1.10938. lr 3.000000e-04, running loss 1.11395, it/sec: 4.492431560310164
epoch 1 iter 28840: train loss 1.10309. lr 3.000000e-04, running loss 1.11398, it/sec: 4.48840060813317
epoch 1 iter 28860: train loss 1.12995. lr 3.000000e-04, running loss 1.11393, it/sec: 4.46582259576753
epoch 1 iter 28880: train loss 1.12619. lr 3.000000e-04, running loss 1.11389, it/sec: 4.476296196570421
epoch 1 iter 28900: train loss 1.09683. lr 3.000000e-04, running loss 1.11392, it/sec: 4.500949848469012
epoch 1 iter 28920: train loss 1.12078. lr 3.000000e-04, running loss 1.11391, it/sec: 4.464495585664284
epoch 1 iter 28940: train loss 1.12548. lr 3.000000e-04, running loss 1.11390, it/sec: 4.464367388422696
epoch 1 iter 28960: train loss 1.10090. lr 3.000000e-04, running loss 1.11397, it/sec: 4.52451451657384
epoch 1 iter 28980: train loss 1.10813. lr 3.000000e-04, running loss 1.11385, it/sec: 4.4553199389065075
epoch 1 iter 29000: train loss 1.12803. lr 3.000000e-04, running loss 1.11380, it/sec: 4.51543757938797
epoch 1 iter 29020: train loss 1.13779. lr 3.000000e-04, running loss 1.11380, it/sec: 4.459408475171743
epoch 1 iter 29040: train loss 1.10466. lr 3.000000e-04, running loss 1.11388, it/sec: 4.476592466162027
epoch 1 iter 29060: train loss 1.12042. lr 3.000000e-04, running loss 1.11390, it/sec: 4.472687818908666
epoch 1 iter 29080: train loss 1.10615. lr 3.000000e-04, running loss 1.11392, it/sec: 4.480920009679278
epoch 1 iter 29100: train loss 1.12246. lr 3.000000e-04, running loss 1.11400, it/sec: 4.501423745667936
epoch 1 iter 29120: train loss 1.09701. lr 3.000000e-04, running loss 1.11398, it/sec: 4.47256182999015
epoch 1 iter 29140: train loss 1.12581. lr 3.000000e-04, running loss 1.11383, it/sec: 4.512565226383111
epoch 1 iter 29160: train loss 1.08366. lr 3.000000e-04, running loss 1.11375, it/sec: 4.497522931115962
epoch 1 iter 29180: train loss 1.11156. lr 3.000000e-04, running loss 1.11373, it/sec: 4.510735049200098
epoch 1 iter 29200: train loss 1.11121. lr 3.000000e-04, running loss 1.11376, it/sec: 4.465266857278716
epoch 1 iter 29220: train loss 1.10111. lr 3.000000e-04, running loss 1.11379, it/sec: 4.505495327226705
epoch 1 iter 29240: train loss 1.09045. lr 3.000000e-04, running loss 1.11373, it/sec: 4.459696984318333
epoch 1 iter 29260: train loss 1.11589. lr 3.000000e-04, running loss 1.11358, it/sec: 4.482120191110034
epoch 1 iter 29280: train loss 1.10584. lr 3.000000e-04, running loss 1.11353, it/sec: 4.521507757099873
epoch 1 iter 29300: train loss 1.10949. lr 3.000000e-04, running loss 1.11363, it/sec: 4.456215409367988
epoch 1 iter 29320: train loss 1.12552. lr 3.000000e-04, running loss 1.11374, it/sec: 4.503871304531037
epoch 1 iter 29340: train loss 1.12924. lr 3.000000e-04, running loss 1.11374, it/sec: 4.455552790273551
epoch 1 iter 29360: train loss 1.11598. lr 3.000000e-04, running loss 1.11385, it/sec: 4.447199297320593
epoch 1 iter 29380: train loss 1.11495. lr 3.000000e-04, running loss 1.11391, it/sec: 4.489836411771164
epoch 1 iter 29400: train loss 1.09919. lr 3.000000e-04, running loss 1.11388, it/sec: 4.493603346033256
epoch 1 iter 29420: train loss 1.11284. lr 3.000000e-04, running loss 1.11381, it/sec: 4.51407825409254
epoch 1 iter 29440: train loss 1.09392. lr 3.000000e-04, running loss 1.11380, it/sec: 4.4824788744863
epoch 1 iter 29460: train loss 1.12280. lr 3.000000e-04, running loss 1.11383, it/sec: 4.5233569806644285
epoch 1 iter 29480: train loss 1.10040. lr 3.000000e-04, running loss 1.11375, it/sec: 4.465085084234614
epoch 1 iter 29500: train loss 1.11101. lr 3.000000e-04, running loss 1.11377, it/sec: 4.524818800480191
epoch 1 iter 29520: train loss 1.10542. lr 3.000000e-04, running loss 1.11379, it/sec: 4.450470242131568
epoch 1 iter 29540: train loss 1.11250. lr 3.000000e-04, running loss 1.11369, it/sec: 4.526248541404643
epoch 1 iter 29560: train loss 1.10101. lr 3.000000e-04, running loss 1.11359, it/sec: 4.48516916275602
epoch 1 iter 29580: train loss 1.10509. lr 3.000000e-04, running loss 1.11350, it/sec: 4.452184855554718
epoch 1 iter 29600: train loss 1.10001. lr 3.000000e-04, running loss 1.11343, it/sec: 4.481877161607814
epoch 1 iter 29620: train loss 1.12557. lr 3.000000e-04, running loss 1.11357, it/sec: 4.490987885071637
epoch 1 iter 29640: train loss 1.10399. lr 3.000000e-04, running loss 1.11379, it/sec: 4.47522041636482
epoch 1 iter 29660: train loss 1.12438. lr 3.000000e-04, running loss 1.11391, it/sec: 4.504811602442182
epoch 1 iter 29680: train loss 1.10666. lr 3.000000e-04, running loss 1.11393, it/sec: 4.4687679682728
epoch 1 iter 29700: train loss 1.09924. lr 3.000000e-04, running loss 1.11399, it/sec: 4.503214678351649
epoch 1 iter 29720: train loss 1.10823. lr 3.000000e-04, running loss 1.11392, it/sec: 4.492422983523536
epoch 1 iter 29740: train loss 1.16337. lr 3.000000e-04, running loss 1.11395, it/sec: 4.516017439564262
epoch 1 iter 29760: train loss 1.09946. lr 3.000000e-04, running loss 1.11394, it/sec: 4.464887079417104
epoch 1 iter 29780: train loss 1.08395. lr 3.000000e-04, running loss 1.11400, it/sec: 4.502375489085826
epoch 1 iter 29800: train loss 1.10212. lr 3.000000e-04, running loss 1.11392, it/sec: 4.4576701733670925
epoch 1 iter 29820: train loss 1.11020. lr 3.000000e-04, running loss 1.11400, it/sec: 4.524627480224319
epoch 1 iter 29840: train loss 1.10618. lr 3.000000e-04, running loss 1.11411, it/sec: 4.468820589597039
epoch 1 iter 29860: train loss 1.11843. lr 3.000000e-04, running loss 1.11405, it/sec: 4.496444647623988
epoch 1 iter 29880: train loss 1.11035. lr 3.000000e-04, running loss 1.11403, it/sec: 4.524999121924872
epoch 1 iter 29900: train loss 1.12868. lr 3.000000e-04, running loss 1.11404, it/sec: 4.495610969887811
epoch 1 iter 29920: train loss 1.10853. lr 3.000000e-04, running loss 1.11394, it/sec: 4.493321638967585
epoch 1 iter 29940: train loss 1.42633. lr 3.000000e-04, running loss 1.11417, it/sec: 4.497874292350502
epoch 1 iter 29960: train loss 1.10996. lr 3.000000e-04, running loss 1.11416, it/sec: 4.47219279084358
epoch 1 iter 29980: train loss 1.10040. lr 3.000000e-04, running loss 1.11413, it/sec: 4.511437445882247
epoch 1 iter 30000: train loss 1.11873. lr 3.000000e-04, running loss 1.11416, it/sec: 4.493770082417098
epoch 1 iter 30020: train loss 1.11220. lr 3.000000e-04, running loss 1.11406, it/sec: 4.520394526293419
epoch 1 iter 30040: train loss 1.11932. lr 3.000000e-04, running loss 1.11402, it/sec: 4.455018103763869
epoch 1 iter 30060: train loss 1.09316. lr 3.000000e-04, running loss 1.11397, it/sec: 4.504030161371684
epoch 1 iter 30080: train loss 1.11271. lr 3.000000e-04, running loss 1.11398, it/sec: 4.473345296962005
epoch 1 iter 30100: train loss 1.10911. lr 3.000000e-04, running loss 1.11414, it/sec: 4.502900032023726
epoch 1 iter 30120: train loss 1.11778. lr 3.000000e-04, running loss 1.11420, it/sec: 4.452961376970028
epoch 1 iter 30140: train loss 1.10704. lr 3.000000e-04, running loss 1.11413, it/sec: 4.499665640998313
epoch 1 iter 30160: train loss 1.10266. lr 3.000000e-04, running loss 1.11414, it/sec: 4.464542743952034
epoch 1 iter 30180: train loss 1.10720. lr 3.000000e-04, running loss 1.11414, it/sec: 4.466077309518753
epoch 1 iter 30200: train loss 1.10924. lr 3.000000e-04, running loss 1.11405, it/sec: 4.491505964993577
epoch 1 iter 30220: train loss 1.09462. lr 3.000000e-04, running loss 1.11401, it/sec: 4.510544245409846
epoch 1 iter 30240: train loss 1.10402. lr 3.000000e-04, running loss 1.11398, it/sec: 4.474928252945938
epoch 1 iter 30260: train loss 1.10569. lr 3.000000e-04, running loss 1.11399, it/sec: 4.517176221933593
epoch 1 iter 30280: train loss 1.13103. lr 3.000000e-04, running loss 1.11398, it/sec: 4.500088195458843
epoch 1 iter 30300: train loss 1.11390. lr 3.000000e-04, running loss 1.11412, it/sec: 4.5210843592980385
epoch 1 iter 30320: train loss 1.17713. lr 3.000000e-04, running loss 1.11426, it/sec: 4.464800501489171
epoch 1 iter 30340: train loss 1.15917. lr 3.000000e-04, running loss 1.11427, it/sec: 4.5250625154723645
epoch 1 iter 30360: train loss 1.11601. lr 3.000000e-04, running loss 1.11418, it/sec: 4.434722684354921
epoch 1 iter 30380: train loss 1.10556. lr 3.000000e-04, running loss 1.11413, it/sec: 4.5280119435885124
epoch 1 iter 30400: train loss 1.11257. lr 3.000000e-04, running loss 1.11408, it/sec: 4.444425460952071
epoch 1 iter 30420: train loss 1.09835. lr 3.000000e-04, running loss 1.11410, it/sec: 4.509561631761949
epoch 1 iter 30440: train loss 1.12362. lr 3.000000e-04, running loss 1.11405, it/sec: 4.434240135041367
epoch 1 iter 30460: train loss 1.10157. lr 3.000000e-04, running loss 1.11408, it/sec: 4.507332876177507
epoch 1 iter 30480: train loss 1.11344. lr 3.000000e-04, running loss 1.11405, it/sec: 4.467770535217227
epoch 1 iter 30500: train loss 1.13340. lr 3.000000e-04, running loss 1.11403, it/sec: 4.5121586101951054
epoch 1 iter 30520: train loss 1.10225. lr 3.000000e-04, running loss 1.11391, it/sec: 4.467057662672603
epoch 1 iter 30540: train loss 1.11869. lr 3.000000e-04, running loss 1.11409, it/sec: 4.508428766608621
epoch 1 iter 30560: train loss 1.10315. lr 3.000000e-04, running loss 1.11405, it/sec: 4.417263569035683
epoch 1 iter 30580: train loss 1.10298. lr 3.000000e-04, running loss 1.11401, it/sec: 4.493910091531188
epoch 1 iter 30600: train loss 1.10204. lr 3.000000e-04, running loss 1.11392, it/sec: 4.4750127785640155
epoch 1 iter 30620: train loss 1.11836. lr 3.000000e-04, running loss 1.11400, it/sec: 4.5144148861255715
epoch 1 iter 30640: train loss 1.11371. lr 3.000000e-04, running loss 1.11419, it/sec: 4.459643146772296
epoch 1 iter 30660: train loss 1.12089. lr 3.000000e-04, running loss 1.11423, it/sec: 4.519506395746099
epoch 1 iter 30680: train loss 1.11896. lr 3.000000e-04, running loss 1.11409, it/sec: 4.453249746239429
epoch 1 iter 30700: train loss 1.10114. lr 3.000000e-04, running loss 1.11399, it/sec: 4.475755818212287
epoch 1 iter 30720: train loss 1.10204. lr 3.000000e-04, running loss 1.11394, it/sec: 4.466131323424863
epoch 1 iter 30740: train loss 1.11223. lr 3.000000e-04, running loss 1.11399, it/sec: 4.520201738929061
epoch 1 iter 30760: train loss 1.08143. lr 3.000000e-04, running loss 1.11395, it/sec: 4.447391642358017
epoch 1 iter 30780: train loss 1.10806. lr 3.000000e-04, running loss 1.11397, it/sec: 4.5217905769340705
epoch 1 iter 30800: train loss 1.09975. lr 3.000000e-04, running loss 1.11394, it/sec: 4.469903065042895
epoch 1 iter 30820: train loss 1.12612. lr 3.000000e-04, running loss 1.11392, it/sec: 4.524947094036884
epoch 1 iter 30840: train loss 1.12380. lr 3.000000e-04, running loss 1.11395, it/sec: 4.452498282571125
epoch 1 iter 30860: train loss 1.10782. lr 3.000000e-04, running loss 1.11387, it/sec: 4.50096664391691
epoch 1 iter 30880: train loss 1.11048. lr 3.000000e-04, running loss 1.11418, it/sec: 4.4477149780143455
epoch 1 iter 30900: train loss 1.10639. lr 3.000000e-04, running loss 1.11410, it/sec: 4.536264239461016
epoch 1 iter 30920: train loss 1.09838. lr 3.000000e-04, running loss 1.11410, it/sec: 4.450149536304111
epoch 1 iter 30940: train loss 1.14525. lr 3.000000e-04, running loss 1.11404, it/sec: 4.519034850677834
epoch 1 iter 30960: train loss 1.13113. lr 3.000000e-04, running loss 1.11417, it/sec: 4.44399127620694
epoch 1 iter 30980: train loss 1.12436. lr 3.000000e-04, running loss 1.11424, it/sec: 4.5090760785127095
epoch 1 iter 31000: train loss 1.11130. lr 3.000000e-04, running loss 1.11420, it/sec: 4.449378013357568
epoch 1 iter 31020: train loss 1.10779. lr 3.000000e-04, running loss 1.11410, it/sec: 4.515788728076742
epoch 1 iter 31040: train loss 1.10071. lr 3.000000e-04, running loss 1.11409, it/sec: 4.453061612709248
epoch 1 iter 31060: train loss 1.08343. lr 3.000000e-04, running loss 1.11394, it/sec: 4.523623742165639
epoch 1 iter 31080: train loss 1.11678. lr 3.000000e-04, running loss 1.11390, it/sec: 4.463520439827741
epoch 1 iter 31100: train loss 1.11332. lr 3.000000e-04, running loss 1.11385, it/sec: 4.488426918730393
epoch 1 iter 31120: train loss 1.09703. lr 3.000000e-04, running loss 1.11382, it/sec: 4.474806924663185
epoch 1 iter 31140: train loss 1.12702. lr 3.000000e-04, running loss 1.11384, it/sec: 4.514905912433939
epoch 1 iter 31160: train loss 1.09416. lr 3.000000e-04, running loss 1.11376, it/sec: 4.448257273119013
epoch 1 iter 31180: train loss 1.11772. lr 3.000000e-04, running loss 1.11368, it/sec: 4.511833510768928
epoch 1 iter 31200: train loss 1.11330. lr 3.000000e-04, running loss 1.11382, it/sec: 4.454428938732926
epoch 1 iter 31220: train loss 1.10035. lr 3.000000e-04, running loss 1.11369, it/sec: 4.5007969028111345
epoch 1 iter 31240: train loss 1.15445. lr 3.000000e-04, running loss 1.11372, it/sec: 4.454193308498132
epoch 1 iter 31260: train loss 1.11547. lr 3.000000e-04, running loss 1.11380, it/sec: 4.502057392346517
epoch 1 iter 31280: train loss 1.13339. lr 3.000000e-04, running loss 1.11378, it/sec: 4.481557878094207
epoch 1 iter 31300: train loss 1.12740. lr 3.000000e-04, running loss 1.11378, it/sec: 4.504733576654341
epoch 1 iter 31320: train loss 1.10646. lr 3.000000e-04, running loss 1.11374, it/sec: 4.5123494676306635
epoch 1 iter 31340: train loss 1.09954. lr 3.000000e-04, running loss 1.11371, it/sec: 4.463548371864374
epoch 1 iter 31360: train loss 1.10791. lr 3.000000e-04, running loss 1.11369, it/sec: 4.477672616200352
epoch 1 iter 31380: train loss 1.10271. lr 3.000000e-04, running loss 1.11362, it/sec: 4.505332166781538
epoch 1 iter 31400: train loss 1.12481. lr 3.000000e-04, running loss 1.11381, it/sec: 4.476226287266022
epoch 1 iter 31420: train loss 1.11456. lr 3.000000e-04, running loss 1.11387, it/sec: 4.498398839952872
epoch 1 iter 31440: train loss 1.09744. lr 3.000000e-04, running loss 1.11381, it/sec: 4.504689561361427
epoch 1 iter 31460: train loss 1.10998. lr 3.000000e-04, running loss 1.11371, it/sec: 4.491046658062774
epoch 1 iter 31480: train loss 1.09776. lr 3.000000e-04, running loss 1.11360, it/sec: 4.495700668003925
epoch 1 iter 31500: train loss 1.10435. lr 3.000000e-04, running loss 1.11365, it/sec: 4.502038745950525
epoch 1 iter 31520: train loss 1.10811. lr 3.000000e-04, running loss 1.11357, it/sec: 4.484696991177981
epoch 1 iter 31540: train loss 1.11713. lr 3.000000e-04, running loss 1.11365, it/sec: 4.450558740558759
epoch 1 iter 31560: train loss 1.14914. lr 3.000000e-04, running loss 1.11368, it/sec: 4.507778552280787
epoch 1 iter 31580: train loss 1.12080. lr 3.000000e-04, running loss 1.11364, it/sec: 4.468997193769132
epoch 1 iter 31600: train loss 1.12925. lr 3.000000e-04, running loss 1.11359, it/sec: 4.5029717294650915
epoch 1 iter 31620: train loss 1.10828. lr 3.000000e-04, running loss 1.11347, it/sec: 4.475935997081505
epoch 1 iter 31640: train loss 1.12158. lr 3.000000e-04, running loss 1.11340, it/sec: 4.492732634377346
epoch 1 iter 31660: train loss 1.09014. lr 3.000000e-04, running loss 1.11340, it/sec: 4.489580190407274
epoch 1 iter 31680: train loss 1.09955. lr 3.000000e-04, running loss 1.11332, it/sec: 4.464268554850168
epoch 1 iter 31700: train loss 1.10670. lr 3.000000e-04, running loss 1.11328, it/sec: 4.52418077893233
epoch 1 iter 31720: train loss 1.11311. lr 3.000000e-04, running loss 1.11322, it/sec: 4.464753516444166
epoch 1 iter 31740: train loss 1.09857. lr 3.000000e-04, running loss 1.11372, it/sec: 4.500842198084187
epoch 1 iter 31760: train loss 1.11965. lr 3.000000e-04, running loss 1.11378, it/sec: 4.502856296855887
epoch 1 iter 31780: train loss 1.11921. lr 3.000000e-04, running loss 1.11379, it/sec: 4.483965559887031
epoch 1 iter 31800: train loss 1.11316. lr 3.000000e-04, running loss 1.11388, it/sec: 4.517413665659721
epoch 1 iter 31820: train loss 1.15038. lr 3.000000e-04, running loss 1.11397, it/sec: 4.493243162222028
epoch 1 iter 31840: train loss 1.11149. lr 3.000000e-04, running loss 1.11393, it/sec: 4.46125277986335
epoch 1 iter 31860: train loss 1.11530. lr 3.000000e-04, running loss 1.11385, it/sec: 4.462380795255046
epoch 1 iter 31880: train loss 1.09648. lr 3.000000e-04, running loss 1.11383, it/sec: 4.513202483355142
epoch 1 iter 31900: train loss 1.10802. lr 3.000000e-04, running loss 1.11376, it/sec: 4.479010701203496
epoch 1 iter 31920: train loss 1.12584. lr 3.000000e-04, running loss 1.11379, it/sec: 4.506493166904521
epoch 1 iter 31940: train loss 1.12015. lr 3.000000e-04, running loss 1.11387, it/sec: 4.4465745503593395
epoch 1 iter 31960: train loss 1.10818. lr 3.000000e-04, running loss 1.11388, it/sec: 4.498174984611268
epoch 1 iter 31980: train loss 1.14063. lr 3.000000e-04, running loss 1.11391, it/sec: 4.494125927989757
epoch 1 iter 32000: train loss 1.12101. lr 3.000000e-04, running loss 1.11391, it/sec: 4.495270671697724
epoch 1 iter 32020: train loss 1.09199. lr 3.000000e-04, running loss 1.11496, it/sec: 4.510323899571032
epoch 1 iter 32040: train loss 1.11784. lr 3.000000e-04, running loss 1.11499, it/sec: 4.476555072799856
epoch 1 iter 32060: train loss 1.10233. lr 3.000000e-04, running loss 1.11525, it/sec: 4.485082440472519
epoch 1 iter 32080: train loss 1.08753. lr 3.000000e-04, running loss 1.11519, it/sec: 4.509628254846943
epoch 1 iter 32100: train loss 1.09504. lr 3.000000e-04, running loss 1.11657, it/sec: 4.4723515805388185
epoch 1 iter 32120: train loss 1.11103. lr 3.000000e-04, running loss 1.11639, it/sec: 4.521031991894129
epoch 1 iter 32140: train loss 1.10257. lr 3.000000e-04, running loss 1.11632, it/sec: 4.484141573584547
epoch 1 iter 32160: train loss 1.10471. lr 3.000000e-04, running loss 1.11634, it/sec: 4.496028721411258
epoch 1 iter 32180: train loss 1.10261. lr 3.000000e-04, running loss 1.11625, it/sec: 4.45312403855254
epoch 1 iter 32200: train loss 1.12417. lr 3.000000e-04, running loss 1.11617, it/sec: 4.489699840747174
epoch 1 iter 32220: train loss 1.08658. lr 3.000000e-04, running loss 1.11601, it/sec: 4.463364189921208
epoch 1 iter 32240: train loss 1.14296. lr 3.000000e-04, running loss 1.11600, it/sec: 4.512941204236374
epoch 1 iter 32260: train loss 1.11312. lr 3.000000e-04, running loss 1.11599, it/sec: 4.460096766876859
epoch 1 iter 32280: train loss 1.12258. lr 3.000000e-04, running loss 1.11581, it/sec: 4.472809811356838
epoch 1 iter 32300: train loss 1.10230. lr 3.000000e-04, running loss 1.11578, it/sec: 4.495157370777243
epoch 1 iter 32320: train loss 1.11818. lr 3.000000e-04, running loss 1.11577, it/sec: 4.475283804276235
epoch 1 iter 32340: train loss 1.10988. lr 3.000000e-04, running loss 1.11577, it/sec: 4.497838848294093
epoch 1 iter 32360: train loss 1.12635. lr 3.000000e-04, running loss 1.11563, it/sec: 4.4839959392634015
epoch 1 iter 32380: train loss 1.10557. lr 3.000000e-04, running loss 1.11559, it/sec: 4.465413091633103
epoch 1 iter 32400: train loss 1.10978. lr 3.000000e-04, running loss 1.11564, it/sec: 4.526925387680285
epoch 1 iter 32420: train loss 1.09814. lr 3.000000e-04, running loss 1.11572, it/sec: 4.436240927718807
epoch 1 iter 32440: train loss 1.10631. lr 3.000000e-04, running loss 1.11565, it/sec: 4.501492235076909
epoch 1 iter 32460: train loss 1.12261. lr 3.000000e-04, running loss 1.11556, it/sec: 4.472177350340298
epoch 1 iter 32480: train loss 1.11547. lr 3.000000e-04, running loss 1.11553, it/sec: 4.510806425498804
epoch 1 iter 32500: train loss 1.16068. lr 3.000000e-04, running loss 1.11555, it/sec: 4.466292196775002
epoch 1 iter 32520: train loss 1.10102. lr 3.000000e-04, running loss 1.11555, it/sec: 4.490416530513177
epoch 1 iter 32540: train loss 1.10861. lr 3.000000e-04, running loss 1.11540, it/sec: 4.474209971772018
epoch 1 iter 32560: train loss 1.10694. lr 3.000000e-04, running loss 1.11525, it/sec: 4.500048382305929
epoch 1 iter 32580: train loss 1.11169. lr 3.000000e-04, running loss 1.11523, it/sec: 4.434728978234569
epoch 1 iter 32600: train loss 1.10702. lr 3.000000e-04, running loss 1.11523, it/sec: 4.494964388450654
epoch 1 iter 32620: train loss 1.08740. lr 3.000000e-04, running loss 1.11510, it/sec: 4.479216582370854
epoch 1 iter 32640: train loss 1.12116. lr 3.000000e-04, running loss 1.11498, it/sec: 4.530578227123139
epoch 1 iter 32660: train loss 1.14034. lr 3.000000e-04, running loss 1.11494, it/sec: 4.473307176271548
epoch 1 iter 32680: train loss 1.13806. lr 3.000000e-04, running loss 1.11505, it/sec: 4.510762415808847
epoch 1 iter 32700: train loss 1.09639. lr 3.000000e-04, running loss 1.11502, it/sec: 4.452222952851048
epoch 1 iter 32720: train loss 1.10478. lr 3.000000e-04, running loss 1.11501, it/sec: 4.512297710075129
epoch 1 iter 32740: train loss 1.10280. lr 3.000000e-04, running loss 1.11511, it/sec: 4.44722933941522
epoch 1 iter 32760: train loss 1.11466. lr 3.000000e-04, running loss 1.11502, it/sec: 4.529423044243104
epoch 1 iter 32780: train loss 1.10945. lr 3.000000e-04, running loss 1.11500, it/sec: 4.481459467305986
epoch 1 iter 32800: train loss 1.10766. lr 3.000000e-04, running loss 1.11493, it/sec: 4.50629646779839
epoch 1 iter 32820: train loss 1.10624. lr 3.000000e-04, running loss 1.11479, it/sec: 4.451933209862543
epoch 1 iter 32840: train loss 1.12667. lr 3.000000e-04, running loss 1.11483, it/sec: 4.514731306341682
epoch 1 iter 32860: train loss 1.11878. lr 3.000000e-04, running loss 1.11481, it/sec: 4.4403643812053994
epoch 1 iter 32880: train loss 1.11750. lr 3.000000e-04, running loss 1.11481, it/sec: 4.514102279696527
epoch 1 iter 32900: train loss 1.12068. lr 3.000000e-04, running loss 1.11482, it/sec: 4.456340377299139
epoch 1 iter 32920: train loss 1.14111. lr 3.000000e-04, running loss 1.11477, it/sec: 4.507967984216925
epoch 1 iter 32940: train loss 1.10415. lr 3.000000e-04, running loss 1.11474, it/sec: 4.441727981510589
epoch 1 iter 32960: train loss 1.09841. lr 3.000000e-04, running loss 1.11466, it/sec: 4.511494312413632
epoch 1 iter 32980: train loss 1.13667. lr 3.000000e-04, running loss 1.11461, it/sec: 4.471153746521072
epoch 1 iter 33000: train loss 1.11630. lr 3.000000e-04, running loss 1.11458, it/sec: 4.492718443967159
epoch 1 iter 33020: train loss 1.11252. lr 3.000000e-04, running loss 1.11455, it/sec: 4.473746049633604
epoch 1 iter 33040: train loss 1.12991. lr 3.000000e-04, running loss 1.11448, it/sec: 4.500173391324612
epoch 1 iter 33060: train loss 1.11714. lr 3.000000e-04, running loss 1.11447, it/sec: 4.479324406024135
epoch 1 iter 33080: train loss 1.09115. lr 3.000000e-04, running loss 1.11442, it/sec: 4.489673636074529
epoch 1 iter 33100: train loss 1.12837. lr 3.000000e-04, running loss 1.11432, it/sec: 4.510558751156006
epoch 1 iter 33120: train loss 1.09485. lr 3.000000e-04, running loss 1.11440, it/sec: 4.461537945183528
epoch 1 iter 33140: train loss 1.07984. lr 3.000000e-04, running loss 1.11439, it/sec: 4.498089944478315
epoch 1 iter 33160: train loss 1.09679. lr 3.000000e-04, running loss 1.11441, it/sec: 4.5010599368165165
epoch 1 iter 33180: train loss 1.11194. lr 3.000000e-04, running loss 1.11444, it/sec: 4.462819777385815
epoch 1 iter 33200: train loss 1.15085. lr 3.000000e-04, running loss 1.11444, it/sec: 4.524629875426025
epoch 1 iter 33220: train loss 1.10277. lr 3.000000e-04, running loss 1.11440, it/sec: 4.445711788916104
epoch 1 iter 33240: train loss 1.11630. lr 3.000000e-04, running loss 1.11442, it/sec: 4.491707407689042
epoch 1 iter 33260: train loss 1.12401. lr 3.000000e-04, running loss 1.11441, it/sec: 4.452984021175115
epoch 1 iter 33280: train loss 1.12637. lr 3.000000e-04, running loss 1.11441, it/sec: 4.495873299246037
epoch 1 iter 33300: train loss 1.10991. lr 3.000000e-04, running loss 1.11441, it/sec: 4.5053304606959745
epoch 1 iter 33320: train loss 1.09015. lr 3.000000e-04, running loss 1.11440, it/sec: 4.485218489504961
epoch 1 iter 33340: train loss 1.10497. lr 3.000000e-04, running loss 1.11433, it/sec: 4.502069087567599
epoch 1 iter 33360: train loss 1.10823. lr 3.000000e-04, running loss 1.11438, it/sec: 4.465643608605165
epoch 1 iter 33380: train loss 1.11329. lr 3.000000e-04, running loss 1.11424, it/sec: 4.5047231456230605
epoch 1 iter 33400: train loss 1.10194. lr 3.000000e-04, running loss 1.11427, it/sec: 4.466637937906265
epoch 1 iter 33420: train loss 1.10881. lr 3.000000e-04, running loss 1.11422, it/sec: 4.516935089558903
epoch 1 iter 33440: train loss 1.12769. lr 3.000000e-04, running loss 1.11424, it/sec: 4.451999983488603
epoch 1 iter 33460: train loss 1.14219. lr 3.000000e-04, running loss 1.11424, it/sec: 4.489132703584763
epoch 1 iter 33480: train loss 1.13743. lr 3.000000e-04, running loss 1.11435, it/sec: 4.448242432810947
epoch 1 iter 33500: train loss 1.08703. lr 3.000000e-04, running loss 1.11432, it/sec: 4.506138547879784
epoch 1 iter 33520: train loss 1.12012. lr 3.000000e-04, running loss 1.11428, it/sec: 4.50484110968917
epoch 1 iter 33540: train loss 1.10807. lr 3.000000e-04, running loss 1.11433, it/sec: 4.50491820598842
epoch 1 iter 33560: train loss 1.13390. lr 3.000000e-04, running loss 1.11441, it/sec: 4.463485335670141
epoch 1 iter 33580: train loss 1.12323. lr 3.000000e-04, running loss 1.11442, it/sec: 4.5038163538382365
epoch 1 iter 33600: train loss 1.10152. lr 3.000000e-04, running loss 1.11435, it/sec: 4.48190795583672
epoch 1 iter 33620: train loss 1.11516. lr 3.000000e-04, running loss 1.11435, it/sec: 4.496568101878827
epoch 1 iter 33640: train loss 1.11603. lr 3.000000e-04, running loss 1.11427, it/sec: 4.488287915913694
epoch 1 iter 33660: train loss 1.11072. lr 3.000000e-04, running loss 1.11429, it/sec: 4.483491289830617
epoch 1 iter 33680: train loss 1.10350. lr 3.000000e-04, running loss 1.11424, it/sec: 4.4816815824743665
epoch 1 iter 33700: train loss 1.10968. lr 3.000000e-04, running loss 1.11421, it/sec: 4.508508405331332
epoch 1 iter 33720: train loss 1.13986. lr 3.000000e-04, running loss 1.11415, it/sec: 4.474593499222153
epoch 1 iter 33740: train loss 1.09360. lr 3.000000e-04, running loss 1.11410, it/sec: 4.50545923597992
epoch 1 iter 33760: train loss 1.12266. lr 3.000000e-04, running loss 1.11414, it/sec: 4.487520391171699
epoch 1 iter 33780: train loss 1.10968. lr 3.000000e-04, running loss 1.11401, it/sec: 4.4979535798267305
epoch 1 iter 33800: train loss 1.11762. lr 3.000000e-04, running loss 1.11398, it/sec: 4.436117260919869
epoch 1 iter 33820: train loss 1.11693. lr 3.000000e-04, running loss 1.11405, it/sec: 4.494825060393157
epoch 1 iter 33840: train loss 1.09572. lr 3.000000e-04, running loss 1.11404, it/sec: 4.491409275019165
epoch 1 iter 33860: train loss 1.10898. lr 3.000000e-04, running loss 1.11396, it/sec: 4.511971919903452
epoch 1 iter 33880: train loss 1.15346. lr 3.000000e-04, running loss 1.11393, it/sec: 4.463342315505759
epoch 1 iter 33900: train loss 1.10333. lr 3.000000e-04, running loss 1.11398, it/sec: 4.51106192227141
epoch 1 iter 33920: train loss 1.11600. lr 3.000000e-04, running loss 1.11392, it/sec: 4.437439088952992
epoch 1 iter 33940: train loss 1.10551. lr 3.000000e-04, running loss 1.11401, it/sec: 4.519844898589132
epoch 1 iter 33960: train loss 1.10944. lr 3.000000e-04, running loss 1.11408, it/sec: 4.476012286767085
epoch 1 iter 33980: train loss 1.10776. lr 3.000000e-04, running loss 1.11402, it/sec: 4.500534282716551
epoch 1 iter 34000: train loss 1.10629. lr 3.000000e-04, running loss 1.11409, it/sec: 4.5092792625042915
epoch 1 iter 34020: train loss 1.12929. lr 3.000000e-04, running loss 1.11403, it/sec: 4.483012940309645
epoch 1 iter 34040: train loss 1.13030. lr 3.000000e-04, running loss 1.11407, it/sec: 4.483679670072825
epoch 1 iter 34060: train loss 1.09021. lr 3.000000e-04, running loss 1.11407, it/sec: 4.481502969015194
epoch 1 iter 34080: train loss 1.12226. lr 3.000000e-04, running loss 1.11414, it/sec: 4.455876757837086
epoch 1 iter 34100: train loss 1.14044. lr 3.000000e-04, running loss 1.11410, it/sec: 4.500233174756404
epoch 1 iter 34120: train loss 1.09282. lr 3.000000e-04, running loss 1.11405, it/sec: 4.47177715800308
epoch 1 iter 34140: train loss 1.10120. lr 3.000000e-04, running loss 1.11407, it/sec: 4.51601252492963
epoch 1 iter 34160: train loss 1.10596. lr 3.000000e-04, running loss 1.11422, it/sec: 4.457700953927608
epoch 1 iter 34180: train loss 1.12307. lr 3.000000e-04, running loss 1.11441, it/sec: 4.516760346672188
epoch 1 iter 34200: train loss 1.08795. lr 3.000000e-04, running loss 1.11445, it/sec: 4.484306099759007
epoch 1 iter 34220: train loss 1.12702. lr 3.000000e-04, running loss 1.11442, it/sec: 4.495059938999819
epoch 1 iter 34240: train loss 1.10775. lr 3.000000e-04, running loss 1.11441, it/sec: 4.495700020954361
epoch 1 iter 34260: train loss 1.11058. lr 3.000000e-04, running loss 1.11433, it/sec: 4.4930432162687675
epoch 1 iter 34280: train loss 1.13166. lr 3.000000e-04, running loss 1.11429, it/sec: 4.524239461240425
epoch 1 iter 34300: train loss 1.08979. lr 3.000000e-04, running loss 1.11426, it/sec: 4.497353328381661
epoch 1 iter 34320: train loss 1.09728. lr 3.000000e-04, running loss 1.11427, it/sec: 4.500096031811376
epoch 1 iter 34340: train loss 1.11558. lr 3.000000e-04, running loss 1.11422, it/sec: 4.476473934490181
epoch 1 iter 34360: train loss 1.09930. lr 3.000000e-04, running loss 1.11429, it/sec: 4.4764735134152085
epoch 1 iter 34380: train loss 1.10455. lr 3.000000e-04, running loss 1.11431, it/sec: 4.507823136566666
epoch 1 iter 34400: train loss 1.10426. lr 3.000000e-04, running loss 1.11423, it/sec: 4.49242352860255
epoch 1 iter 34420: train loss 1.09023. lr 3.000000e-04, running loss 1.11418, it/sec: 4.489774121052559
epoch 1 iter 34440: train loss 1.11624. lr 3.000000e-04, running loss 1.11417, it/sec: 4.438425978417703
epoch 1 iter 34460: train loss 1.11320. lr 3.000000e-04, running loss 1.11410, it/sec: 4.517311754398838
epoch 1 iter 34480: train loss 1.10868. lr 3.000000e-04, running loss 1.11414, it/sec: 4.473930209770312
epoch 1 iter 34500: train loss 1.10945. lr 3.000000e-04, running loss 1.11406, it/sec: 4.508367323040691
epoch 1 iter 34520: train loss 1.10408. lr 3.000000e-04, running loss 1.11429, it/sec: 4.463883486145648
epoch 1 iter 34540: train loss 1.11017. lr 3.000000e-04, running loss 1.11425, it/sec: 4.51760166306227
epoch 1 iter 34560: train loss 1.10344. lr 3.000000e-04, running loss 1.11420, it/sec: 4.45635515378241
epoch 1 iter 34580: train loss 1.11899. lr 3.000000e-04, running loss 1.11413, it/sec: 4.486248363607232
epoch 1 iter 34600: train loss 1.10385. lr 3.000000e-04, running loss 1.11417, it/sec: 4.482414660410775
epoch 1 iter 34620: train loss 1.14897. lr 3.000000e-04, running loss 1.11419, it/sec: 4.460478496904715
epoch 1 iter 34640: train loss 1.13282. lr 3.000000e-04, running loss 1.11417, it/sec: 4.488838679832745
epoch 1 iter 34660: train loss 1.10574. lr 3.000000e-04, running loss 1.11419, it/sec: 4.484288422969056
epoch 1 iter 34680: train loss 1.09804. lr 3.000000e-04, running loss 1.11412, it/sec: 4.453777920040974
epoch 1 iter 34700: train loss 1.09356. lr 3.000000e-04, running loss 1.11409, it/sec: 4.510811024424169
epoch 1 iter 34720: train loss 1.12941. lr 3.000000e-04, running loss 1.11413, it/sec: 4.445000920019997
epoch 1 iter 34740: train loss 1.10488. lr 3.000000e-04, running loss 1.11413, it/sec: 4.486015412325175
epoch 1 iter 34760: train loss 1.11487. lr 3.000000e-04, running loss 1.11414, it/sec: 4.4542443364260365
epoch 1 iter 34780: train loss 1.09516. lr 3.000000e-04, running loss 1.11427, it/sec: 4.514870952878675
epoch 1 iter 34800: train loss 1.11891. lr 3.000000e-04, running loss 1.11421, it/sec: 4.474157663897552
epoch 1 iter 34820: train loss 1.12104. lr 3.000000e-04, running loss 1.11428, it/sec: 4.500361780237037
epoch 1 iter 34840: train loss 1.11058. lr 3.000000e-04, running loss 1.11417, it/sec: 4.4918796313824645
epoch 1 iter 34860: train loss 1.10315. lr 3.000000e-04, running loss 1.11425, it/sec: 4.4805613735691
epoch 1 iter 34880: train loss 1.12735. lr 3.000000e-04, running loss 1.11424, it/sec: 4.4735733719203
epoch 1 iter 34900: train loss 1.10319. lr 3.000000e-04, running loss 1.11413, it/sec: 4.419757506141039
epoch 1 iter 34920: train loss 1.10187. lr 3.000000e-04, running loss 1.11411, it/sec: 4.512546798648151
epoch 1 iter 34940: train loss 1.09804. lr 3.000000e-04, running loss 1.11412, it/sec: 4.444908295401291
epoch 1 iter 34960: train loss 1.11876. lr 3.000000e-04, running loss 1.11405, it/sec: 4.5145253470557405
epoch 1 iter 34980: train loss 1.12676. lr 3.000000e-04, running loss 1.11408, it/sec: 4.455072087327405
epoch 1 iter 35000: train loss 1.11765. lr 3.000000e-04, running loss 1.11410, it/sec: 4.472920327852841
epoch 1 iter 35020: train loss 1.09152. lr 3.000000e-04, running loss 1.11398, it/sec: 4.475044941449892
epoch 1 iter 35040: train loss 1.12481. lr 3.000000e-04, running loss 1.11393, it/sec: 4.511667569877615
epoch 1 iter 35060: train loss 1.11384. lr 3.000000e-04, running loss 1.11392, it/sec: 4.464914768631976
epoch 1 iter 35080: train loss 1.08402. lr 3.000000e-04, running loss 1.11377, it/sec: 4.504447935513149
epoch 1 iter 35100: train loss 1.12505. lr 3.000000e-04, running loss 1.11383, it/sec: 4.438184671585227
epoch 1 iter 35120: train loss 1.09431. lr 3.000000e-04, running loss 1.11381, it/sec: 4.512215473576529
epoch 1 iter 35140: train loss 1.09572. lr 3.000000e-04, running loss 1.11390, it/sec: 4.428838281220285
epoch 1 iter 35160: train loss 1.11666. lr 3.000000e-04, running loss 1.11382, it/sec: 4.516570787792472
epoch 1 iter 35180: train loss 1.10187. lr 3.000000e-04, running loss 1.11383, it/sec: 4.464116356574848
epoch 1 iter 35200: train loss 1.10955. lr 3.000000e-04, running loss 1.11368, it/sec: 4.514140445302283
epoch 1 iter 35220: train loss 1.11386. lr 3.000000e-04, running loss 1.11377, it/sec: 4.469011334703188
epoch 1 iter 35240: train loss 1.11604. lr 3.000000e-04, running loss 1.11374, it/sec: 4.491247272896439
epoch 1 iter 35260: train loss 1.10193. lr 3.000000e-04, running loss 1.11375, it/sec: 4.49897030499576
epoch 1 iter 35280: train loss 1.09767. lr 3.000000e-04, running loss 1.11374, it/sec: 4.511022891910592
epoch 1 iter 35300: train loss 1.11667. lr 3.000000e-04, running loss 1.11369, it/sec: 4.466800283217887
epoch 1 iter 35320: train loss 1.11833. lr 3.000000e-04, running loss 1.11366, it/sec: 4.514087913645076
epoch 1 iter 35340: train loss 1.11180. lr 3.000000e-04, running loss 1.11371, it/sec: 4.427963662034887
epoch 1 iter 35360: train loss 1.12096. lr 3.000000e-04, running loss 1.11368, it/sec: 4.524631062303034
epoch 1 iter 35380: train loss 1.14031. lr 3.000000e-04, running loss 1.11370, it/sec: 4.476161470820456
epoch 1 iter 35400: train loss 1.17696. lr 3.000000e-04, running loss 1.11366, it/sec: 4.494347058394791
epoch 1 iter 35420: train loss 1.09678. lr 3.000000e-04, running loss 1.11365, it/sec: 4.475571667487309
epoch 1 iter 35440: train loss 1.12139. lr 3.000000e-04, running loss 1.11364, it/sec: 4.517661644345815
epoch 1 iter 35460: train loss 1.12849. lr 3.000000e-04, running loss 1.11363, it/sec: 4.470898432465362
epoch 1 iter 35480: train loss 1.11005. lr 3.000000e-04, running loss 1.11367, it/sec: 4.504841981446381
epoch 1 iter 35500: train loss 1.11002. lr 3.000000e-04, running loss 1.11358, it/sec: 4.487520673666258
epoch 1 iter 35520: train loss 1.11152. lr 3.000000e-04, running loss 1.11357, it/sec: 4.5031279674058515
epoch 1 iter 35540: train loss 1.11657. lr 3.000000e-04, running loss 1.11360, it/sec: 4.4710662051267684
epoch 1 iter 35560: train loss 1.10523. lr 3.000000e-04, running loss 1.11362, it/sec: 4.503606176859611
epoch 1 iter 35580: train loss 1.11705. lr 3.000000e-04, running loss 1.11360, it/sec: 4.453953159417829
epoch 1 iter 35600: train loss 1.13503. lr 3.000000e-04, running loss 1.11372, it/sec: 4.50877793260506
epoch 1 iter 35620: train loss 1.10299. lr 3.000000e-04, running loss 1.11380, it/sec: 4.461318798782688
epoch 1 iter 35640: train loss 1.13490. lr 3.000000e-04, running loss 1.11386, it/sec: 4.499576799269925
epoch 1 iter 35660: train loss 1.11725. lr 3.000000e-04, running loss 1.11457, it/sec: 4.4398793802633545
epoch 1 iter 35680: train loss 1.09948. lr 3.000000e-04, running loss 1.11465, it/sec: 4.5124506051622015
epoch 1 iter 35700: train loss 1.11624. lr 3.000000e-04, running loss 1.11455, it/sec: 4.442377711627575
epoch 1 iter 35720: train loss 1.10177. lr 3.000000e-04, running loss 1.11452, it/sec: 4.508859209587346
epoch 1 iter 35740: train loss 1.14893. lr 3.000000e-04, running loss 1.11455, it/sec: 4.469826043801478
epoch 1 iter 35760: train loss 1.10668. lr 3.000000e-04, running loss 1.11462, it/sec: 4.512550403109328
epoch 1 iter 35780: train loss 1.11174. lr 3.000000e-04, running loss 1.11455, it/sec: 4.472753615425416
epoch 1 iter 35800: train loss 1.10462. lr 3.000000e-04, running loss 1.11448, it/sec: 4.484876382678567
epoch 1 iter 35820: train loss 1.11427. lr 3.000000e-04, running loss 1.11457, it/sec: 4.4690480836473405
epoch 1 iter 35840: train loss 1.08333. lr 3.000000e-04, running loss 1.11458, it/sec: 4.495235329826212
epoch 1 iter 35860: train loss 1.12899. lr 3.000000e-04, running loss 1.11452, it/sec: 4.475095086726518
epoch 1 iter 35880: train loss 1.10226. lr 3.000000e-04, running loss 1.11446, it/sec: 4.505728012377198
epoch 1 iter 35900: train loss 1.09327. lr 3.000000e-04, running loss 1.11442, it/sec: 4.448420700426599
epoch 1 iter 35920: train loss 1.13188. lr 3.000000e-04, running loss 1.11455, it/sec: 4.5057098224229115
epoch 1 iter 35940: train loss 1.09046. lr 3.000000e-04, running loss 1.11448, it/sec: 4.443309110146673
epoch 1 iter 35960: train loss 1.11228. lr 3.000000e-04, running loss 1.11445, it/sec: 4.47709227698232
epoch 1 iter 35980: train loss 1.10644. lr 3.000000e-04, running loss 1.11449, it/sec: 4.465020169954371
epoch 1 iter 36000: train loss 1.12182. lr 3.000000e-04, running loss 1.11448, it/sec: 4.50446400475201
epoch 1 iter 36020: train loss 1.10968. lr 3.000000e-04, running loss 1.11447, it/sec: 4.5160609416698
epoch 1 iter 36040: train loss 1.13675. lr 3.000000e-04, running loss 1.11451, it/sec: 4.472056591423927
epoch 1 iter 36060: train loss 1.09569. lr 3.000000e-04, running loss 1.11443, it/sec: 4.508878848562211
epoch 1 iter 36080: train loss 1.11255. lr 3.000000e-04, running loss 1.11441, it/sec: 4.452199344888449
epoch 1 iter 36100: train loss 1.08900. lr 3.000000e-04, running loss 1.11438, it/sec: 4.517505029438045
epoch 1 iter 36120: train loss 1.11224. lr 3.000000e-04, running loss 1.11437, it/sec: 4.458694432392252
epoch 1 iter 36140: train loss 1.11164. lr 3.000000e-04, running loss 1.11454, it/sec: 4.503969749204465
epoch 1 iter 36160: train loss 1.11222. lr 3.000000e-04, running loss 1.11455, it/sec: 4.447423803182061
epoch 1 iter 36180: train loss 1.09604. lr 3.000000e-04, running loss 1.11460, it/sec: 4.505553465539213
epoch 1 iter 36200: train loss 1.10525. lr 3.000000e-04, running loss 1.11462, it/sec: 4.464314852198949
epoch 1 iter 36220: train loss 1.12338. lr 3.000000e-04, running loss 1.11464, it/sec: 4.497615272398405
epoch 1 iter 36240: train loss 1.12666. lr 3.000000e-04, running loss 1.11464, it/sec: 4.476296116094331
epoch 1 iter 36260: train loss 1.11687. lr 3.000000e-04, running loss 1.11457, it/sec: 4.490107096732964
epoch 1 iter 36280: train loss 1.11334. lr 3.000000e-04, running loss 1.11457, it/sec: 4.459809558249039
epoch 1 iter 36300: train loss 1.11082. lr 3.000000e-04, running loss 1.11454, it/sec: 4.506792229631452
epoch 1 iter 36320: train loss 1.11782. lr 3.000000e-04, running loss 1.11455, it/sec: 4.440629450473336
epoch 1 iter 36340: train loss 1.12233. lr 3.000000e-04, running loss 1.11447, it/sec: 4.514642784599811
epoch 1 iter 36360: train loss 1.12146. lr 3.000000e-04, running loss 1.11466, it/sec: 4.450407416840835
epoch 1 iter 36380: train loss 1.09533. lr 3.000000e-04, running loss 1.11461, it/sec: 4.476540523653194
epoch 1 iter 36400: train loss 1.09832. lr 3.000000e-04, running loss 1.11447, it/sec: 4.4924852244134605
epoch 1 iter 36420: train loss 1.10734. lr 3.000000e-04, running loss 1.11442, it/sec: 4.497618670539507
epoch 1 iter 36440: train loss 1.12462. lr 3.000000e-04, running loss 1.11433, it/sec: 4.478371573035642
epoch 1 iter 36460: train loss 1.13276. lr 3.000000e-04, running loss 1.11447, it/sec: 4.478053388735961
epoch 1 iter 36480: train loss 1.08608. lr 3.000000e-04, running loss 1.11453, it/sec: 4.482374436620013
epoch 1 iter 36500: train loss 1.08980. lr 3.000000e-04, running loss 1.11440, it/sec: 4.526011705005582
epoch 1 iter 36520: train loss 1.10698. lr 3.000000e-04, running loss 1.11441, it/sec: 4.457971316470694
epoch 1 iter 36540: train loss 1.09537. lr 3.000000e-04, running loss 1.11439, it/sec: 4.503937190455515
epoch 1 iter 36560: train loss 1.08771. lr 3.000000e-04, running loss 1.11430, it/sec: 4.485094852081215
epoch 1 iter 36580: train loss 1.09380. lr 3.000000e-04, running loss 1.11426, it/sec: 4.4816595071041325
epoch 1 iter 36600: train loss 1.11671. lr 3.000000e-04, running loss 1.11433, it/sec: 4.500320239909111
epoch 1 iter 36620: train loss 1.09731. lr 3.000000e-04, running loss 1.11423, it/sec: 4.453716548471354
epoch 1 iter 36640: train loss 1.10765. lr 3.000000e-04, running loss 1.11419, it/sec: 4.514795369997103
epoch 1 iter 36660: train loss 1.09188. lr 3.000000e-04, running loss 1.11410, it/sec: 4.48692163304947
epoch 1 iter 36680: train loss 1.11698. lr 3.000000e-04, running loss 1.11400, it/sec: 4.511380437438805
epoch 1 iter 36700: train loss 1.10943. lr 3.000000e-04, running loss 1.11394, it/sec: 4.476026651722453
epoch 1 iter 36720: train loss 1.11272. lr 3.000000e-04, running loss 1.11379, it/sec: 4.512141079326599
epoch 1 iter 36740: train loss 1.09288. lr 3.000000e-04, running loss 1.11371, it/sec: 4.4634185359698275
epoch 1 iter 36760: train loss 1.11484. lr 3.000000e-04, running loss 1.11421, it/sec: 4.501122884262027
epoch 1 iter 36780: train loss 1.11597. lr 3.000000e-04, running loss 1.11418, it/sec: 4.433299739512213
epoch 1 iter 36800: train loss 1.10662. lr 3.000000e-04, running loss 1.11402, it/sec: 4.499947030269653
epoch 1 iter 36820: train loss 1.12065. lr 3.000000e-04, running loss 1.11399, it/sec: 4.475530684979365
epoch 1 iter 36840: train loss 1.11365. lr 3.000000e-04, running loss 1.11401, it/sec: 4.536784214366125
epoch 1 iter 36860: train loss 1.11437. lr 3.000000e-04, running loss 1.11406, it/sec: 4.455219362315438
epoch 1 iter 36880: train loss 1.08504. lr 3.000000e-04, running loss 1.11398, it/sec: 4.5089908079501635
epoch 1 iter 36900: train loss 1.11328. lr 3.000000e-04, running loss 1.11393, it/sec: 4.434707777387123
epoch 1 iter 36920: train loss 1.13869. lr 3.000000e-04, running loss 1.11384, it/sec: 4.497816474513732
epoch 1 iter 36940: train loss 1.08843. lr 3.000000e-04, running loss 1.11383, it/sec: 4.4658739311299716
epoch 1 iter 36960: train loss 1.09308. lr 3.000000e-04, running loss 1.11424, it/sec: 4.5316265341463655
epoch 1 iter 36980: train loss 1.10852. lr 3.000000e-04, running loss 1.11419, it/sec: 4.4771836610356335
epoch 1 iter 37000: train loss 1.10633. lr 3.000000e-04, running loss 1.11411, it/sec: 4.5009322241732415
epoch 1 iter 37020: train loss 1.14317. lr 3.000000e-04, running loss 1.11419, it/sec: 4.458505123486576
epoch 1 iter 37040: train loss 1.10434. lr 3.000000e-04, running loss 1.11425, it/sec: 4.481251914103208
epoch 1 iter 37060: train loss 1.09949. lr 3.000000e-04, running loss 1.11423, it/sec: 4.522262492559788
epoch 1 iter 37080: train loss 1.10812. lr 3.000000e-04, running loss 1.11427, it/sec: 4.518198450978516
epoch 1 iter 37100: train loss 1.11335. lr 3.000000e-04, running loss 1.11430, it/sec: 4.477421749864797
epoch 1 iter 37120: train loss 1.09645. lr 3.000000e-04, running loss 1.11434, it/sec: 4.529220132913681
epoch 1 iter 37140: train loss 1.09815. lr 3.000000e-04, running loss 1.11433, it/sec: 4.428218291638607
epoch 1 iter 37160: train loss 1.12401. lr 3.000000e-04, running loss 1.11422, it/sec: 4.504837132448831
epoch 1 iter 37180: train loss 1.09338. lr 3.000000e-04, running loss 1.11416, it/sec: 4.469227582169376
epoch 1 iter 37200: train loss 1.12489. lr 3.000000e-04, running loss 1.11416, it/sec: 4.51105146309661
epoch 1 iter 37220: train loss 1.09878. lr 3.000000e-04, running loss 1.11426, it/sec: 4.444790743367064
epoch 1 iter 37240: train loss 1.09740. lr 3.000000e-04, running loss 1.11432, it/sec: 4.515817726449863
epoch 1 iter 37260: train loss 1.10261. lr 3.000000e-04, running loss 1.11428, it/sec: 4.501898958939521
epoch 1 iter 37280: train loss 1.10751. lr 3.000000e-04, running loss 1.11424, it/sec: 4.4951545832570305
epoch 1 iter 37300: train loss 1.11037. lr 3.000000e-04, running loss 1.11432, it/sec: 4.483367064324938
epoch 1 iter 37320: train loss 1.12691. lr 3.000000e-04, running loss 1.11448, it/sec: 4.484062692712251
epoch 1 iter 37340: train loss 1.11859. lr 3.000000e-04, running loss 1.11454, it/sec: 4.476805962181991
epoch 1 iter 37360: train loss 1.12050. lr 3.000000e-04, running loss 1.11463, it/sec: 4.455877016714721
epoch 1 iter 37380: train loss 1.12261. lr 3.000000e-04, running loss 1.11507, it/sec: 4.497662122690362
epoch 1 iter 37400: train loss 1.11267. lr 3.000000e-04, running loss 1.11497, it/sec: 4.521861854841902
epoch 1 iter 37420: train loss 1.11423. lr 3.000000e-04, running loss 1.11494, it/sec: 4.444507338812688
epoch 1 iter 37440: train loss 2.40828. lr 3.000000e-04, running loss 1.11624, it/sec: 4.514735973781952
epoch 1 iter 37460: train loss 1.10644. lr 3.000000e-04, running loss 1.11616, it/sec: 4.43330752230686
epoch 1 iter 37480: train loss 1.10719. lr 3.000000e-04, running loss 1.11613, it/sec: 4.511538765268698
epoch 1 iter 37500: train loss 1.21418. lr 3.000000e-04, running loss 1.11616, it/sec: 1.1070891289001787
epoch 1 iter 37520: train loss 1.11586. lr 3.000000e-04, running loss 1.11607, it/sec: 4.485761256087888
epoch 1 iter 37540: train loss 1.11859. lr 3.000000e-04, running loss 1.11672, it/sec: 4.488575239250034
epoch 1 iter 37560: train loss 1.09632. lr 3.000000e-04, running loss 1.11663, it/sec: 4.469179805170929
epoch 1 iter 37580: train loss 1.11190. lr 3.000000e-04, running loss 1.11645, it/sec: 4.507914558756132
epoch 1 iter 37600: train loss 1.11986. lr 3.000000e-04, running loss 1.11926, it/sec: 4.444230015353061
epoch 1 iter 37620: train loss 1.11825. lr 3.000000e-04, running loss 1.11919, it/sec: 4.511804522071034
epoch 1 iter 37640: train loss 1.12639. lr 3.000000e-04, running loss 1.11916, it/sec: 4.47942922388278
epoch 1 iter 37660: train loss 1.14411. lr 3.000000e-04, running loss 1.11914, it/sec: 4.506046404673604
epoch 1 iter 37680: train loss 1.12450. lr 3.000000e-04, running loss 1.11913, it/sec: 4.452676672202971
epoch 1 iter 37700: train loss 1.12443. lr 3.000000e-04, running loss 1.11896, it/sec: 4.444712707292164
epoch 1 iter 37720: train loss 1.12008. lr 3.000000e-04, running loss 1.11876, it/sec: 4.5211492375491265
epoch 1 iter 37740: train loss 1.09867. lr 3.000000e-04, running loss 1.11866, it/sec: 4.460643558911567
epoch 1 iter 37760: train loss 1.09558. lr 3.000000e-04, running loss 1.11848, it/sec: 4.514191105248735
epoch 1 iter 37780: train loss 1.12786. lr 3.000000e-04, running loss 1.11843, it/sec: 4.494696430136441
epoch 1 iter 37800: train loss 1.14269. lr 3.000000e-04, running loss 1.11850, it/sec: 4.487132490541277
epoch 1 iter 37820: train loss 1.10103. lr 3.000000e-04, running loss 1.11834, it/sec: 4.48461437157758
epoch 1 iter 37840: train loss 1.11173. lr 3.000000e-04, running loss 1.11832, it/sec: 4.497933590767567
epoch 1 iter 37860: train loss 1.11476. lr 3.000000e-04, running loss 1.11823, it/sec: 4.490682043104906
epoch 1 iter 37880: train loss 1.11343. lr 3.000000e-04, running loss 1.11826, it/sec: 4.479846119253631
epoch 1 iter 37900: train loss 1.11402. lr 3.000000e-04, running loss 1.11827, it/sec: 4.503261117726266
epoch 1 iter 37920: train loss 1.09275. lr 3.000000e-04, running loss 1.11815, it/sec: 4.4857632483981185
epoch 1 iter 37940: train loss 1.11146. lr 3.000000e-04, running loss 1.11807, it/sec: 4.485575537443991
epoch 1 iter 37960: train loss 1.10979. lr 3.000000e-04, running loss 1.11791, it/sec: 4.462521145709015
epoch 1 iter 37980: train loss 1.12008. lr 3.000000e-04, running loss 1.11784, it/sec: 4.515371376744228
epoch 1 iter 38000: train loss 1.12016. lr 3.000000e-04, running loss 1.11779, it/sec: 4.510232906543729
epoch 1 iter 38020: train loss 1.12112. lr 3.000000e-04, running loss 1.11760, it/sec: 4.462722386043292
epoch 1 iter 38040: train loss 1.12480. lr 3.000000e-04, running loss 1.11762, it/sec: 4.505080221239598
epoch 1 iter 38060: train loss 1.10040. lr 3.000000e-04, running loss 1.11750, it/sec: 4.455628865103981
epoch 1 iter 38080: train loss 1.11752. lr 3.000000e-04, running loss 1.11744, it/sec: 4.512213275192751
epoch 1 iter 38100: train loss 1.10197. lr 3.000000e-04, running loss 1.11736, it/sec: 4.465487647600465
epoch 1 iter 38120: train loss 1.10882. lr 3.000000e-04, running loss 1.11718, it/sec: 4.49387547634936
epoch 1 iter 38140: train loss 1.10886. lr 3.000000e-04, running loss 1.11711, it/sec: 4.52375749385752
epoch 1 iter 38160: train loss 1.14609. lr 3.000000e-04, running loss 1.11700, it/sec: 4.50197615748165
epoch 1 iter 38180: train loss 1.11942. lr 3.000000e-04, running loss 1.11710, it/sec: 4.511441068711427
epoch 1 iter 38200: train loss 1.10113. lr 3.000000e-04, running loss 1.11701, it/sec: 4.5112496969926354
epoch 1 iter 38220: train loss 1.10137. lr 3.000000e-04, running loss 1.11689, it/sec: 4.4849466818638914
epoch 1 iter 38240: train loss 1.12374. lr 3.000000e-04, running loss 1.11688, it/sec: 4.459037804952274
epoch 1 iter 38260: train loss 1.12599. lr 3.000000e-04, running loss 1.11681, it/sec: 4.4943929094134445
epoch 1 iter 38280: train loss 1.11495. lr 3.000000e-04, running loss 1.11668, it/sec: 4.480443634186294
epoch 1 iter 38300: train loss 1.10308. lr 3.000000e-04, running loss 1.11665, it/sec: 4.4584620069409855
epoch 1 iter 38320: train loss 1.12599. lr 3.000000e-04, running loss 1.11737, it/sec: 4.5064503572281085
epoch 1 iter 38340: train loss 1.10730. lr 3.000000e-04, running loss 1.11718, it/sec: 4.455266822808862
epoch 1 iter 38360: train loss 1.09653. lr 3.000000e-04, running loss 1.11713, it/sec: 4.500879005439324
epoch 1 iter 38380: train loss 1.11130. lr 3.000000e-04, running loss 1.11707, it/sec: 4.43835228321818
epoch 1 iter 38400: train loss 1.10331. lr 3.000000e-04, running loss 1.11693, it/sec: 4.508202266371936
epoch 1 iter 38420: train loss 1.10431. lr 3.000000e-04, running loss 1.11690, it/sec: 4.456862570370686
epoch 1 iter 38440: train loss 1.09988. lr 3.000000e-04, running loss 1.11681, it/sec: 4.5203898471459345
epoch 1 iter 38460: train loss 1.10465. lr 3.000000e-04, running loss 1.11679, it/sec: 4.482999596179124
epoch 1 iter 38480: train loss 1.11786. lr 3.000000e-04, running loss 1.11681, it/sec: 4.466003271385824
epoch 1 iter 38500: train loss 1.08915. lr 3.000000e-04, running loss 1.11663, it/sec: 4.4756735463749875
epoch 1 iter 38520: train loss 1.11650. lr 3.000000e-04, running loss 1.11659, it/sec: 4.495006696741011
epoch 1 iter 38540: train loss 1.09455. lr 3.000000e-04, running loss 1.11646, it/sec: 4.477729236909011
epoch 1 iter 38560: train loss 1.14561. lr 3.000000e-04, running loss 1.11647, it/sec: 4.531041693283113
epoch 1 iter 38580: train loss 1.10783. lr 3.000000e-04, running loss 1.11639, it/sec: 4.491049400573817
epoch 1 iter 38600: train loss 1.10128. lr 3.000000e-04, running loss 1.11627, it/sec: 4.518707411882498
epoch 1 iter 38620: train loss 1.10050. lr 3.000000e-04, running loss 1.11630, it/sec: 4.471943278478668
epoch 1 iter 38640: train loss 1.12034. lr 3.000000e-04, running loss 1.11630, it/sec: 4.5041561014256555
epoch 1 iter 38660: train loss 1.09673. lr 3.000000e-04, running loss 1.11616, it/sec: 4.461529465097636
epoch 1 iter 38680: train loss 1.11805. lr 3.000000e-04, running loss 1.11610, it/sec: 4.518297319773513
epoch 1 iter 38700: train loss 1.09969. lr 3.000000e-04, running loss 1.11601, it/sec: 4.430412854358066
epoch 1 iter 38720: train loss 1.10010. lr 3.000000e-04, running loss 1.11599, it/sec: 4.519824877393949
epoch 1 iter 38740: train loss 1.10982. lr 3.000000e-04, running loss 1.11589, it/sec: 4.455220097126218
epoch 1 iter 38760: train loss 1.11573. lr 3.000000e-04, running loss 1.11583, it/sec: 4.505929291017545
epoch 1 iter 38780: train loss 1.09999. lr 3.000000e-04, running loss 1.11574, it/sec: 4.483417798775469
epoch 1 iter 38800: train loss 1.12185. lr 3.000000e-04, running loss 1.11578, it/sec: 4.493809662307057
epoch 1 iter 38820: train loss 1.13008. lr 3.000000e-04, running loss 1.11569, it/sec: 4.49248316503968
epoch 1 iter 38840: train loss 1.32655. lr 3.000000e-04, running loss 1.11587, it/sec: 4.491214837946592
epoch 1 iter 38860: train loss 1.16342. lr 3.000000e-04, running loss 1.11577, it/sec: 4.490442401137147
epoch 1 iter 38880: train loss 1.10693. lr 3.000000e-04, running loss 1.11574, it/sec: 4.518793458030192
epoch 1 iter 38900: train loss 1.10576. lr 3.000000e-04, running loss 1.11570, it/sec: 4.441552678515516
epoch 1 iter 38920: train loss 1.10542. lr 3.000000e-04, running loss 1.11565, it/sec: 4.49473008663506
epoch 1 iter 38940: train loss 1.11660. lr 3.000000e-04, running loss 1.11554, it/sec: 4.455931221037711
epoch 1 iter 38960: train loss 1.11654. lr 3.000000e-04, running loss 1.11551, it/sec: 4.522799083449313
epoch 1 iter 38980: train loss 1.10825. lr 3.000000e-04, running loss 1.11540, it/sec: 4.454155057373831
epoch 1 iter 39000: train loss 1.09951. lr 3.000000e-04, running loss 1.11537, it/sec: 4.504707744260031
epoch 1 iter 39020: train loss 1.10969. lr 3.000000e-04, running loss 1.11528, it/sec: 4.466173848844628
epoch 1 iter 39040: train loss 1.10383. lr 3.000000e-04, running loss 1.11516, it/sec: 4.509550874144822
epoch 1 iter 39060: train loss 1.10330. lr 3.000000e-04, running loss 1.11501, it/sec: 4.47564752500011
epoch 1 iter 39080: train loss 1.09934. lr 3.000000e-04, running loss 1.11495, it/sec: 4.489421262510884
epoch 1 iter 39100: train loss 1.13083. lr 3.000000e-04, running loss 1.11494, it/sec: 4.505471354161537
epoch 1 iter 39120: train loss 1.12584. lr 3.000000e-04, running loss 1.11497, it/sec: 4.479914355484808
epoch 1 iter 39140: train loss 1.11875. lr 3.000000e-04, running loss 1.11480, it/sec: 4.504622821908772
epoch 1 iter 39160: train loss 1.10134. lr 3.000000e-04, running loss 1.11478, it/sec: 4.468226310005709
epoch 1 iter 39180: train loss 1.10977. lr 3.000000e-04, running loss 1.11483, it/sec: 4.5061502241285565
epoch 1 iter 39200: train loss 1.12240. lr 3.000000e-04, running loss 1.11478, it/sec: 4.510376832711429
epoch 1 iter 39220: train loss 1.11773. lr 3.000000e-04, running loss 1.11488, it/sec: 4.451211354484696
epoch 1 iter 39240: train loss 1.20114. lr 3.000000e-04, running loss 1.11499, it/sec: 4.507669477434002
epoch 1 iter 39260: train loss 1.12637. lr 3.000000e-04, running loss 1.11503, it/sec: 4.466807725334041
epoch 1 iter 39280: train loss 1.11599. lr 3.000000e-04, running loss 1.11501, it/sec: 4.524296835647305
epoch 1 iter 39300: train loss 1.11793. lr 3.000000e-04, running loss 1.11504, it/sec: 4.486761807075484
epoch 1 iter 39320: train loss 1.12160. lr 3.000000e-04, running loss 1.11503, it/sec: 4.494408807050208
epoch 1 iter 39340: train loss 1.11209. lr 3.000000e-04, running loss 1.11490, it/sec: 4.499540598906792
epoch 1 iter 39360: train loss 1.10865. lr 3.000000e-04, running loss 1.11498, it/sec: 4.46545363022476
epoch 1 iter 39380: train loss 1.09542. lr 3.000000e-04, running loss 1.11507, it/sec: 4.513427632995077
epoch 1 iter 39400: train loss 1.09745. lr 3.000000e-04, running loss 1.11520, it/sec: 4.454036716669254
epoch 1 iter 39420: train loss 1.11931. lr 3.000000e-04, running loss 1.11514, it/sec: 4.493306314292618
epoch 1 iter 39440: train loss 1.09949. lr 3.000000e-04, running loss 1.11510, it/sec: 4.4490223689771655
epoch 1 iter 39460: train loss 1.11441. lr 3.000000e-04, running loss 1.11495, it/sec: 4.504681872002581
epoch 1 iter 39480: train loss 1.10628. lr 3.000000e-04, running loss 1.11493, it/sec: 4.445863148982695
epoch 1 iter 39500: train loss 1.10727. lr 3.000000e-04, running loss 1.11489, it/sec: 4.514845288847082
epoch 1 iter 39520: train loss 1.11901. lr 3.000000e-04, running loss 1.11477, it/sec: 4.465341228467108
epoch 1 iter 39540: train loss 1.12416. lr 3.000000e-04, running loss 1.11472, it/sec: 4.465652384085893
epoch 1 iter 39560: train loss 1.12464. lr 3.000000e-04, running loss 1.11470, it/sec: 4.438734417145457
epoch 1 iter 39580: train loss 1.13725. lr 3.000000e-04, running loss 1.11485, it/sec: 4.439221355441172
epoch 1 iter 39600: train loss 1.11130. lr 3.000000e-04, running loss 1.11476, it/sec: 4.492947752612579
epoch 1 iter 39620: train loss 1.10148. lr 3.000000e-04, running loss 1.11480, it/sec: 4.4599232123047035
epoch 1 iter 39640: train loss 1.10415. lr 3.000000e-04, running loss 1.11476, it/sec: 4.515106850199606
epoch 1 iter 39660: train loss 1.11634. lr 3.000000e-04, running loss 1.11466, it/sec: 4.458791249923356
epoch 1 iter 39680: train loss 1.09933. lr 3.000000e-04, running loss 1.11464, it/sec: 4.486054111884521
epoch 1 iter 39700: train loss 1.10763. lr 3.000000e-04, running loss 1.11469, it/sec: 4.4503722419548675
epoch 1 iter 39720: train loss 1.09974. lr 3.000000e-04, running loss 1.11467, it/sec: 4.480568881475821
epoch 1 iter 39740: train loss 1.11985. lr 3.000000e-04, running loss 1.11466, it/sec: 4.495629260737926
epoch 1 iter 39760: train loss 1.10536. lr 3.000000e-04, running loss 1.11466, it/sec: 4.462220343788042
epoch 1 iter 39780: train loss 1.11978. lr 3.000000e-04, running loss 1.11470, it/sec: 4.494514736553679
epoch 1 iter 39800: train loss 1.13236. lr 3.000000e-04, running loss 1.11465, it/sec: 4.442281725039954
epoch 1 iter 39820: train loss 1.10200. lr 3.000000e-04, running loss 1.11465, it/sec: 4.471153265937014
epoch 1 iter 39840: train loss 1.10690. lr 3.000000e-04, running loss 1.11474, it/sec: 4.487839477963055
epoch 1 iter 39860: train loss 1.09148. lr 3.000000e-04, running loss 1.11465, it/sec: 4.519223575063357
epoch 1 iter 39880: train loss 1.11054. lr 3.000000e-04, running loss 1.11465, it/sec: 4.4522687618508465
epoch 1 iter 39900: train loss 1.10787. lr 3.000000e-04, running loss 1.11479, it/sec: 4.514438465712469
epoch 1 iter 39920: train loss 1.11047. lr 3.000000e-04, running loss 1.11481, it/sec: 4.432460352278088
epoch 1 iter 39940: train loss 1.10784. lr 3.000000e-04, running loss 1.11483, it/sec: 4.524015134188781
epoch 1 iter 39960: train loss 1.10302. lr 3.000000e-04, running loss 1.11484, it/sec: 4.438944985989507
epoch 1 iter 39980: train loss 1.11664. lr 3.000000e-04, running loss 1.11478, it/sec: 4.452356024476771
epoch 1 iter 40000: train loss 1.12387. lr 3.000000e-04, running loss 1.11479, it/sec: 4.498991679380707
epoch 1 iter 40020: train loss 1.10547. lr 3.000000e-04, running loss 1.11476, it/sec: 4.459333544469416
epoch 1 iter 40040: train loss 1.11343. lr 3.000000e-04, running loss 1.11482, it/sec: 4.5019512084655275
epoch 1 iter 40060: train loss 1.09625. lr 3.000000e-04, running loss 1.11476, it/sec: 4.444189486405696
epoch 1 iter 40080: train loss 1.13595. lr 3.000000e-04, running loss 1.11471, it/sec: 4.5194207706608704
epoch 1 iter 40100: train loss 1.11305. lr 3.000000e-04, running loss 1.11463, it/sec: 4.4432245322562975
epoch 1 iter 40120: train loss 1.10779. lr 3.000000e-04, running loss 1.11450, it/sec: 4.516300776837856
epoch 1 iter 40140: train loss 1.10352. lr 3.000000e-04, running loss 1.11446, it/sec: 4.467282981585448
epoch 1 iter 40160: train loss 1.11264. lr 3.000000e-04, running loss 1.11446, it/sec: 4.496587512705508
epoch 1 iter 40180: train loss 1.12432. lr 3.000000e-04, running loss 1.11446, it/sec: 4.489179718677411
epoch 1 iter 40200: train loss 1.12156. lr 3.000000e-04, running loss 1.11450, it/sec: 4.463445511193153
epoch 1 iter 40220: train loss 1.11576. lr 3.000000e-04, running loss 1.11446, it/sec: 4.516731541626917
epoch 1 iter 40240: train loss 1.10067. lr 3.000000e-04, running loss 1.11448, it/sec: 4.464918437809428
epoch 1 iter 40260: train loss 1.11770. lr 3.000000e-04, running loss 1.11447, it/sec: 4.5096391548844785
epoch 1 iter 40280: train loss 1.09091. lr 3.000000e-04, running loss 1.11452, it/sec: 4.465944094424465
epoch 1 iter 40300: train loss 1.08810. lr 3.000000e-04, running loss 1.11448, it/sec: 4.480248157942766
epoch 1 iter 40320: train loss 1.12001. lr 3.000000e-04, running loss 1.11448, it/sec: 4.462149918469653
epoch 1 iter 40340: train loss 1.12279. lr 3.000000e-04, running loss 1.11450, it/sec: 4.487123671455144
epoch 1 iter 40360: train loss 1.11752. lr 3.000000e-04, running loss 1.11456, it/sec: 4.504278864685382
epoch 1 iter 40380: train loss 1.11930. lr 3.000000e-04, running loss 1.11451, it/sec: 4.482051023537679
epoch 1 iter 40400: train loss 1.11945. lr 3.000000e-04, running loss 1.11449, it/sec: 4.4985997682143495
epoch 1 iter 40420: train loss 1.10793. lr 3.000000e-04, running loss 1.11450, it/sec: 4.442267220929276
epoch 1 iter 40440: train loss 1.12023. lr 3.000000e-04, running loss 1.11454, it/sec: 4.499482352875411
epoch 1 iter 40460: train loss 1.11994. lr 3.000000e-04, running loss 1.11467, it/sec: 4.477835183427711
epoch 1 iter 40480: train loss 1.08903. lr 3.000000e-04, running loss 1.11465, it/sec: 4.498994614221441
epoch 1 iter 40500: train loss 1.09759. lr 3.000000e-04, running loss 1.11465, it/sec: 4.4676674589472105
epoch 1 iter 40520: train loss 1.09562. lr 3.000000e-04, running loss 1.11456, it/sec: 4.479677044595685
epoch 1 iter 40540: train loss 1.09845. lr 3.000000e-04, running loss 1.11452, it/sec: 4.497890377134478
epoch 1 iter 40560: train loss 1.11446. lr 3.000000e-04, running loss 1.11446, it/sec: 4.448787113376286
epoch 1 iter 40580: train loss 1.15163. lr 3.000000e-04, running loss 1.11453, it/sec: 4.507370299456961
epoch 1 iter 40600: train loss 1.10413. lr 3.000000e-04, running loss 1.11454, it/sec: 4.42131440637572
epoch 1 iter 40620: train loss 1.11090. lr 3.000000e-04, running loss 1.11450, it/sec: 4.50708428245936
epoch 1 iter 40640: train loss 1.11742. lr 3.000000e-04, running loss 1.11440, it/sec: 4.416279299812186
epoch 1 iter 40660: train loss 1.10676. lr 3.000000e-04, running loss 1.11439, it/sec: 4.482512710825805
epoch 1 iter 40680: train loss 1.10629. lr 3.000000e-04, running loss 1.11440, it/sec: 4.466186075908974
epoch 1 iter 40700: train loss 1.09844. lr 3.000000e-04, running loss 1.11429, it/sec: 4.501993872536333
epoch 1 iter 40720: train loss 1.13909. lr 3.000000e-04, running loss 1.11425, it/sec: 4.4490343052914865
epoch 1 iter 40740: train loss 1.12521. lr 3.000000e-04, running loss 1.11435, it/sec: 4.498663557464906
epoch 1 iter 40760: train loss 1.10219. lr 3.000000e-04, running loss 1.11427, it/sec: 4.46844583803078
epoch 1 iter 40780: train loss 1.09727. lr 3.000000e-04, running loss 1.11419, it/sec: 4.487648350781323
epoch 1 iter 40800: train loss 1.09595. lr 3.000000e-04, running loss 1.11426, it/sec: 4.505977249513778
epoch 1 iter 40820: train loss 1.11059. lr 3.000000e-04, running loss 1.11418, it/sec: 4.450677568721861
epoch 1 iter 40840: train loss 1.12597. lr 3.000000e-04, running loss 1.11415, it/sec: 4.509236440119094
epoch 1 iter 40860: train loss 1.10963. lr 3.000000e-04, running loss 1.11422, it/sec: 4.4667953160140215
epoch 1 iter 40880: train loss 1.10736. lr 3.000000e-04, running loss 1.11421, it/sec: 4.509050440081307
epoch 1 iter 40900: train loss 1.11800. lr 3.000000e-04, running loss 1.11416, it/sec: 4.45394683163771
epoch 1 iter 40920: train loss 1.10561. lr 3.000000e-04, running loss 1.11415, it/sec: 4.506886069224538
epoch 1 iter 40940: train loss 1.12492. lr 3.000000e-04, running loss 1.11409, it/sec: 4.480365787734456
epoch 1 iter 40960: train loss 1.12198. lr 3.000000e-04, running loss 1.11401, it/sec: 4.473845903294452
epoch 1 iter 40980: train loss 1.11015. lr 3.000000e-04, running loss 1.11396, it/sec: 4.474111602613028
epoch 1 iter 41000: train loss 1.11685. lr 3.000000e-04, running loss 1.11423, it/sec: 4.524676551382809
epoch 1 iter 41020: train loss 1.10250. lr 3.000000e-04, running loss 1.11419, it/sec: 4.418083214346842
epoch 1 iter 41040: train loss 1.10004. lr 3.000000e-04, running loss 1.11417, it/sec: 4.521480913254911
epoch 1 iter 41060: train loss 1.14256. lr 3.000000e-04, running loss 1.11420, it/sec: 4.466311067259933
epoch 1 iter 41080: train loss 1.14248. lr 3.000000e-04, running loss 1.11419, it/sec: 4.441161991607129
epoch 1 iter 41100: train loss 1.11580. lr 3.000000e-04, running loss 1.11422, it/sec: 4.474543605259658
epoch 1 iter 41120: train loss 1.10229. lr 3.000000e-04, running loss 1.11423, it/sec: 4.491539089926136
epoch 1 iter 41140: train loss 1.10179. lr 3.000000e-04, running loss 1.11428, it/sec: 4.512059683983938
epoch 1 iter 41160: train loss 1.10748. lr 3.000000e-04, running loss 1.11446, it/sec: 4.478321814249881
epoch 1 iter 41180: train loss 1.11765. lr 3.000000e-04, running loss 1.11451, it/sec: 4.5041040049956464
epoch 1 iter 41200: train loss 1.12566. lr 3.000000e-04, running loss 1.11449, it/sec: 4.445863070747669
epoch 1 iter 41220: train loss 1.11166. lr 3.000000e-04, running loss 1.11445, it/sec: 4.5038181378845765
epoch 1 iter 41240: train loss 1.13261. lr 3.000000e-04, running loss 1.11444, it/sec: 4.4844094617900065
epoch 1 iter 41260: train loss 1.11270. lr 3.000000e-04, running loss 1.11444, it/sec: 4.486424637413044
epoch 1 iter 41280: train loss 1.18731. lr 3.000000e-04, running loss 1.11453, it/sec: 4.4704688713050835
epoch 1 iter 41300: train loss 1.11751. lr 3.000000e-04, running loss 1.11453, it/sec: 4.4824167491578955
epoch 1 iter 41320: train loss 1.11668. lr 3.000000e-04, running loss 1.11456, it/sec: 4.44561775190233
epoch 1 iter 41340: train loss 1.13396. lr 3.000000e-04, running loss 1.11461, it/sec: 4.465683871630398
epoch 1 iter 41360: train loss 1.09822. lr 3.000000e-04, running loss 1.11446, it/sec: 4.500754928794735
epoch 1 iter 41380: train loss 1.14597. lr 3.000000e-04, running loss 1.11444, it/sec: 4.451191521533832
epoch 1 iter 41400: train loss 1.10244. lr 3.000000e-04, running loss 1.11451, it/sec: 4.528528039546301
epoch 1 iter 41420: train loss 1.11547. lr 3.000000e-04, running loss 1.11466, it/sec: 4.44971965621574
epoch 1 iter 41440: train loss 1.10131. lr 3.000000e-04, running loss 1.11484, it/sec: 4.5078057611988935
epoch 1 iter 41460: train loss 1.11304. lr 3.000000e-04, running loss 1.11500, it/sec: 4.476254959368698
epoch 1 iter 41480: train loss 1.12093. lr 3.000000e-04, running loss 1.11498, it/sec: 4.48479236678559
epoch 1 iter 41500: train loss 1.09891. lr 3.000000e-04, running loss 1.11499, it/sec: 4.479065210575232
epoch 1 iter 41520: train loss 1.11186. lr 3.000000e-04, running loss 1.11489, it/sec: 4.466734422499853
epoch 1 iter 41540: train loss 1.12108. lr 3.000000e-04, running loss 1.11482, it/sec: 4.497538627194538
epoch 1 iter 41560: train loss 1.11456. lr 3.000000e-04, running loss 1.11479, it/sec: 4.444215122577618
epoch 1 iter 41580: train loss 1.10900. lr 3.000000e-04, running loss 1.11470, it/sec: 4.513099417563851
epoch 1 iter 41600: train loss 1.13467. lr 3.000000e-04, running loss 1.11551, it/sec: 4.448953805539644
epoch 1 iter 41620: train loss 1.12751. lr 3.000000e-04, running loss 1.11543, it/sec: 4.471851348126055
epoch 1 iter 41640: train loss 1.13009. lr 3.000000e-04, running loss 1.11543, it/sec: 4.4457296748286135
epoch 1 iter 41660: train loss 1.08802. lr 3.000000e-04, running loss 1.11541, it/sec: 4.50316542122387
epoch 1 iter 41680: train loss 1.12306. lr 3.000000e-04, running loss 1.11549, it/sec: 4.464134850240617
epoch 1 iter 41700: train loss 1.12379. lr 3.000000e-04, running loss 1.11550, it/sec: 4.500904268026263
epoch 1 iter 41720: train loss 1.11990. lr 3.000000e-04, running loss 1.11547, it/sec: 4.469676803802692
epoch 1 iter 41740: train loss 1.11467. lr 3.000000e-04, running loss 1.11546, it/sec: 4.471591096399958
epoch 1 iter 41760: train loss 1.13321. lr 3.000000e-04, running loss 1.11546, it/sec: 4.50433338025584
epoch 1 iter 41780: train loss 1.10363. lr 3.000000e-04, running loss 1.11541, it/sec: 4.481875576132144
epoch 1 iter 41800: train loss 1.10411. lr 3.000000e-04, running loss 1.11538, it/sec: 4.499509116448492
epoch 1 iter 41820: train loss 1.11678. lr 3.000000e-04, running loss 1.11627, it/sec: 4.43909665527703
epoch 1 iter 41840: train loss 1.12250. lr 3.000000e-04, running loss 1.11625, it/sec: 4.499517538832034
epoch 1 iter 41860: train loss 1.12433. lr 3.000000e-04, running loss 1.11621, it/sec: 4.43372332663253
epoch 1 iter 41880: train loss 1.10991. lr 3.000000e-04, running loss 1.11613, it/sec: 4.489959463519281
epoch 1 iter 41900: train loss 1.12182. lr 3.000000e-04, running loss 1.11611, it/sec: 4.442153162442833
epoch 1 iter 41920: train loss 1.11842. lr 3.000000e-04, running loss 1.11601, it/sec: 4.485162745967304
epoch 1 iter 41940: train loss 1.10718. lr 3.000000e-04, running loss 1.11590, it/sec: 4.476003953315996
epoch 1 iter 41960: train loss 1.12207. lr 3.000000e-04, running loss 1.11594, it/sec: 4.524197501578966
epoch 1 iter 41980: train loss 1.09431. lr 3.000000e-04, running loss 1.11591, it/sec: 4.432662644058847
epoch 1 iter 42000: train loss 1.10247. lr 3.000000e-04, running loss 1.11581, it/sec: 4.491938629727965
epoch 1 iter 42020: train loss 1.11248. lr 3.000000e-04, running loss 1.11572, it/sec: 4.494217947601617
epoch 1 iter 42040: train loss 1.10572. lr 3.000000e-04, running loss 1.11576, it/sec: 4.467962987114863
epoch 1 iter 42060: train loss 1.11966. lr 3.000000e-04, running loss 1.11601, it/sec: 4.5155090654219805
epoch 1 iter 42080: train loss 1.11532. lr 3.000000e-04, running loss 1.11588, it/sec: 4.440922812827058
epoch 1 iter 42100: train loss 1.12234. lr 3.000000e-04, running loss 1.11590, it/sec: 4.507395877394934
epoch 1 iter 42120: train loss 1.46140. lr 3.000000e-04, running loss 1.11620, it/sec: 4.4476229541356584
epoch 1 iter 42140: train loss 1.09391. lr 3.000000e-04, running loss 1.11604, it/sec: 4.501270584999657
epoch 1 iter 42160: train loss 1.12312. lr 3.000000e-04, running loss 1.11604, it/sec: 4.451255120921061
epoch 1 iter 42180: train loss 1.11022. lr 3.000000e-04, running loss 1.11600, it/sec: 4.4904177405878904
epoch 1 iter 42200: train loss 1.10818. lr 3.000000e-04, running loss 1.11596, it/sec: 4.490646450550165
epoch 1 iter 42220: train loss 1.10273. lr 3.000000e-04, running loss 1.11604, it/sec: 4.486308341141813
epoch 1 iter 42240: train loss 1.10882. lr 3.000000e-04, running loss 1.11599, it/sec: 4.48764589493281
epoch 1 iter 42260: train loss 1.10583. lr 3.000000e-04, running loss 1.11588, it/sec: 4.495849144921856
epoch 1 iter 42280: train loss 1.09429. lr 3.000000e-04, running loss 1.11592, it/sec: 4.49164458239412
epoch 1 iter 42300: train loss 1.10647. lr 3.000000e-04, running loss 1.11591, it/sec: 4.46197504868344
epoch 1 iter 42320: train loss 1.10569. lr 3.000000e-04, running loss 1.11583, it/sec: 4.5159306432297255
epoch 1 iter 42340: train loss 1.09843. lr 3.000000e-04, running loss 1.11584, it/sec: 4.458065935838555
epoch 1 iter 42360: train loss 1.09979. lr 3.000000e-04, running loss 1.11586, it/sec: 4.501256200259399
epoch 1 iter 42380: train loss 1.13309. lr 3.000000e-04, running loss 1.11579, it/sec: 4.457240806203867
epoch 1 iter 42400: train loss 1.10349. lr 3.000000e-04, running loss 1.11571, it/sec: 4.490272282777935
epoch 1 iter 42420: train loss 1.11613. lr 3.000000e-04, running loss 1.11570, it/sec: 4.504495901456053
epoch 1 iter 42440: train loss 1.11351. lr 3.000000e-04, running loss 1.11558, it/sec: 4.47050180764881
epoch 1 iter 42460: train loss 1.10225. lr 3.000000e-04, running loss 1.11558, it/sec: 4.4709234992442735
epoch 1 iter 42480: train loss 1.10361. lr 3.000000e-04, running loss 1.11547, it/sec: 4.430106434638417
epoch 1 iter 42500: train loss 1.12413. lr 3.000000e-04, running loss 1.11538, it/sec: 4.4883231697337616
epoch 1 iter 42520: train loss 1.11251. lr 3.000000e-04, running loss 1.11523, it/sec: 4.4810964681144245
epoch 1 iter 42540: train loss 1.10700. lr 3.000000e-04, running loss 1.11523, it/sec: 4.493904558379583
epoch 1 iter 42560: train loss 1.10861. lr 3.000000e-04, running loss 1.11520, it/sec: 4.433203809631636
epoch 1 iter 42580: train loss 1.10318. lr 3.000000e-04, running loss 1.11509, it/sec: 4.528859835351699
epoch 1 iter 42600: train loss 1.10068. lr 3.000000e-04, running loss 1.11504, it/sec: 4.456421343411449
epoch 1 iter 42620: train loss 1.13422. lr 3.000000e-04, running loss 1.11503, it/sec: 4.484521898458492
epoch 1 iter 42640: train loss 1.11351. lr 3.000000e-04, running loss 1.11499, it/sec: 4.495713763150685
epoch 1 iter 42660: train loss 1.11542. lr 3.000000e-04, running loss 1.11483, it/sec: 4.446159831402634
epoch 1 iter 42680: train loss 1.11407. lr 3.000000e-04, running loss 1.11489, it/sec: 4.49493519273707
epoch 1 iter 42700: train loss 1.10536. lr 3.000000e-04, running loss 1.11486, it/sec: 4.448554375382237
epoch 1 iter 42720: train loss 1.10072. lr 3.000000e-04, running loss 1.11486, it/sec: 4.509323143835885
epoch 1 iter 42740: train loss 1.09104. lr 3.000000e-04, running loss 1.11482, it/sec: 4.463446546745796
epoch 1 iter 42760: train loss 1.11939. lr 3.000000e-04, running loss 1.11483, it/sec: 4.476916013593156
epoch 1 iter 42780: train loss 1.11040. lr 3.000000e-04, running loss 1.11488, it/sec: 4.461444213001585
epoch 1 iter 42800: train loss 1.10940. lr 3.000000e-04, running loss 1.11487, it/sec: 4.480771050527052
epoch 1 iter 42820: train loss 1.09102. lr 3.000000e-04, running loss 1.11478, it/sec: 4.4268662826613685
epoch 1 iter 42840: train loss 1.10877. lr 3.000000e-04, running loss 1.11474, it/sec: 4.492012925405953
epoch 1 iter 42860: train loss 1.10930. lr 3.000000e-04, running loss 1.11475, it/sec: 4.472244973618487
epoch 1 iter 42880: train loss 1.10641. lr 3.000000e-04, running loss 1.11473, it/sec: 4.491631852670739
epoch 1 iter 42900: train loss 1.11013. lr 3.000000e-04, running loss 1.11484, it/sec: 4.461149267345603
epoch 1 iter 42920: train loss 1.12967. lr 3.000000e-04, running loss 1.11475, it/sec: 4.474680317509251
epoch 1 iter 42940: train loss 1.12453. lr 3.000000e-04, running loss 1.11472, it/sec: 4.503485012107276
epoch 1 iter 42960: train loss 1.10726. lr 3.000000e-04, running loss 1.11468, it/sec: 4.430088732475206
epoch 1 iter 42980: train loss 1.09780. lr 3.000000e-04, running loss 1.11461, it/sec: 4.534101505357574
epoch 1 iter 43000: train loss 1.11215. lr 3.000000e-04, running loss 1.11444, it/sec: 4.436528573224829
epoch 1 iter 43020: train loss 1.10358. lr 3.000000e-04, running loss 1.11435, it/sec: 4.496534559075443
epoch 1 iter 43040: train loss 1.09992. lr 3.000000e-04, running loss 1.11448, it/sec: 4.517889157897167
epoch 1 iter 43060: train loss 1.11934. lr 3.000000e-04, running loss 1.11454, it/sec: 4.468386057059813
epoch 1 iter 43080: train loss 1.13356. lr 3.000000e-04, running loss 1.11465, it/sec: 4.4944434092844485
epoch 1 iter 43100: train loss 1.11302. lr 3.000000e-04, running loss 1.11463, it/sec: 4.44135422924476
epoch 1 iter 43120: train loss 1.11104. lr 3.000000e-04, running loss 1.11450, it/sec: 4.486678667291579
epoch 1 iter 43140: train loss 1.10867. lr 3.000000e-04, running loss 1.11448, it/sec: 4.462853896082089
epoch 1 iter 43160: train loss 1.09192. lr 3.000000e-04, running loss 1.11439, it/sec: 4.498718990356719
epoch 1 iter 43180: train loss 1.15384. lr 3.000000e-04, running loss 1.11443, it/sec: 4.456014933428426
epoch 1 iter 43200: train loss 1.10638. lr 3.000000e-04, running loss 1.11439, it/sec: 4.49983975196904
epoch 1 iter 43220: train loss 1.08592. lr 3.000000e-04, running loss 1.11443, it/sec: 4.454857803581079
epoch 1 iter 43240: train loss 1.13071. lr 3.000000e-04, running loss 1.11471, it/sec: 4.51643438197869
epoch 1 iter 43260: train loss 1.11597. lr 3.000000e-04, running loss 1.11480, it/sec: 4.487646982773729
epoch 1 iter 43280: train loss 1.13655. lr 3.000000e-04, running loss 1.11483, it/sec: 4.5138882881309454
epoch 1 iter 43300: train loss 1.13533. lr 3.000000e-04, running loss 1.11483, it/sec: 4.46936558709336
epoch 1 iter 43320: train loss 1.09544. lr 3.000000e-04, running loss 1.11483, it/sec: 4.482775541996022
epoch 1 iter 43340: train loss 1.11150. lr 3.000000e-04, running loss 1.11487, it/sec: 4.522117051008414
epoch 1 iter 43360: train loss 1.13048. lr 3.000000e-04, running loss 1.11488, it/sec: 4.4639380652199305
epoch 1 iter 43380: train loss 1.09417. lr 3.000000e-04, running loss 1.11478, it/sec: 4.529825269028609
epoch 1 iter 43400: train loss 1.10059. lr 3.000000e-04, running loss 1.11477, it/sec: 4.4640491795622514
epoch 1 iter 43420: train loss 1.11137. lr 3.000000e-04, running loss 1.11481, it/sec: 4.50454591760691
epoch 1 iter 43440: train loss 1.10260. lr 3.000000e-04, running loss 1.11475, it/sec: 4.480161747358542
epoch 1 iter 43460: train loss 1.09567. lr 3.000000e-04, running loss 1.11470, it/sec: 4.490951803993002
epoch 1 iter 43480: train loss 1.11066. lr 3.000000e-04, running loss 1.11462, it/sec: 4.458940698963198
epoch 1 iter 43500: train loss 1.12510. lr 3.000000e-04, running loss 1.11454, it/sec: 4.4741879105196
epoch 1 iter 43520: train loss 1.11547. lr 3.000000e-04, running loss 1.11446, it/sec: 4.456532859563863
epoch 1 iter 43540: train loss 1.10450. lr 3.000000e-04, running loss 1.11440, it/sec: 4.509549451321224
epoch 1 iter 43560: train loss 1.11047. lr 3.000000e-04, running loss 1.11455, it/sec: 4.451849076261271
epoch 1 iter 43580: train loss 1.12861. lr 3.000000e-04, running loss 1.11453, it/sec: 4.482859703422285
epoch 1 iter 43600: train loss 1.09450. lr 3.000000e-04, running loss 1.11449, it/sec: 4.459526105966093
epoch 1 iter 43620: train loss 1.13604. lr 3.000000e-04, running loss 1.11456, it/sec: 4.4648976041096455
epoch 1 iter 43640: train loss 1.10577. lr 3.000000e-04, running loss 1.11448, it/sec: 4.500277891656311
epoch 1 iter 43660: train loss 1.09861. lr 3.000000e-04, running loss 1.11443, it/sec: 4.477987997627233
epoch 1 iter 43680: train loss 1.10929. lr 3.000000e-04, running loss 1.11442, it/sec: 4.498720021136834
epoch 1 iter 43700: train loss 1.12100. lr 3.000000e-04, running loss 1.11450, it/sec: 4.475876716357101
epoch 1 iter 43720: train loss 1.10297. lr 3.000000e-04, running loss 1.11440, it/sec: 4.472473355545397
epoch 1 iter 43740: train loss 1.11775. lr 3.000000e-04, running loss 1.11440, it/sec: 4.492343811026957
epoch 1 iter 43760: train loss 1.11074. lr 3.000000e-04, running loss 1.11433, it/sec: 4.488675976470827
epoch 1 iter 43780: train loss 1.11273. lr 3.000000e-04, running loss 1.11433, it/sec: 4.511192041777347
epoch 1 iter 43800: train loss 1.11170. lr 3.000000e-04, running loss 1.11431, it/sec: 4.481533535982571
epoch 1 iter 43820: train loss 1.09675. lr 3.000000e-04, running loss 1.11429, it/sec: 4.505217119961289
epoch 1 iter 43840: train loss 1.08836. lr 3.000000e-04, running loss 1.11420, it/sec: 4.4960557674905
epoch 1 iter 43860: train loss 1.09725. lr 3.000000e-04, running loss 1.11431, it/sec: 4.472655469819709
epoch 1 iter 43880: train loss 1.10750. lr 3.000000e-04, running loss 1.11429, it/sec: 4.514994076657166
epoch 1 iter 43900: train loss 1.11206. lr 3.000000e-04, running loss 1.11434, it/sec: 4.473793644707796
epoch 1 iter 43920: train loss 1.10782. lr 3.000000e-04, running loss 1.11426, it/sec: 4.486313090583826
epoch 1 iter 43940: train loss 1.13197. lr 3.000000e-04, running loss 1.11434, it/sec: 4.444451733377793
epoch 1 iter 43960: train loss 1.10866. lr 3.000000e-04, running loss 1.11430, it/sec: 4.477428847002975
epoch 1 iter 43980: train loss 1.12448. lr 3.000000e-04, running loss 1.11436, it/sec: 4.440158014321157
epoch 1 iter 44000: train loss 1.14281. lr 3.000000e-04, running loss 1.11449, it/sec: 4.5139258606703665
epoch 1 iter 44020: train loss 1.11540. lr 3.000000e-04, running loss 1.11452, it/sec: 4.461176035586196
epoch 1 iter 44040: train loss 1.10890. lr 3.000000e-04, running loss 1.11457, it/sec: 4.515984299226704
epoch 1 iter 44060: train loss 1.12800. lr 3.000000e-04, running loss 1.11452, it/sec: 4.469839050282538
epoch 1 iter 44080: train loss 1.12583. lr 3.000000e-04, running loss 1.11436, it/sec: 4.508883544143847
epoch 1 iter 44100: train loss 1.10558. lr 3.000000e-04, running loss 1.11434, it/sec: 4.437494402140842
epoch 1 iter 44120: train loss 1.13122. lr 3.000000e-04, running loss 1.11435, it/sec: 4.498894646818692
epoch 1 iter 44140: train loss 1.11056. lr 3.000000e-04, running loss 1.11432, it/sec: 4.428432570339524
epoch 1 iter 44160: train loss 1.11727. lr 3.000000e-04, running loss 1.11439, it/sec: 4.51392848887393
epoch 1 iter 44180: train loss 1.11009. lr 3.000000e-04, running loss 1.11449, it/sec: 4.471656121574451
epoch 1 iter 44200: train loss 1.11117. lr 3.000000e-04, running loss 1.11441, it/sec: 4.497815320503729
epoch 1 iter 44220: train loss 1.11874. lr 3.000000e-04, running loss 1.11448, it/sec: 4.505901719480768
epoch 1 iter 44240: train loss 1.11198. lr 3.000000e-04, running loss 1.11447, it/sec: 4.463192411258564
epoch 1 iter 44260: train loss 1.12093. lr 3.000000e-04, running loss 1.11444, it/sec: 4.4802400692594055
epoch 1 iter 44280: train loss 1.11661. lr 3.000000e-04, running loss 1.11449, it/sec: 4.444774760128569
epoch 1 iter 44300: train loss 1.12564. lr 3.000000e-04, running loss 1.11446, it/sec: 4.503995309817121
epoch 1 iter 44320: train loss 1.11870. lr 3.000000e-04, running loss 1.11443, it/sec: 4.530598959015285
epoch 1 iter 44340: train loss 1.11446. lr 3.000000e-04, running loss 1.11438, it/sec: 4.438652791982773
epoch 1 iter 44360: train loss 1.13098. lr 3.000000e-04, running loss 1.11456, it/sec: 4.514882734938194
epoch 1 iter 44380: train loss 1.10912. lr 3.000000e-04, running loss 1.11461, it/sec: 4.457127725920329
epoch 1 iter 44400: train loss 1.13366. lr 3.000000e-04, running loss 1.11454, it/sec: 4.498949072192462
epoch 1 iter 44420: train loss 1.10185. lr 3.000000e-04, running loss 1.11446, it/sec: 4.5040720323635295
epoch 1 iter 44440: train loss 1.10901. lr 3.000000e-04, running loss 1.11453, it/sec: 4.4570327497614715
epoch 1 iter 44460: train loss 1.11416. lr 3.000000e-04, running loss 1.11449, it/sec: 4.523758149008705
epoch 1 iter 44480: train loss 1.11024. lr 3.000000e-04, running loss 1.11457, it/sec: 4.464996147576928
epoch 1 iter 44500: train loss 1.11253. lr 3.000000e-04, running loss 1.11484, it/sec: 4.48314715460376
epoch 1 iter 44520: train loss 1.13035. lr 3.000000e-04, running loss 1.11481, it/sec: 4.454276280000418
epoch 1 iter 44540: train loss 1.10616. lr 3.000000e-04, running loss 1.11482, it/sec: 4.510837496556249
epoch 1 iter 44560: train loss 1.12383. lr 3.000000e-04, running loss 1.11478, it/sec: 4.430877766460393
epoch 1 iter 44580: train loss 1.09891. lr 3.000000e-04, running loss 1.11477, it/sec: 4.5217062565080015
epoch 1 iter 44600: train loss 1.12887. lr 3.000000e-04, running loss 1.11479, it/sec: 4.4574240274535155
epoch 1 iter 44620: train loss 1.10994. lr 3.000000e-04, running loss 1.11483, it/sec: 4.495591893353201
epoch 1 iter 44640: train loss 1.11867. lr 3.000000e-04, running loss 1.11485, it/sec: 4.457408628623766
epoch 1 iter 44660: train loss 1.10190. lr 3.000000e-04, running loss 1.11514, it/sec: 4.505353073523621
epoch 1 iter 44680: train loss 1.10338. lr 3.000000e-04, running loss 1.11512, it/sec: 4.474133042064881
epoch 1 iter 44700: train loss 1.12866. lr 3.000000e-04, running loss 1.11510, it/sec: 4.5232720277820855
epoch 1 iter 44720: train loss 1.10565. lr 3.000000e-04, running loss 1.11514, it/sec: 4.456847752367377
epoch 1 iter 44740: train loss 1.08527. lr 3.000000e-04, running loss 1.11505, it/sec: 4.521896758497161
epoch 1 iter 44760: train loss 1.10506. lr 3.000000e-04, running loss 1.11499, it/sec: 4.458539274594823
epoch 1 iter 44780: train loss 1.10311. lr 3.000000e-04, running loss 1.11503, it/sec: 4.519068894056052
epoch 1 iter 44800: train loss 1.10059. lr 3.000000e-04, running loss 1.11501, it/sec: 4.452584184400331
epoch 1 iter 44820: train loss 1.10592. lr 3.000000e-04, running loss 1.11495, it/sec: 4.513055810073614
epoch 1 iter 44840: train loss 1.10911. lr 3.000000e-04, running loss 1.11489, it/sec: 4.49045570980082
epoch 1 iter 44860: train loss 1.12490. lr 3.000000e-04, running loss 1.11485, it/sec: 4.511281262221426
epoch 1 iter 44880: train loss 1.10898. lr 3.000000e-04, running loss 1.11482, it/sec: 4.436379147197572
epoch 1 iter 44900: train loss 1.10332. lr 3.000000e-04, running loss 1.11474, it/sec: 4.526842124514241
epoch 1 iter 44920: train loss 1.11496. lr 3.000000e-04, running loss 1.11476, it/sec: 4.458805464052735
epoch 1 iter 44940: train loss 1.10665. lr 3.000000e-04, running loss 1.11469, it/sec: 4.47959226092416
epoch 1 iter 44960: train loss 1.12029. lr 3.000000e-04, running loss 1.11468, it/sec: 4.476256863935226
epoch 1 iter 44980: train loss 1.11738. lr 3.000000e-04, running loss 1.11468, it/sec: 4.529013834605519
epoch 1 iter 45000: train loss 1.08213. lr 3.000000e-04, running loss 1.11462, it/sec: 4.49584154454036
epoch 1 iter 45020: train loss 1.13915. lr 3.000000e-04, running loss 1.11468, it/sec: 4.493478581878141
epoch 1 iter 45040: train loss 1.11858. lr 3.000000e-04, running loss 1.11459, it/sec: 4.501790634400201
epoch 1 iter 45060: train loss 1.10799. lr 3.000000e-04, running loss 1.11460, it/sec: 4.470834389192151
epoch 1 iter 45080: train loss 1.10944. lr 3.000000e-04, running loss 1.11466, it/sec: 4.502814285599621
epoch 1 iter 45100: train loss 1.11585. lr 3.000000e-04, running loss 1.11469, it/sec: 4.439141642834431
epoch 1 iter 45120: train loss 1.08599. lr 3.000000e-04, running loss 1.11459, it/sec: 4.514322158589155
epoch 1 iter 45140: train loss 1.10184. lr 3.000000e-04, running loss 1.11461, it/sec: 4.4831969797297635
epoch 1 iter 45160: train loss 1.10631. lr 3.000000e-04, running loss 1.11454, it/sec: 4.4968489637889615
epoch 1 iter 45180: train loss 1.13461. lr 3.000000e-04, running loss 1.11447, it/sec: 4.494179834794073
epoch 1 iter 45200: train loss 1.12968. lr 3.000000e-04, running loss 1.11441, it/sec: 4.517302878049139
epoch 1 iter 45220: train loss 1.12933. lr 3.000000e-04, running loss 1.11449, it/sec: 4.437828628168145
epoch 1 iter 45240: train loss 1.11282. lr 3.000000e-04, running loss 1.11443, it/sec: 4.526312952609499
epoch 1 iter 45260: train loss 1.11250. lr 3.000000e-04, running loss 1.11444, it/sec: 4.479080115958886
epoch 1 iter 45280: train loss 1.12692. lr 3.000000e-04, running loss 1.11446, it/sec: 4.482953874388152
epoch 1 iter 45300: train loss 1.09968. lr 3.000000e-04, running loss 1.11438, it/sec: 4.486109314118624
epoch 1 iter 45320: train loss 1.09644. lr 3.000000e-04, running loss 1.11430, it/sec: 4.485381685930892
epoch 1 iter 45340: train loss 1.11675. lr 3.000000e-04, running loss 1.11415, it/sec: 4.5003412428067735
epoch 1 iter 45360: train loss 1.11658. lr 3.000000e-04, running loss 1.11404, it/sec: 4.447746747402242
epoch 1 iter 45380: train loss 1.11639. lr 3.000000e-04, running loss 1.11409, it/sec: 4.4791615496375465
epoch 1 iter 45400: train loss 1.11018. lr 3.000000e-04, running loss 1.11411, it/sec: 4.46867361241103
epoch 1 iter 45420: train loss 1.10611. lr 3.000000e-04, running loss 1.11405, it/sec: 4.5179671719601995
epoch 1 iter 45440: train loss 1.13508. lr 3.000000e-04, running loss 1.11403, it/sec: 4.461097503041511
epoch 1 iter 45460: train loss 1.10266. lr 3.000000e-04, running loss 1.11396, it/sec: 4.522276276204157
epoch 1 iter 45480: train loss 1.09458. lr 3.000000e-04, running loss 1.11402, it/sec: 4.419173098786506
epoch 1 iter 45500: train loss 1.15470. lr 3.000000e-04, running loss 1.11456, it/sec: 4.509642713270434
epoch 1 iter 45520: train loss 1.11547. lr 3.000000e-04, running loss 1.11457, it/sec: 4.456135163890178
epoch 1 iter 45540: train loss 1.11251. lr 3.000000e-04, running loss 1.11476, it/sec: 4.505726225635756
epoch 1 iter 45560: train loss 1.10497. lr 3.000000e-04, running loss 1.11468, it/sec: 4.4703718460977475
epoch 1 iter 45580: train loss 1.09884. lr 3.000000e-04, running loss 1.11459, it/sec: 4.503753087816815
epoch 1 iter 45600: train loss 1.11869. lr 3.000000e-04, running loss 1.11458, it/sec: 4.447812446646475
epoch 1 iter 45620: train loss 1.10062. lr 3.000000e-04, running loss 1.11469, it/sec: 4.510862077507763
epoch 1 iter 45640: train loss 1.12425. lr 3.000000e-04, running loss 1.11476, it/sec: 4.438830606027952
epoch 1 iter 45660: train loss 1.10419. lr 3.000000e-04, running loss 1.11473, it/sec: 4.504325873982669
epoch 1 iter 45680: train loss 1.11834. lr 3.000000e-04, running loss 1.11465, it/sec: 4.497369508335329
epoch 1 iter 45700: train loss 1.10042. lr 3.000000e-04, running loss 1.11461, it/sec: 4.463079746487752
epoch 1 iter 45720: train loss 1.11738. lr 3.000000e-04, running loss 1.11454, it/sec: 4.516199427469048
epoch 1 iter 45740: train loss 1.11575. lr 3.000000e-04, running loss 1.11478, it/sec: 4.468870875626887
epoch 1 iter 45760: train loss 1.12795. lr 3.000000e-04, running loss 1.11474, it/sec: 4.507227153981325
epoch 1 iter 45780: train loss 1.08873. lr 3.000000e-04, running loss 1.11478, it/sec: 4.466064223914015
epoch 1 iter 45800: train loss 1.10036. lr 3.000000e-04, running loss 1.11484, it/sec: 4.493955752601117
epoch 1 iter 45820: train loss 1.10307. lr 3.000000e-04, running loss 1.11476, it/sec: 4.459166055084443
epoch 1 iter 45840: train loss 1.11981. lr 3.000000e-04, running loss 1.11468, it/sec: 4.486379973964289
epoch 1 iter 45860: train loss 1.13491. lr 3.000000e-04, running loss 1.11467, it/sec: 4.466341267102102
epoch 1 iter 45880: train loss 1.12160. lr 3.000000e-04, running loss 1.11458, it/sec: 4.472505980278711
epoch 1 iter 45900: train loss 1.10764. lr 3.000000e-04, running loss 1.11446, it/sec: 4.451386707971331
epoch 1 iter 45920: train loss 1.09839. lr 3.000000e-04, running loss 1.11443, it/sec: 4.51189647434274
epoch 1 iter 45940: train loss 1.12340. lr 3.000000e-04, running loss 1.11444, it/sec: 4.467269032799868
epoch 1 iter 45960: train loss 1.09870. lr 3.000000e-04, running loss 1.11442, it/sec: 4.469235192820756
epoch 1 iter 45980: train loss 1.09929. lr 3.000000e-04, running loss 1.11449, it/sec: 4.46242157785672
epoch 1 iter 46000: train loss 1.16476. lr 3.000000e-04, running loss 1.11452, it/sec: 4.4890543320366545
epoch 1 iter 46020: train loss 1.12444. lr 3.000000e-04, running loss 1.11467, it/sec: 4.482378836244699
epoch 1 iter 46040: train loss 1.13263. lr 3.000000e-04, running loss 1.11469, it/sec: 4.4456900078033454
epoch 1 iter 46060: train loss 1.12521. lr 3.000000e-04, running loss 1.11469, it/sec: 4.503202429479958
epoch 1 iter 46080: train loss 1.11551. lr 3.000000e-04, running loss 1.11461, it/sec: 4.467090229350752
epoch 1 iter 46100: train loss 1.10327. lr 3.000000e-04, running loss 1.11468, it/sec: 4.44855206004751
epoch 1 iter 46120: train loss 1.10081. lr 3.000000e-04, running loss 1.11457, it/sec: 4.487743368979349
epoch 1 iter 46140: train loss 1.10210. lr 3.000000e-04, running loss 1.11464, it/sec: 4.496127975503426
epoch 1 iter 46160: train loss 1.10715. lr 3.000000e-04, running loss 1.11460, it/sec: 4.4767862598009405
epoch 1 iter 46180: train loss 1.11754. lr 3.000000e-04, running loss 1.11457, it/sec: 4.507352115161176
epoch 1 iter 46200: train loss 1.11593. lr 3.000000e-04, running loss 1.11448, it/sec: 4.48829410009606
epoch 1 iter 46220: train loss 1.11168. lr 3.000000e-04, running loss 1.11439, it/sec: 4.471155365147557
epoch 1 iter 46240: train loss 1.10961. lr 3.000000e-04, running loss 1.11429, it/sec: 4.4720650906210615
epoch 1 iter 46260: train loss 1.11784. lr 3.000000e-04, running loss 1.11420, it/sec: 4.465908732721483
epoch 1 iter 46280: train loss 1.10067. lr 3.000000e-04, running loss 1.11426, it/sec: 4.4472509364507555
epoch 1 iter 46300: train loss 1.12265. lr 3.000000e-04, running loss 1.11436, it/sec: 4.512725429681596
epoch 1 iter 46320: train loss 1.13563. lr 3.000000e-04, running loss 1.11430, it/sec: 4.46424230746759
epoch 1 iter 46340: train loss 1.14210. lr 3.000000e-04, running loss 1.11436, it/sec: 4.497126138477583
epoch 1 iter 46360: train loss 1.12257. lr 3.000000e-04, running loss 1.11431, it/sec: 4.475629957216049
epoch 1 iter 46380: train loss 1.13670. lr 3.000000e-04, running loss 1.11431, it/sec: 4.488653733630003
epoch 1 iter 46400: train loss 1.10189. lr 3.000000e-04, running loss 1.11428, it/sec: 4.504414862012073
epoch 1 iter 46420: train loss 1.09861. lr 3.000000e-04, running loss 1.11429, it/sec: 4.498739775687256
epoch 1 iter 46440: train loss 1.12349. lr 3.000000e-04, running loss 1.11432, it/sec: 4.466510814826526
epoch 1 iter 46460: train loss 1.13900. lr 3.000000e-04, running loss 1.11422, it/sec: 4.489350237590009
epoch 1 iter 46480: train loss 1.12282. lr 3.000000e-04, running loss 1.11424, it/sec: 4.484070915752977
epoch 1 iter 46500: train loss 1.10399. lr 3.000000e-04, running loss 1.11415, it/sec: 4.522228585556954
epoch 1 iter 46520: train loss 1.10899. lr 3.000000e-04, running loss 1.11413, it/sec: 4.490685995389951
epoch 1 iter 46540: train loss 1.12074. lr 3.000000e-04, running loss 1.11407, it/sec: 4.463976126145459
epoch 1 iter 46560: train loss 1.09725. lr 3.000000e-04, running loss 1.11419, it/sec: 4.522804218854196
epoch 1 iter 46580: train loss 1.10543. lr 3.000000e-04, running loss 1.11421, it/sec: 4.4563927468090645
epoch 1 iter 46600: train loss 1.15145. lr 3.000000e-04, running loss 1.11422, it/sec: 4.493060658979189
epoch 1 iter 46620: train loss 1.11406. lr 3.000000e-04, running loss 1.11425, it/sec: 4.44611897153362
epoch 1 iter 46640: train loss 1.09516. lr 3.000000e-04, running loss 1.11417, it/sec: 4.5010048719226745
epoch 1 iter 46660: train loss 1.10998. lr 3.000000e-04, running loss 1.11422, it/sec: 4.464879763218502
epoch 1 iter 46680: train loss 1.09551. lr 3.000000e-04, running loss 1.11428, it/sec: 4.515805552547989
epoch 1 iter 46700: train loss 1.09571. lr 3.000000e-04, running loss 1.11411, it/sec: 4.437380036176763
epoch 1 iter 46720: train loss 1.20267. lr 3.000000e-04, running loss 1.11419, it/sec: 4.511161373323873
epoch 1 iter 46740: train loss 1.11646. lr 3.000000e-04, running loss 1.11422, it/sec: 4.473313399609264
epoch 1 iter 46760: train loss 1.11159. lr 3.000000e-04, running loss 1.11416, it/sec: 4.503563766742843
epoch 1 iter 46780: train loss 1.11823. lr 3.000000e-04, running loss 1.11413, it/sec: 4.48509893621804
epoch 1 iter 46800: train loss 1.09911. lr 3.000000e-04, running loss 1.11404, it/sec: 4.506218024156044
epoch 1 iter 46820: train loss 1.14079. lr 3.000000e-04, running loss 1.11407, it/sec: 4.464924875710307
epoch 1 iter 46840: train loss 1.11411. lr 3.000000e-04, running loss 1.11403, it/sec: 4.506195017328407
epoch 1 iter 46860: train loss 1.09992. lr 3.000000e-04, running loss 1.11396, it/sec: 4.4618814765929935
epoch 1 iter 46880: train loss 1.10699. lr 3.000000e-04, running loss 1.11421, it/sec: 4.496489168047987
epoch 1 iter 46900: train loss 1.12937. lr 3.000000e-04, running loss 1.11417, it/sec: 4.505935808727605
epoch 1 iter 46920: train loss 1.07829. lr 3.000000e-04, running loss 1.11407, it/sec: 4.47688865481144
epoch 1 iter 46940: train loss 1.10787. lr 3.000000e-04, running loss 1.11397, it/sec: 4.505076141979567
epoch 1 iter 46960: train loss 1.11949. lr 3.000000e-04, running loss 1.11390, it/sec: 4.4630115641588715
epoch 1 iter 46980: train loss 1.10086. lr 3.000000e-04, running loss 1.11384, it/sec: 4.508164870680666
epoch 1 iter 47000: train loss 1.12372. lr 3.000000e-04, running loss 1.11373, it/sec: 4.488905618600234
epoch 1 iter 47020: train loss 1.10472. lr 3.000000e-04, running loss 1.11452, it/sec: 4.444832428663077
epoch 1 iter 47040: train loss 1.11481. lr 3.000000e-04, running loss 1.11453, it/sec: 4.504255572973084
epoch 1 iter 47060: train loss 1.11095. lr 3.000000e-04, running loss 1.11448, it/sec: 4.47606740307669
epoch 1 iter 47080: train loss 1.12925. lr 3.000000e-04, running loss 1.11450, it/sec: 4.4946045908294225
epoch 1 iter 47100: train loss 1.10639. lr 3.000000e-04, running loss 1.11440, it/sec: 4.460258239848274
epoch 1 iter 47120: train loss 1.28555. lr 3.000000e-04, running loss 1.11457, it/sec: 4.48005197675926
epoch 1 iter 47140: train loss 1.11188. lr 3.000000e-04, running loss 1.11449, it/sec: 4.500495637097634
epoch 1 iter 47160: train loss 1.10094. lr 3.000000e-04, running loss 1.11442, it/sec: 4.44432008232715
epoch 1 iter 47180: train loss 1.08657. lr 3.000000e-04, running loss 1.11437, it/sec: 4.481267718910649
epoch 1 iter 47200: train loss 1.11498. lr 3.000000e-04, running loss 1.11435, it/sec: 4.475351760527164
epoch 1 iter 47220: train loss 1.14760. lr 3.000000e-04, running loss 1.11434, it/sec: 4.511077388435647
epoch 1 iter 47240: train loss 1.11387. lr 3.000000e-04, running loss 1.11426, it/sec: 4.452486505370214
epoch 1 iter 47260: train loss 1.11227. lr 3.000000e-04, running loss 1.11424, it/sec: 4.512704821926724
epoch 1 iter 47280: train loss 1.09348. lr 3.000000e-04, running loss 1.11430, it/sec: 4.463318509464993
epoch 1 iter 47300: train loss 1.10496. lr 3.000000e-04, running loss 1.11448, it/sec: 4.519617820574681
epoch 1 iter 47320: train loss 1.11181. lr 3.000000e-04, running loss 1.11433, it/sec: 4.443989953079314
epoch 1 iter 47340: train loss 1.10451. lr 3.000000e-04, running loss 1.11428, it/sec: 4.496392222218549
epoch 1 iter 47360: train loss 1.11156. lr 3.000000e-04, running loss 1.11436, it/sec: 4.469784626850791
epoch 1 iter 47380: train loss 1.09844. lr 3.000000e-04, running loss 1.11428, it/sec: 4.50029247408693
epoch 1 iter 47400: train loss 1.11090. lr 3.000000e-04, running loss 1.11414, it/sec: 4.478787588494758
epoch 1 iter 47420: train loss 1.10100. lr 3.000000e-04, running loss 1.11413, it/sec: 4.511722183877705
epoch 1 iter 47440: train loss 1.12649. lr 3.000000e-04, running loss 1.11415, it/sec: 4.446816731270484
epoch 1 iter 47460: train loss 1.12674. lr 3.000000e-04, running loss 1.11415, it/sec: 4.520202127834104
epoch 1 iter 47480: train loss 1.14352. lr 3.000000e-04, running loss 1.11408, it/sec: 4.468952756896462
epoch 1 iter 47500: train loss 1.09007. lr 3.000000e-04, running loss 1.11433, it/sec: 4.493909202840528
epoch 1 iter 47520: train loss 1.09637. lr 3.000000e-04, running loss 1.11438, it/sec: 4.46780934007528
epoch 1 iter 47540: train loss 1.12619. lr 3.000000e-04, running loss 1.11441, it/sec: 4.483638096625745
epoch 1 iter 47560: train loss 1.11599. lr 3.000000e-04, running loss 1.11444, it/sec: 4.484902409616984
epoch 1 iter 47580: train loss 1.15201. lr 3.000000e-04, running loss 1.11459, it/sec: 4.496065106499817
epoch 1 iter 47600: train loss 1.10006. lr 3.000000e-04, running loss 1.11458, it/sec: 4.5019960008061455
epoch 1 iter 47620: train loss 1.09352. lr 3.000000e-04, running loss 1.11448, it/sec: 4.512439141582521
epoch 1 iter 47640: train loss 1.11480. lr 3.000000e-04, running loss 1.11446, it/sec: 4.411514176960565
epoch 1 iter 47660: train loss 1.11209. lr 3.000000e-04, running loss 1.11454, it/sec: 4.521062549611128
epoch 1 iter 47680: train loss 1.11289. lr 3.000000e-04, running loss 1.11453, it/sec: 4.4430995484877
epoch 1 iter 47700: train loss 1.11099. lr 3.000000e-04, running loss 1.11442, it/sec: 4.525420078853791
epoch 1 iter 47720: train loss 1.10708. lr 3.000000e-04, running loss 1.11442, it/sec: 4.481938087517114
epoch 1 iter 47740: train loss 1.10556. lr 3.000000e-04, running loss 1.11446, it/sec: 4.475888656102898
epoch 1 iter 47760: train loss 1.12433. lr 3.000000e-04, running loss 1.11445, it/sec: 4.490513339724923
epoch 1 iter 47780: train loss 1.13048. lr 3.000000e-04, running loss 1.11442, it/sec: 4.4507540110224015
epoch 1 iter 47800: train loss 1.10053. lr 3.000000e-04, running loss 1.11433, it/sec: 4.5027531767131475
epoch 1 iter 47820: train loss 1.13681. lr 3.000000e-04, running loss 1.11435, it/sec: 4.45459198591574
epoch 1 iter 47840: train loss 1.10417. lr 3.000000e-04, running loss 1.11437, it/sec: 4.518377654524018
epoch 1 iter 47860: train loss 1.11752. lr 3.000000e-04, running loss 1.11435, it/sec: 4.483738834695402
epoch 1 iter 47880: train loss 1.09806. lr 3.000000e-04, running loss 1.11426, it/sec: 4.499575118753165
epoch 1 iter 47900: train loss 1.13597. lr 3.000000e-04, running loss 1.11433, it/sec: 4.435981773516198
epoch 1 iter 47920: train loss 1.14526. lr 3.000000e-04, running loss 1.11446, it/sec: 4.507083165073407
epoch 1 iter 47940: train loss 1.12564. lr 3.000000e-04, running loss 1.11443, it/sec: 4.473921482116578
epoch 1 iter 47960: train loss 1.10227. lr 3.000000e-04, running loss 1.11438, it/sec: 4.5058234928432555
epoch 1 iter 47980: train loss 1.09246. lr 3.000000e-04, running loss 1.11437, it/sec: 4.453570959114406
epoch 1 iter 48000: train loss 1.14982. lr 3.000000e-04, running loss 1.11441, it/sec: 4.516185048066245
epoch 1 iter 48020: train loss 1.09028. lr 3.000000e-04, running loss 1.11434, it/sec: 4.473253269356985
epoch 1 iter 48040: train loss 1.09575. lr 3.000000e-04, running loss 1.11427, it/sec: 4.481828572432724
epoch 1 iter 48060: train loss 1.11417. lr 3.000000e-04, running loss 1.11431, it/sec: 4.479791090908464
epoch 1 iter 48080: train loss 1.09624. lr 3.000000e-04, running loss 1.11433, it/sec: 4.500944825070767
epoch 1 iter 48100: train loss 1.11706. lr 3.000000e-04, running loss 1.11436, it/sec: 4.510001342381407
epoch 1 iter 48120: train loss 1.09229. lr 3.000000e-04, running loss 1.11442, it/sec: 4.451094023532158
epoch 1 iter 48140: train loss 1.17713. lr 3.000000e-04, running loss 1.11449, it/sec: 4.483995215995862
epoch 1 iter 48160: train loss 1.11382. lr 3.000000e-04, running loss 1.11436, it/sec: 4.493887331286728
epoch 1 iter 48180: train loss 1.13932. lr 3.000000e-04, running loss 1.11431, it/sec: 4.510746951804289
epoch 1 iter 48200: train loss 1.12617. lr 3.000000e-04, running loss 1.11445, it/sec: 4.442626542300094
epoch 1 iter 48220: train loss 1.11595. lr 3.000000e-04, running loss 1.11435, it/sec: 4.502765929427241
epoch 1 iter 48240: train loss 1.11304. lr 3.000000e-04, running loss 1.11503, it/sec: 4.4956961598370135
epoch 1 iter 48260: train loss 1.13414. lr 3.000000e-04, running loss 1.11514, it/sec: 4.453978611593851
epoch 1 iter 48280: train loss 1.11825. lr 3.000000e-04, running loss 1.11502, it/sec: 4.462450272749609
epoch 1 iter 48300: train loss 1.09292. lr 3.000000e-04, running loss 1.11499, it/sec: 4.504134699196766
epoch 1 iter 48320: train loss 1.12042. lr 3.000000e-04, running loss 1.11495, it/sec: 4.464356485714679
epoch 1 iter 48340: train loss 1.09887. lr 3.000000e-04, running loss 1.11477, it/sec: 4.502381733389393
epoch 1 iter 48360: train loss 1.10251. lr 3.000000e-04, running loss 1.11472, it/sec: 4.435157068084608
epoch 1 iter 48380: train loss 1.10497. lr 3.000000e-04, running loss 1.11469, it/sec: 4.52180560498507
epoch 1 iter 48400: train loss 1.10445. lr 3.000000e-04, running loss 1.11467, it/sec: 4.449792916845786
epoch 1 iter 48420: train loss 1.09695. lr 3.000000e-04, running loss 1.11463, it/sec: 4.514142748757262
epoch 1 iter 48440: train loss 1.10473. lr 3.000000e-04, running loss 1.11460, it/sec: 4.488105935743516
epoch 1 iter 48460: train loss 1.10669. lr 3.000000e-04, running loss 1.11451, it/sec: 4.4735116737530145
epoch 1 iter 48480: train loss 1.10564. lr 3.000000e-04, running loss 1.11444, it/sec: 4.508032304398228
epoch 1 iter 48500: train loss 1.09486. lr 3.000000e-04, running loss 1.11445, it/sec: 4.468515023336422
epoch 1 iter 48520: train loss 1.10297. lr 3.000000e-04, running loss 1.11444, it/sec: 4.480164778022612
epoch 1 iter 48540: train loss 1.10407. lr 3.000000e-04, running loss 1.11439, it/sec: 4.510476171849049
epoch 1 iter 48560: train loss 1.10467. lr 3.000000e-04, running loss 1.11435, it/sec: 4.462059147088914
epoch 1 iter 48580: train loss 1.10189. lr 3.000000e-04, running loss 1.11425, it/sec: 4.51674041454395
epoch 1 iter 48600: train loss 1.11422. lr 3.000000e-04, running loss 1.11422, it/sec: 4.430594287943417
epoch 1 iter 48620: train loss 1.10994. lr 3.000000e-04, running loss 1.11434, it/sec: 4.489294652906757
epoch 1 iter 48640: train loss 1.10864. lr 3.000000e-04, running loss 1.11432, it/sec: 4.517201341171918
epoch 1 iter 48660: train loss 1.11490. lr 3.000000e-04, running loss 1.11432, it/sec: 4.470369507996701
epoch 1 iter 48680: train loss 1.10709. lr 3.000000e-04, running loss 1.11462, it/sec: 4.499094586066928
epoch 1 iter 48700: train loss 1.09129. lr 3.000000e-04, running loss 1.11454, it/sec: 4.465955981236922
epoch 1 iter 48720: train loss 1.10254. lr 3.000000e-04, running loss 1.11459, it/sec: 4.517346750575891
epoch 1 iter 48740: train loss 1.23275. lr 3.000000e-04, running loss 1.11473, it/sec: 4.443550974553791
epoch 1 iter 48760: train loss 1.11774. lr 3.000000e-04, running loss 1.11460, it/sec: 4.504762148548832
epoch 1 iter 48780: train loss 1.10968. lr 3.000000e-04, running loss 1.11462, it/sec: 4.455214598760234
epoch 1 iter 48800: train loss 1.12652. lr 3.000000e-04, running loss 1.11453, it/sec: 4.491740757551217
epoch 1 iter 48820: train loss 1.11214. lr 3.000000e-04, running loss 1.11445, it/sec: 4.448678559184504
epoch 1 iter 48840: train loss 1.12537. lr 3.000000e-04, running loss 1.11442, it/sec: 4.511646400702683
epoch 1 iter 48860: train loss 1.12157. lr 3.000000e-04, running loss 1.11452, it/sec: 4.448453549971239
epoch 1 iter 48880: train loss 1.11217. lr 3.000000e-04, running loss 1.11459, it/sec: 4.513687662781109
epoch 1 iter 48900: train loss 1.11402. lr 3.000000e-04, running loss 1.11449, it/sec: 4.438042676601718
epoch 1 iter 48920: train loss 1.10333. lr 3.000000e-04, running loss 1.11442, it/sec: 4.532813497176679
epoch 1 iter 48940: train loss 1.10744. lr 3.000000e-04, running loss 1.11446, it/sec: 4.4648020969535684
epoch 1 iter 48960: train loss 1.08442. lr 3.000000e-04, running loss 1.11436, it/sec: 4.500223676964976
epoch 1 iter 48980: train loss 1.10883. lr 3.000000e-04, running loss 1.11434, it/sec: 4.488410217909074
epoch 1 iter 49000: train loss 1.13955. lr 3.000000e-04, running loss 1.11423, it/sec: 4.4680630824591425
epoch 1 iter 49020: train loss 1.09941. lr 3.000000e-04, running loss 1.11418, it/sec: 4.5259685236930425
epoch 1 iter 49040: train loss 1.30564. lr 3.000000e-04, running loss 1.11436, it/sec: 4.457202463402791
epoch 1 iter 49060: train loss 1.13636. lr 3.000000e-04, running loss 1.11450, it/sec: 4.511373863689417
epoch 1 iter 49080: train loss 1.11954. lr 3.000000e-04, running loss 1.11446, it/sec: 4.441939585589447
epoch 1 iter 49100: train loss 1.14198. lr 3.000000e-04, running loss 1.11448, it/sec: 4.497671427162183
epoch 1 iter 49120: train loss 1.10161. lr 3.000000e-04, running loss 1.11452, it/sec: 4.452491362332941
epoch 1 iter 49140: train loss 1.08804. lr 3.000000e-04, running loss 1.11442, it/sec: 4.49288549773096
epoch 1 iter 49160: train loss 1.14050. lr 3.000000e-04, running loss 1.11445, it/sec: 4.4815496642678765
epoch 1 iter 49180: train loss 1.13813. lr 3.000000e-04, running loss 1.11441, it/sec: 4.4964612670669295
epoch 1 iter 49200: train loss 1.09476. lr 3.000000e-04, running loss 1.11433, it/sec: 4.482539655354496
epoch 1 iter 49220: train loss 1.09620. lr 3.000000e-04, running loss 1.11435, it/sec: 4.48710991960981
epoch 1 iter 49240: train loss 1.10873. lr 3.000000e-04, running loss 1.11430, it/sec: 4.477001377334684
epoch 1 iter 49260: train loss 1.10154. lr 3.000000e-04, running loss 1.11419, it/sec: 4.494240125701641
epoch 1 iter 49280: train loss 1.08577. lr 3.000000e-04, running loss 1.11424, it/sec: 4.458245964692647
epoch 1 iter 49300: train loss 1.10783. lr 3.000000e-04, running loss 1.11424, it/sec: 4.4897677520949975
epoch 1 iter 49320: train loss 1.11603. lr 3.000000e-04, running loss 1.11423, it/sec: 4.4950815185276625
epoch 1 iter 49340: train loss 1.14236. lr 3.000000e-04, running loss 1.11413, it/sec: 4.473366668501074
epoch 1 iter 49360: train loss 1.13341. lr 3.000000e-04, running loss 1.11421, it/sec: 4.522979061760238
epoch 1 iter 49380: train loss 1.10784. lr 3.000000e-04, running loss 1.11415, it/sec: 4.455470347799664
epoch 1 iter 49400: train loss 1.11176. lr 3.000000e-04, running loss 1.11417, it/sec: 4.499211162070801
epoch 1 iter 49420: train loss 1.13283. lr 3.000000e-04, running loss 1.11416, it/sec: 4.4555360154262145
epoch 1 iter 49440: train loss 1.12842. lr 3.000000e-04, running loss 1.11426, it/sec: 4.496929608803139
epoch 1 iter 49460: train loss 1.11666. lr 3.000000e-04, running loss 1.11432, it/sec: 4.470009021850438
epoch 1 iter 49480: train loss 1.13698. lr 3.000000e-04, running loss 1.11438, it/sec: 4.412227827573746
epoch 1 iter 49500: train loss 1.10924. lr 3.000000e-04, running loss 1.11425, it/sec: 4.458966704658702
epoch 1 iter 49520: train loss 1.11474. lr 3.000000e-04, running loss 1.11430, it/sec: 4.502452643792525
epoch 1 iter 49540: train loss 1.12186. lr 3.000000e-04, running loss 1.11437, it/sec: 4.483402683154104
epoch 1 iter 49560: train loss 1.11361. lr 3.000000e-04, running loss 1.11440, it/sec: 4.541167642003506
epoch 1 iter 49580: train loss 1.09702. lr 3.000000e-04, running loss 1.11436, it/sec: 4.454908132820832
epoch 1 iter 49600: train loss 1.11478. lr 3.000000e-04, running loss 1.11430, it/sec: 4.505496931820688
epoch 1 iter 49620: train loss 1.09849. lr 3.000000e-04, running loss 1.11441, it/sec: 4.48283267433456
epoch 1 iter 49640: train loss 1.10567. lr 3.000000e-04, running loss 1.11436, it/sec: 4.456482552563897
epoch 1 iter 49660: train loss 1.11023. lr 3.000000e-04, running loss 1.11426, it/sec: 4.499532055055536
epoch 1 iter 49680: train loss 1.09668. lr 3.000000e-04, running loss 1.11420, it/sec: 4.471786816617353
epoch 1 iter 49700: train loss 1.09749. lr 3.000000e-04, running loss 1.11420, it/sec: 4.50421674178509
epoch 1 iter 49720: train loss 1.10605. lr 3.000000e-04, running loss 1.11415, it/sec: 4.471436200037164
epoch 1 iter 49740: train loss 1.13930. lr 3.000000e-04, running loss 1.11424, it/sec: 4.461714871064866
epoch 1 iter 49760: train loss 1.12021. lr 3.000000e-04, running loss 1.11424, it/sec: 4.5028831017617685
epoch 1 iter 49780: train loss 1.10641. lr 3.000000e-04, running loss 1.11428, it/sec: 4.4691909500230524
epoch 1 iter 49800: train loss 1.11316. lr 3.000000e-04, running loss 1.11426, it/sec: 4.474222823166489
epoch 1 iter 49820: train loss 1.10361. lr 3.000000e-04, running loss 1.11422, it/sec: 4.5190933805475675
epoch 1 iter 49840: train loss 1.10070. lr 3.000000e-04, running loss 1.11413, it/sec: 4.469917091921482
epoch 1 iter 49860: train loss 1.11092. lr 3.000000e-04, running loss 1.11405, it/sec: 4.485683325468284
epoch 1 iter 49880: train loss 1.11421. lr 3.000000e-04, running loss 1.11412, it/sec: 4.447112968850068
epoch 1 iter 49900: train loss 1.11981. lr 3.000000e-04, running loss 1.11420, it/sec: 4.488494126662217
epoch 1 iter 49920: train loss 1.11495. lr 3.000000e-04, running loss 1.11444, it/sec: 4.4424297925371095
epoch 1 iter 49940: train loss 1.10788. lr 3.000000e-04, running loss 1.11440, it/sec: 4.50562263014296
epoch 1 iter 49960: train loss 1.15023. lr 3.000000e-04, running loss 1.11432, it/sec: 4.4739829119609045
epoch 1 iter 49980: train loss 1.12482. lr 3.000000e-04, running loss 1.11434, it/sec: 4.488888853226018
epoch 1 iter 50000: train loss 1.11799. lr 3.000000e-04, running loss 1.11433, it/sec: 1.1237141685064922
epoch 1 iter 50020: train loss 1.14743. lr 3.000000e-04, running loss 1.11430, it/sec: 4.470294628405221
epoch 1 iter 50040: train loss 1.12169. lr 3.000000e-04, running loss 1.11446, it/sec: 4.455265670888034
epoch 1 iter 50060: train loss 1.10664. lr 3.000000e-04, running loss 1.11449, it/sec: 4.528544036342468
epoch 1 iter 50080: train loss 1.10641. lr 3.000000e-04, running loss 1.11448, it/sec: 4.454651020654627
epoch 1 iter 50100: train loss 1.11759. lr 3.000000e-04, running loss 1.11453, it/sec: 4.506702739779268
epoch 1 iter 50120: train loss 1.12244. lr 3.000000e-04, running loss 1.11444, it/sec: 4.469151942121197
epoch 1 iter 50140: train loss 1.09033. lr 3.000000e-04, running loss 1.11438, it/sec: 4.4841218686141175
epoch 1 iter 50160: train loss 1.11741. lr 3.000000e-04, running loss 1.11442, it/sec: 4.496494020274359
epoch 1 iter 50180: train loss 1.11570. lr 3.000000e-04, running loss 1.11442, it/sec: 4.500955764537146
epoch 1 iter 50200: train loss 1.10106. lr 3.000000e-04, running loss 1.11443, it/sec: 4.4970922836351335
epoch 1 iter 50220: train loss 1.11476. lr 3.000000e-04, running loss 1.11449, it/sec: 4.46277452773344
epoch 1 iter 50240: train loss 1.11309. lr 3.000000e-04, running loss 1.11455, it/sec: 4.499801785365113
epoch 1 iter 50260: train loss 1.11529. lr 3.000000e-04, running loss 1.11461, it/sec: 4.456025913321178
epoch 1 iter 50280: train loss 1.11653. lr 3.000000e-04, running loss 1.11450, it/sec: 4.4993092204833545
epoch 1 iter 50300: train loss 1.10456. lr 3.000000e-04, running loss 1.11455, it/sec: 4.466839590368024
epoch 1 iter 50320: train loss 1.11171. lr 3.000000e-04, running loss 1.11444, it/sec: 4.510801115960529
epoch 1 iter 50340: train loss 1.11337. lr 3.000000e-04, running loss 1.11444, it/sec: 4.487367027665207
epoch 1 iter 50360: train loss 1.11327. lr 3.000000e-04, running loss 1.11454, it/sec: 4.468561429726066
epoch 1 iter 50380: train loss 1.11010. lr 3.000000e-04, running loss 1.11477, it/sec: 4.46211543388014
epoch 1 iter 50400: train loss 1.11629. lr 3.000000e-04, running loss 1.11495, it/sec: 4.504543401891297
epoch 1 iter 50420: train loss 1.13083. lr 3.000000e-04, running loss 1.11497, it/sec: 4.4695820490827245
epoch 1 iter 50440: train loss 1.09960. lr 3.000000e-04, running loss 1.11489, it/sec: 4.5139167331424135
epoch 1 iter 50460: train loss 1.11437. lr 3.000000e-04, running loss 1.11483, it/sec: 4.4561468598426295
epoch 1 iter 50480: train loss 1.10500. lr 3.000000e-04, running loss 1.11478, it/sec: 4.512623363572276
epoch 1 iter 50500: train loss 1.12517. lr 3.000000e-04, running loss 1.11478, it/sec: 4.463948128440471
epoch 1 iter 50520: train loss 1.11837. lr 3.000000e-04, running loss 1.11485, it/sec: 4.474068544891141
epoch 1 iter 50540: train loss 1.10721. lr 3.000000e-04, running loss 1.11477, it/sec: 4.4881999461819975
epoch 1 iter 50560: train loss 1.10562. lr 3.000000e-04, running loss 1.11480, it/sec: 4.461243026523199
epoch 1 iter 50580: train loss 1.10265. lr 3.000000e-04, running loss 1.11478, it/sec: 4.5111900884130485
epoch 1 iter 50600: train loss 1.10845. lr 3.000000e-04, running loss 1.11477, it/sec: 4.477651162652271
epoch 1 iter 50620: train loss 1.12554. lr 3.000000e-04, running loss 1.11479, it/sec: 4.503912321244421
epoch 1 iter 50640: train loss 1.11481. lr 3.000000e-04, running loss 1.11476, it/sec: 4.518944568491747
epoch 1 iter 50660: train loss 1.09587. lr 3.000000e-04, running loss 1.11472, it/sec: 4.473945841804452
epoch 1 iter 50680: train loss 1.13858. lr 3.000000e-04, running loss 1.11480, it/sec: 4.482218530349971
epoch 1 iter 50700: train loss 1.10561. lr 3.000000e-04, running loss 1.11469, it/sec: 4.50097302463983
epoch 1 iter 50720: train loss 1.13070. lr 3.000000e-04, running loss 1.11467, it/sec: 4.461915043243005
epoch 1 iter 50740: train loss 1.11656. lr 3.000000e-04, running loss 1.11471, it/sec: 4.50090601085335
epoch 1 iter 50760: train loss 1.11531. lr 3.000000e-04, running loss 1.11461, it/sec: 4.456764942863393
epoch 1 iter 50780: train loss 1.15466. lr 3.000000e-04, running loss 1.11465, it/sec: 4.508253097162234
epoch 1 iter 50800: train loss 1.09262. lr 3.000000e-04, running loss 1.11461, it/sec: 4.446355270710954
epoch 1 iter 50820: train loss 1.09779. lr 3.000000e-04, running loss 1.11468, it/sec: 4.493102084196585
epoch 1 iter 50840: train loss 1.10306. lr 3.000000e-04, running loss 1.11462, it/sec: 4.479562401389671
epoch 1 iter 50860: train loss 1.13193. lr 3.000000e-04, running loss 1.11482, it/sec: 4.46870180890759
epoch 1 iter 50880: train loss 1.10358. lr 3.000000e-04, running loss 1.11470, it/sec: 4.51003571745783
epoch 1 iter 50900: train loss 1.11998. lr 3.000000e-04, running loss 1.11480, it/sec: 4.462426854091449
epoch 1 iter 50920: train loss 1.14578. lr 3.000000e-04, running loss 1.11488, it/sec: 4.504393092392636
epoch 1 iter 50940: train loss 1.11419. lr 3.000000e-04, running loss 1.11485, it/sec: 4.484865219169798
epoch 1 iter 50960: train loss 1.11421. lr 3.000000e-04, running loss 1.11486, it/sec: 4.51062558608566
epoch 1 iter 50980: train loss 1.11531. lr 3.000000e-04, running loss 1.11481, it/sec: 4.46850282419605
epoch 1 iter 51000: train loss 1.12631. lr 3.000000e-04, running loss 1.11479, it/sec: 4.502814407158156
epoch 1 iter 51020: train loss 1.09968. lr 3.000000e-04, running loss 1.11498, it/sec: 4.4717437244814
epoch 1 iter 51040: train loss 1.15177. lr 3.000000e-04, running loss 1.11499, it/sec: 4.51176686398041
epoch 1 iter 51060: train loss 1.10647. lr 3.000000e-04, running loss 1.11492, it/sec: 4.45183444962075
epoch 1 iter 51080: train loss 1.12238. lr 3.000000e-04, running loss 1.11500, it/sec: 4.530662302859849
epoch 1 iter 51100: train loss 1.11117. lr 3.000000e-04, running loss 1.11497, it/sec: 4.4453203700118165
epoch 1 iter 51120: train loss 1.11209. lr 3.000000e-04, running loss 1.11501, it/sec: 4.5138127784692275
epoch 1 iter 51140: train loss 1.10701. lr 3.000000e-04, running loss 1.11511, it/sec: 4.43505008252612
epoch 1 iter 51160: train loss 1.11976. lr 3.000000e-04, running loss 1.11512, it/sec: 4.483270603940695
epoch 1 iter 51180: train loss 1.09686. lr 3.000000e-04, running loss 1.11533, it/sec: 4.4843465595552585
epoch 1 iter 51200: train loss 1.10432. lr 3.000000e-04, running loss 1.11530, it/sec: 4.455290284226677
epoch 1 iter 51220: train loss 1.09234. lr 3.000000e-04, running loss 1.11533, it/sec: 4.501625835995852
epoch 1 iter 51240: train loss 1.11277. lr 3.000000e-04, running loss 1.11526, it/sec: 4.508589123587569
epoch 1 iter 51260: train loss 1.09990. lr 3.000000e-04, running loss 1.11524, it/sec: 4.45513558276676
epoch 1 iter 51280: train loss 1.10455. lr 3.000000e-04, running loss 1.11519, it/sec: 4.502666037460337
epoch 1 iter 51300: train loss 1.13432. lr 3.000000e-04, running loss 1.11510, it/sec: 4.5000700498339326
epoch 1 iter 51320: train loss 1.11263. lr 3.000000e-04, running loss 1.11512, it/sec: 4.474772263978017
epoch 1 iter 51340: train loss 1.09908. lr 3.000000e-04, running loss 1.11496, it/sec: 4.495953970451699
epoch 1 iter 51360: train loss 1.08835. lr 3.000000e-04, running loss 1.11514, it/sec: 4.462168873341644
epoch 1 iter 51380: train loss 1.12015. lr 3.000000e-04, running loss 1.11504, it/sec: 4.527272462854812
epoch 1 iter 51400: train loss 1.12743. lr 3.000000e-04, running loss 1.11489, it/sec: 4.45057147719716
epoch 1 iter 51420: train loss 1.12975. lr 3.000000e-04, running loss 1.11493, it/sec: 4.517465417107145
epoch 1 iter 51440: train loss 1.11019. lr 3.000000e-04, running loss 1.11485, it/sec: 4.410251781438057
epoch 1 iter 51460: train loss 1.11224. lr 3.000000e-04, running loss 1.11483, it/sec: 4.510834158949395
epoch 1 iter 51480: train loss 1.09620. lr 3.000000e-04, running loss 1.11475, it/sec: 4.452460218642481
epoch 1 iter 51500: train loss 1.11260. lr 3.000000e-04, running loss 1.11486, it/sec: 4.497841276451435
epoch 1 iter 51520: train loss 1.10554. lr 3.000000e-04, running loss 1.11481, it/sec: 4.469065499819083
epoch 1 iter 51540: train loss 1.11766. lr 3.000000e-04, running loss 1.11618, it/sec: 4.521108745087706
epoch 1 iter 51560: train loss 1.10156. lr 3.000000e-04, running loss 1.11621, it/sec: 4.483538087372575
epoch 1 iter 51580: train loss 1.10971. lr 3.000000e-04, running loss 1.11612, it/sec: 4.540588767637818
epoch 1 iter 51600: train loss 1.10354. lr 3.000000e-04, running loss 1.11609, it/sec: 4.450462933905232
epoch 1 iter 51620: train loss 1.12401. lr 3.000000e-04, running loss 1.11609, it/sec: 4.477373677323055
epoch 1 iter 51640: train loss 1.10571. lr 3.000000e-04, running loss 1.11619, it/sec: 4.447955383032776
epoch 1 iter 51660: train loss 1.11679. lr 3.000000e-04, running loss 1.11611, it/sec: 4.465328945676365
epoch 1 iter 51680: train loss 1.13712. lr 3.000000e-04, running loss 1.11601, it/sec: 4.475143291037537
epoch 1 iter 51700: train loss 1.11680. lr 3.000000e-04, running loss 1.11610, it/sec: 4.486650907846082
epoch 1 iter 51720: train loss 1.11507. lr 3.000000e-04, running loss 1.11601, it/sec: 4.5048567765643215
epoch 1 iter 51740: train loss 1.12349. lr 3.000000e-04, running loss 1.11612, it/sec: 4.4660367189681995
epoch 1 iter 51760: train loss 1.12747. lr 3.000000e-04, running loss 1.11606, it/sec: 4.516126695603884
epoch 1 iter 51780: train loss 1.08947. lr 3.000000e-04, running loss 1.11599, it/sec: 4.45217613292437
epoch 1 iter 51800: train loss 1.10605. lr 3.000000e-04, running loss 1.11590, it/sec: 4.502526901809799
epoch 1 iter 51820: train loss 1.12026. lr 3.000000e-04, running loss 1.11600, it/sec: 4.4461893462499775
epoch 1 iter 51840: train loss 1.11681. lr 3.000000e-04, running loss 1.11596, it/sec: 4.462382149060868
epoch 1 iter 51860: train loss 1.11989. lr 3.000000e-04, running loss 1.11589, it/sec: 4.486791681812033
epoch 1 iter 51880: train loss 1.10172. lr 3.000000e-04, running loss 1.11586, it/sec: 4.4499434276689644
epoch 1 iter 51900: train loss 1.11186. lr 3.000000e-04, running loss 1.11582, it/sec: 4.466630316356121
epoch 1 iter 51920: train loss 1.13352. lr 3.000000e-04, running loss 1.11588, it/sec: 4.535140029671947
epoch 1 iter 51940: train loss 1.11884. lr 3.000000e-04, running loss 1.11590, it/sec: 4.461085104577188
epoch 1 iter 51960: train loss 1.12613. lr 3.000000e-04, running loss 1.11586, it/sec: 4.505330157050688
epoch 1 iter 51980: train loss 1.09963. lr 3.000000e-04, running loss 1.11588, it/sec: 4.4572109270478695
epoch 1 iter 52000: train loss 1.12623. lr 3.000000e-04, running loss 1.11585, it/sec: 4.503178054695018
epoch 1 iter 52020: train loss 1.11241. lr 3.000000e-04, running loss 1.11579, it/sec: 4.474586732728218
epoch 1 iter 52040: train loss 1.11217. lr 3.000000e-04, running loss 1.11579, it/sec: 4.518734181206244
epoch 1 iter 52060: train loss 1.10953. lr 3.000000e-04, running loss 1.11578, it/sec: 4.4837255260297715
epoch 1 iter 52080: train loss 1.10209. lr 3.000000e-04, running loss 1.11573, it/sec: 4.50496812992538
epoch 1 iter 52100: train loss 1.09542. lr 3.000000e-04, running loss 1.11552, it/sec: 4.479916964091153
epoch 1 iter 52120: train loss 1.12487. lr 3.000000e-04, running loss 1.11540, it/sec: 4.49853312663008
epoch 1 iter 52140: train loss 1.11768. lr 3.000000e-04, running loss 1.11538, it/sec: 4.467274759597531
epoch 1 iter 52160: train loss 1.11856. lr 3.000000e-04, running loss 1.11534, it/sec: 4.5367180012839885
epoch 1 iter 52180: train loss 1.10667. lr 3.000000e-04, running loss 1.11531, it/sec: 4.4522063819065805
epoch 1 iter 52200: train loss 1.11820. lr 3.000000e-04, running loss 1.11538, it/sec: 4.512697978794991
epoch 1 iter 52220: train loss 1.10376. lr 3.000000e-04, running loss 1.11525, it/sec: 4.45878840664547
epoch 1 iter 52240: train loss 1.11529. lr 3.000000e-04, running loss 1.11522, it/sec: 4.513290029723128
epoch 1 iter 52260: train loss 1.08365. lr 3.000000e-04, running loss 1.11514, it/sec: 4.453521057225111
epoch 1 iter 52280: train loss 1.10609. lr 3.000000e-04, running loss 1.11516, it/sec: 4.503654023343703
epoch 1 iter 52300: train loss 1.09544. lr 3.000000e-04, running loss 1.11510, it/sec: 4.513997543172706
epoch 1 iter 52320: train loss 1.11992. lr 3.000000e-04, running loss 1.11508, it/sec: 4.469058089704455
epoch 1 iter 52340: train loss 1.13828. lr 3.000000e-04, running loss 1.11508, it/sec: 4.49137794616696
epoch 1 iter 52360: train loss 1.11699. lr 3.000000e-04, running loss 1.11502, it/sec: 4.450770591912288
epoch 1 iter 52380: train loss 1.11130. lr 3.000000e-04, running loss 1.11496, it/sec: 4.5139370886715104
epoch 1 iter 52400: train loss 1.10600. lr 3.000000e-04, running loss 1.11503, it/sec: 4.474394089698153
epoch 1 iter 52420: train loss 1.10767. lr 3.000000e-04, running loss 1.11501, it/sec: 4.515017784513211
epoch 1 iter 52440: train loss 1.10558. lr 3.000000e-04, running loss 1.11493, it/sec: 4.443880032778857
epoch 1 iter 52460: train loss 1.12186. lr 3.000000e-04, running loss 1.11498, it/sec: 4.50932092578126
epoch 1 iter 52480: train loss 1.10861. lr 3.000000e-04, running loss 1.11518, it/sec: 4.455193639469422
epoch 1 iter 52500: train loss 1.11423. lr 3.000000e-04, running loss 1.11511, it/sec: 4.516122025287651
epoch 1 iter 52520: train loss 1.11669. lr 3.000000e-04, running loss 1.11512, it/sec: 4.442077883604559
epoch 1 iter 52540: train loss 1.10593. lr 3.000000e-04, running loss 1.11514, it/sec: 4.500769028524484
epoch 1 iter 52560: train loss 1.12150. lr 3.000000e-04, running loss 1.11506, it/sec: 4.456844138065974
epoch 1 iter 52580: train loss 1.11368. lr 3.000000e-04, running loss 1.11502, it/sec: 4.52050869383287
epoch 1 iter 52600: train loss 1.11756. lr 3.000000e-04, running loss 1.11496, it/sec: 4.440210299110285
epoch 1 iter 52620: train loss 1.12219. lr 3.000000e-04, running loss 1.11487, it/sec: 4.509048366675852
epoch 1 iter 52640: train loss 1.13235. lr 3.000000e-04, running loss 1.11483, it/sec: 4.457307421118466
epoch 1 iter 52660: train loss 1.11584. lr 3.000000e-04, running loss 1.11509, it/sec: 4.503293971252593
epoch 1 iter 52680: train loss 1.09682. lr 3.000000e-04, running loss 1.11504, it/sec: 4.497306524550629
epoch 1 iter 52700: train loss 1.11339. lr 3.000000e-04, running loss 1.11500, it/sec: 4.474950100032332
epoch 1 iter 52720: train loss 1.10067. lr 3.000000e-04, running loss 1.11494, it/sec: 4.469002627392693
epoch 1 iter 52740: train loss 1.10761. lr 3.000000e-04, running loss 1.11496, it/sec: 4.51977166161741
epoch 1 iter 52760: train loss 1.11480. lr 3.000000e-04, running loss 1.11492, it/sec: 4.458992214167083
epoch 1 iter 52780: train loss 1.09870. lr 3.000000e-04, running loss 1.11484, it/sec: 4.525964077429892
epoch 1 iter 52800: train loss 1.09904. lr 3.000000e-04, running loss 1.11498, it/sec: 4.449190999551116
epoch 1 iter 52820: train loss 1.12953. lr 3.000000e-04, running loss 1.11502, it/sec: 4.506635757422514
epoch 1 iter 52840: train loss 1.28387. lr 3.000000e-04, running loss 1.11515, it/sec: 4.4908458803675835
epoch 1 iter 52860: train loss 1.09010. lr 3.000000e-04, running loss 1.11507, it/sec: 4.444284865636664
epoch 1 iter 52880: train loss 1.09973. lr 3.000000e-04, running loss 1.11505, it/sec: 4.505659800510488
epoch 1 iter 52900: train loss 1.09069. lr 3.000000e-04, running loss 1.11499, it/sec: 4.46654824030804
epoch 1 iter 52920: train loss 1.08987. lr 3.000000e-04, running loss 1.11488, it/sec: 4.502505595302115
epoch 1 iter 52940: train loss 1.10540. lr 3.000000e-04, running loss 1.11484, it/sec: 4.466529647676834
epoch 1 iter 52960: train loss 1.11981. lr 3.000000e-04, running loss 1.11481, it/sec: 4.509727114717669
epoch 1 iter 52980: train loss 1.10315. lr 3.000000e-04, running loss 1.11479, it/sec: 4.439304637313736
epoch 1 iter 53000: train loss 1.09928. lr 3.000000e-04, running loss 1.11476, it/sec: 4.519313786367933
epoch 1 iter 53020: train loss 1.10388. lr 3.000000e-04, running loss 1.11478, it/sec: 4.4665098161719845
epoch 1 iter 53040: train loss 1.12332. lr 3.000000e-04, running loss 1.11485, it/sec: 4.507064435638966
epoch 1 iter 53060: train loss 1.10607. lr 3.000000e-04, running loss 1.11482, it/sec: 4.476054401140304
epoch 1 iter 53080: train loss 1.10763. lr 3.000000e-04, running loss 1.11477, it/sec: 4.516878738945834
epoch 1 iter 53100: train loss 1.10810. lr 3.000000e-04, running loss 1.11482, it/sec: 4.456092969278397
epoch 1 iter 53120: train loss 1.12826. lr 3.000000e-04, running loss 1.11480, it/sec: 4.492552958069675
epoch 1 iter 53140: train loss 1.12426. lr 3.000000e-04, running loss 1.11494, it/sec: 4.465085382479344
epoch 1 iter 53160: train loss 1.10359. lr 3.000000e-04, running loss 1.11479, it/sec: 4.519302799091566
epoch 1 iter 53180: train loss 1.11795. lr 3.000000e-04, running loss 1.11482, it/sec: 4.492221678677564
epoch 1 iter 53200: train loss 1.13211. lr 3.000000e-04, running loss 1.11482, it/sec: 4.497711157054158
epoch 1 iter 53220: train loss 1.13722. lr 3.000000e-04, running loss 1.11476, it/sec: 4.457803987231973
epoch 1 iter 53240: train loss 1.12878. lr 3.000000e-04, running loss 1.11461, it/sec: 4.470124454835258
epoch 1 iter 53260: train loss 1.12003. lr 3.000000e-04, running loss 1.11461, it/sec: 4.447518707610561
epoch 1 iter 53280: train loss 1.10903. lr 3.000000e-04, running loss 1.11460, it/sec: 4.521574630099882
epoch 1 iter 53300: train loss 1.11386. lr 3.000000e-04, running loss 1.11452, it/sec: 4.4587964180559805
epoch 1 iter 53320: train loss 1.11737. lr 3.000000e-04, running loss 1.11458, it/sec: 4.507087796612231
epoch 1 iter 53340: train loss 1.11621. lr 3.000000e-04, running loss 1.11453, it/sec: 4.456831365500142
epoch 1 iter 53360: train loss 1.10402. lr 3.000000e-04, running loss 1.11441, it/sec: 4.538532623470289
epoch 1 iter 53380: train loss 1.12575. lr 3.000000e-04, running loss 1.11434, it/sec: 4.413769199351113
epoch 1 iter 53400: train loss 1.14824. lr 3.000000e-04, running loss 1.11426, it/sec: 4.50776755951684
epoch 1 iter 53420: train loss 1.11562. lr 3.000000e-04, running loss 1.11435, it/sec: 4.440246812321249
epoch 1 iter 53440: train loss 1.12183. lr 3.000000e-04, running loss 1.11426, it/sec: 4.499854938554275
epoch 1 iter 53460: train loss 1.13011. lr 3.000000e-04, running loss 1.11427, it/sec: 4.458618729580552
epoch 1 iter 53480: train loss 1.11249. lr 3.000000e-04, running loss 1.11427, it/sec: 4.49097072142253
epoch 1 iter 53500: train loss 1.11348. lr 3.000000e-04, running loss 1.11425, it/sec: 4.479765925764881
epoch 1 iter 53520: train loss 1.13207. lr 3.000000e-04, running loss 1.11429, it/sec: 4.475033065653762
epoch 1 iter 53540: train loss 1.10592. lr 3.000000e-04, running loss 1.11434, it/sec: 4.500664302585409
epoch 1 iter 53560: train loss 1.11877. lr 3.000000e-04, running loss 1.11431, it/sec: 4.47613916990009
epoch 1 iter 53580: train loss 1.12979. lr 3.000000e-04, running loss 1.11432, it/sec: 4.46077912425387
epoch 1 iter 53600: train loss 1.11017. lr 3.000000e-04, running loss 1.11428, it/sec: 4.442077725103723
epoch 1 iter 53620: train loss 1.12589. lr 3.000000e-04, running loss 1.11436, it/sec: 4.520883175863269
epoch 1 iter 53640: train loss 1.13033. lr 3.000000e-04, running loss 1.11436, it/sec: 4.4767369188656065
epoch 1 iter 53660: train loss 1.12294. lr 3.000000e-04, running loss 1.11435, it/sec: 4.48103606649566
epoch 1 iter 53680: train loss 1.12780. lr 3.000000e-04, running loss 1.11423, it/sec: 4.473391322667745
epoch 1 iter 53700: train loss 1.14454. lr 3.000000e-04, running loss 1.11437, it/sec: 4.4862632770056585
epoch 1 iter 53720: train loss 1.08763. lr 3.000000e-04, running loss 1.11433, it/sec: 4.456220789992791
epoch 1 iter 53740: train loss 1.11407. lr 3.000000e-04, running loss 1.11432, it/sec: 4.517480152713312
epoch 1 iter 53760: train loss 1.11322. lr 3.000000e-04, running loss 1.11447, it/sec: 4.480176641347696
epoch 1 iter 53780: train loss 1.10014. lr 3.000000e-04, running loss 1.11441, it/sec: 4.507764267848547
epoch 1 iter 53800: train loss 1.12922. lr 3.000000e-04, running loss 1.11435, it/sec: 4.459108827654057
epoch 1 iter 53820: train loss 1.11863. lr 3.000000e-04, running loss 1.11433, it/sec: 4.521278938953717
epoch 1 iter 53840: train loss 1.11262. lr 3.000000e-04, running loss 1.11429, it/sec: 4.425781062843165
epoch 1 iter 53860: train loss 1.12502. lr 3.000000e-04, running loss 1.11427, it/sec: 4.497147070369764
epoch 1 iter 53880: train loss 1.12611. lr 3.000000e-04, running loss 1.11425, it/sec: 4.4455516044200065
epoch 1 iter 53900: train loss 1.09906. lr 3.000000e-04, running loss 1.11409, it/sec: 4.4667840820676314
epoch 1 iter 53920: train loss 1.16462. lr 3.000000e-04, running loss 1.11463, it/sec: 4.455915277773041
epoch 1 iter 53940: train loss 1.10506. lr 3.000000e-04, running loss 1.11451, it/sec: 4.5230100732971845
epoch 1 iter 53960: train loss 1.13134. lr 3.000000e-04, running loss 1.11456, it/sec: 4.4453018938765565
epoch 1 iter 53980: train loss 1.10067. lr 3.000000e-04, running loss 1.11448, it/sec: 4.510724060961165
epoch 1 iter 54000: train loss 1.11269. lr 3.000000e-04, running loss 1.11449, it/sec: 4.4557298773666405
epoch 1 iter 54020: train loss 1.11642. lr 3.000000e-04, running loss 1.11446, it/sec: 4.530882219491593
epoch 1 iter 54040: train loss 1.11630. lr 3.000000e-04, running loss 1.11444, it/sec: 4.42694218526462
epoch 1 iter 54060: train loss 1.13180. lr 3.000000e-04, running loss 1.11450, it/sec: 4.49681826758164
epoch 1 iter 54080: train loss 1.12104. lr 3.000000e-04, running loss 1.11463, it/sec: 4.465373491570501
epoch 1 iter 54100: train loss 1.12078. lr 3.000000e-04, running loss 1.11462, it/sec: 4.498531164188211
epoch 1 iter 54120: train loss 1.11438. lr 3.000000e-04, running loss 1.11477, it/sec: 4.492831379932354
epoch 1 iter 54140: train loss 1.09968. lr 3.000000e-04, running loss 1.11468, it/sec: 4.465429980398903
epoch 1 iter 54160: train loss 1.11608. lr 3.000000e-04, running loss 1.11459, it/sec: 4.515881209616213
epoch 1 iter 54180: train loss 1.09468. lr 3.000000e-04, running loss 1.11454, it/sec: 4.45389519392892
epoch 1 iter 54200: train loss 1.11335. lr 3.000000e-04, running loss 1.11447, it/sec: 4.531822781414723
epoch 1 iter 54220: train loss 1.13529. lr 3.000000e-04, running loss 1.11441, it/sec: 4.468916927877392
epoch 1 iter 54240: train loss 1.08133. lr 3.000000e-04, running loss 1.11439, it/sec: 4.509418979135763
epoch 1 iter 54260: train loss 1.10503. lr 3.000000e-04, running loss 1.11443, it/sec: 4.486548448926681
epoch 1 iter 54280: train loss 1.13443. lr 3.000000e-04, running loss 1.11444, it/sec: 4.52034791723097
epoch 1 iter 54300: train loss 1.11227. lr 3.000000e-04, running loss 1.11454, it/sec: 4.451934061262988
epoch 1 iter 54320: train loss 1.13719. lr 3.000000e-04, running loss 1.11457, it/sec: 4.510980199858457
epoch 1 iter 54340: train loss 1.11453. lr 3.000000e-04, running loss 1.11456, it/sec: 4.4500266962190755
epoch 1 iter 54360: train loss 1.11201. lr 3.000000e-04, running loss 1.11453, it/sec: 4.511162798349379
epoch 1 iter 54380: train loss 1.12112. lr 3.000000e-04, running loss 1.11453, it/sec: 4.365667983660657
epoch 1 iter 54400: train loss 1.11390. lr 3.000000e-04, running loss 1.11446, it/sec: 4.486052843247161
epoch 1 iter 54420: train loss 1.14467. lr 3.000000e-04, running loss 1.11455, it/sec: 4.446193754524554
epoch 1 iter 54440: train loss 1.11183. lr 3.000000e-04, running loss 1.11461, it/sec: 4.508036896485783
epoch 1 iter 54460: train loss 1.12237. lr 3.000000e-04, running loss 1.11451, it/sec: 4.453292880930275
epoch 1 iter 54480: train loss 1.11586. lr 3.000000e-04, running loss 1.11444, it/sec: 4.506013613796314
epoch 1 iter 54500: train loss 1.12506. lr 3.000000e-04, running loss 1.11432, it/sec: 4.47536888544091
epoch 1 iter 54520: train loss 1.11581. lr 3.000000e-04, running loss 1.11439, it/sec: 4.467988001234527
epoch 1 iter 54540: train loss 1.10859. lr 3.000000e-04, running loss 1.11441, it/sec: 4.48327382015593
epoch 1 iter 54560: train loss 1.09894. lr 3.000000e-04, running loss 1.11431, it/sec: 4.513715451832431
epoch 1 iter 54580: train loss 1.11646. lr 3.000000e-04, running loss 1.11422, it/sec: 4.452305217497699
epoch 1 iter 54600: train loss 1.12095. lr 3.000000e-04, running loss 1.11419, it/sec: 4.498637349359752
epoch 1 iter 54620: train loss 1.11794. lr 3.000000e-04, running loss 1.11424, it/sec: 4.4346497416765756
epoch 1 iter 54640: train loss 1.09281. lr 3.000000e-04, running loss 1.11427, it/sec: 4.521911889612186
epoch 1 iter 54660: train loss 1.11117. lr 3.000000e-04, running loss 1.11430, it/sec: 4.44150247260752
epoch 1 iter 54680: train loss 1.10767. lr 3.000000e-04, running loss 1.11425, it/sec: 4.4088081655070726
epoch 1 iter 54700: train loss 1.11168. lr 3.000000e-04, running loss 1.11423, it/sec: 4.513439081439868
epoch 1 iter 54720: train loss 1.13497. lr 3.000000e-04, running loss 1.11422, it/sec: 4.474534896189807
epoch 1 iter 54740: train loss 1.10599. lr 3.000000e-04, running loss 1.11431, it/sec: 4.502432168693125
epoch 1 iter 54760: train loss 1.12742. lr 3.000000e-04, running loss 1.11432, it/sec: 4.461111752730447
epoch 1 iter 54780: train loss 1.11858. lr 3.000000e-04, running loss 1.11431, it/sec: 4.52156422327208
epoch 1 iter 54800: train loss 1.10565. lr 3.000000e-04, running loss 1.11420, it/sec: 4.452000022714183
epoch 1 iter 54820: train loss 1.12906. lr 3.000000e-04, running loss 1.11417, it/sec: 4.511757053216146
epoch 1 iter 54840: train loss 1.12954. lr 3.000000e-04, running loss 1.11404, it/sec: 4.452093120807984
epoch 1 iter 54860: train loss 1.10809. lr 3.000000e-04, running loss 1.11416, it/sec: 4.486812476597752
epoch 1 iter 54880: train loss 1.10217. lr 3.000000e-04, running loss 1.11418, it/sec: 4.470085950535637
epoch 1 iter 54900: train loss 1.11587. lr 3.000000e-04, running loss 1.11419, it/sec: 4.508208585998645
epoch 1 iter 54920: train loss 1.08626. lr 3.000000e-04, running loss 1.11420, it/sec: 4.479235622955736
epoch 1 iter 54940: train loss 1.12322. lr 3.000000e-04, running loss 1.11499, it/sec: 4.513605234496118
epoch 1 iter 54960: train loss 1.10343. lr 3.000000e-04, running loss 1.11489, it/sec: 4.416082868661484
epoch 1 iter 54980: train loss 1.09360. lr 3.000000e-04, running loss 1.11494, it/sec: 4.511097250476922
epoch 1 iter 55000: train loss 1.10785. lr 3.000000e-04, running loss 1.11491, it/sec: 4.455166544381701
epoch 1 iter 55020: train loss 1.09598. lr 3.000000e-04, running loss 1.11489, it/sec: 4.514350180838336
epoch 1 iter 55040: train loss 1.11039. lr 3.000000e-04, running loss 1.11488, it/sec: 4.457607720624567
epoch 1 iter 55060: train loss 1.11673. lr 3.000000e-04, running loss 1.11500, it/sec: 4.50354094992756
epoch 1 iter 55080: train loss 1.11344. lr 3.000000e-04, running loss 1.11494, it/sec: 4.496369680423216
epoch 1 iter 55100: train loss 1.10950. lr 3.000000e-04, running loss 1.11481, it/sec: 4.445566802947348
epoch 1 iter 55120: train loss 1.08643. lr 3.000000e-04, running loss 1.11487, it/sec: 4.516296310369886
epoch 1 iter 55140: train loss 1.09940. lr 3.000000e-04, running loss 1.11479, it/sec: 4.483451730025181
epoch 1 iter 55160: train loss 1.10525. lr 3.000000e-04, running loss 1.11478, it/sec: 4.472636686579782
epoch 1 iter 55180: train loss 1.12308. lr 3.000000e-04, running loss 1.11483, it/sec: 4.49896004201715
epoch 1 iter 55200: train loss 1.10658. lr 3.000000e-04, running loss 1.11484, it/sec: 4.513779325373981
epoch 1 iter 55220: train loss 1.09890. lr 3.000000e-04, running loss 1.11489, it/sec: 4.464127197790679
epoch 1 iter 55240: train loss 1.13339. lr 3.000000e-04, running loss 1.11487, it/sec: 4.5015969393936555
epoch 1 iter 55260: train loss 1.09573. lr 3.000000e-04, running loss 1.11479, it/sec: 4.452687616031657
epoch 1 iter 55280: train loss 1.10989. lr 3.000000e-04, running loss 1.11485, it/sec: 4.511140351044716
epoch 1 iter 55300: train loss 1.12918. lr 3.000000e-04, running loss 1.11482, it/sec: 4.468038129085886
epoch 1 iter 55320: train loss 1.09323. lr 3.000000e-04, running loss 1.11481, it/sec: 4.535342360700796
epoch 1 iter 55340: train loss 1.11256. lr 3.000000e-04, running loss 1.11487, it/sec: 4.464806881032921
epoch 1 iter 55360: train loss 1.11835. lr 3.000000e-04, running loss 1.11481, it/sec: 4.53006695903669
epoch 1 iter 55380: train loss 1.12041. lr 3.000000e-04, running loss 1.11467, it/sec: 4.474898034777823
epoch 1 iter 55400: train loss 1.09980. lr 3.000000e-04, running loss 1.11477, it/sec: 4.497416596041832
epoch 1 iter 55420: train loss 1.11488. lr 3.000000e-04, running loss 1.11472, it/sec: 4.459113361119441
epoch 1 iter 55440: train loss 1.10153. lr 3.000000e-04, running loss 1.11471, it/sec: 4.515660628095216
epoch 1 iter 55460: train loss 1.09934. lr 3.000000e-04, running loss 1.11467, it/sec: 4.458689620902296
epoch 1 iter 55480: train loss 1.13745. lr 3.000000e-04, running loss 1.11464, it/sec: 4.504188278366169
epoch 1 iter 55500: train loss 1.09423. lr 3.000000e-04, running loss 1.11488, it/sec: 4.462826249811955
epoch 1 iter 55520: train loss 1.12865. lr 3.000000e-04, running loss 1.11477, it/sec: 4.504484984930388
epoch 1 iter 55540: train loss 1.12252. lr 3.000000e-04, running loss 1.11477, it/sec: 4.452461050627659
epoch 1 iter 55560: train loss 1.10945. lr 3.000000e-04, running loss 1.11483, it/sec: 4.499048557770935
epoch 1 iter 55580: train loss 1.08915. lr 3.000000e-04, running loss 1.11467, it/sec: 4.458672265835535
epoch 1 iter 55600: train loss 1.10333. lr 3.000000e-04, running loss 1.11465, it/sec: 4.515382916939532
epoch 1 iter 55620: train loss 1.11987. lr 3.000000e-04, running loss 1.11460, it/sec: 4.473722592650228
epoch 1 iter 55640: train loss 1.11942. lr 3.000000e-04, running loss 1.11453, it/sec: 4.49279090787598
epoch 1 iter 55660: train loss 1.11089. lr 3.000000e-04, running loss 1.11447, it/sec: 4.467926775411281
epoch 1 iter 55680: train loss 1.09927. lr 3.000000e-04, running loss 1.11448, it/sec: 4.4810016505269274
epoch 1 iter 55700: train loss 1.11691. lr 3.000000e-04, running loss 1.11447, it/sec: 4.482069405338787
epoch 1 iter 55720: train loss 1.13027. lr 3.000000e-04, running loss 1.11446, it/sec: 4.483175775048411
epoch 1 iter 55740: train loss 1.12680. lr 3.000000e-04, running loss 1.11446, it/sec: 4.495342752159701
epoch 1 iter 55760: train loss 1.09571. lr 3.000000e-04, running loss 1.11442, it/sec: 4.506404157025285
epoch 1 iter 55780: train loss 1.11924. lr 3.000000e-04, running loss 1.11442, it/sec: 4.465332456530471
epoch 1 iter 55800: train loss 1.10958. lr 3.000000e-04, running loss 1.11438, it/sec: 4.5131595650831615
epoch 1 iter 55820: train loss 1.10636. lr 3.000000e-04, running loss 1.11440, it/sec: 4.471370260728415
epoch 1 iter 55840: train loss 1.13502. lr 3.000000e-04, running loss 1.11449, it/sec: 4.4699062423365925
epoch 1 iter 55860: train loss 1.10056. lr 3.000000e-04, running loss 1.11449, it/sec: 4.499241890955707
epoch 1 iter 55880: train loss 1.09662. lr 3.000000e-04, running loss 1.11448, it/sec: 4.464976510714156
epoch 1 iter 55900: train loss 1.11667. lr 3.000000e-04, running loss 1.11448, it/sec: 4.479639538945198
epoch 1 iter 55920: train loss 1.11253. lr 3.000000e-04, running loss 1.11439, it/sec: 4.482069364412218
epoch 1 iter 55940: train loss 1.08496. lr 3.000000e-04, running loss 1.11429, it/sec: 4.528330149867079
epoch 1 iter 55960: train loss 1.11544. lr 3.000000e-04, running loss 1.11436, it/sec: 4.459398909311442
epoch 1 iter 55980: train loss 1.11186. lr 3.000000e-04, running loss 1.11429, it/sec: 4.527063329791961
epoch 1 iter 56000: train loss 1.11606. lr 3.000000e-04, running loss 1.11418, it/sec: 4.450399237255447
epoch 1 iter 56020: train loss 1.10658. lr 3.000000e-04, running loss 1.11409, it/sec: 4.517805554363611
epoch 1 iter 56040: train loss 1.10645. lr 3.000000e-04, running loss 1.11421, it/sec: 4.457884590109022
epoch 1 iter 56060: train loss 1.12672. lr 3.000000e-04, running loss 1.11424, it/sec: 4.5035765039790165
epoch 1 iter 56080: train loss 1.10018. lr 3.000000e-04, running loss 1.11429, it/sec: 4.460262497735949
epoch 1 iter 56100: train loss 1.11732. lr 3.000000e-04, running loss 1.11430, it/sec: 4.508227650273793
epoch 1 iter 56120: train loss 1.11775. lr 3.000000e-04, running loss 1.11427, it/sec: 4.442711570766389
epoch 1 iter 56140: train loss 1.10127. lr 3.000000e-04, running loss 1.11444, it/sec: 4.515419433192966
epoch 1 iter 56160: train loss 1.15178. lr 3.000000e-04, running loss 1.11456, it/sec: 4.454529360673629
epoch 1 iter 56180: train loss 1.11296. lr 3.000000e-04, running loss 1.11457, it/sec: 4.473522960202678
epoch 1 iter 56200: train loss 1.13064. lr 3.000000e-04, running loss 1.11461, it/sec: 4.473100617409908
epoch 1 iter 56220: train loss 1.11836. lr 3.000000e-04, running loss 1.11454, it/sec: 4.486661435795931
epoch 1 iter 56240: train loss 1.12032. lr 3.000000e-04, running loss 1.11443, it/sec: 4.481891464847216
epoch 1 iter 56260: train loss 1.12688. lr 3.000000e-04, running loss 1.11453, it/sec: 4.476869395017653
epoch 1 iter 56280: train loss 1.09126. lr 3.000000e-04, running loss 1.11454, it/sec: 4.5199090468035985
epoch 1 iter 56300: train loss 1.12473. lr 3.000000e-04, running loss 1.11454, it/sec: 4.492017020977198
epoch 1 iter 56320: train loss 1.10566. lr 3.000000e-04, running loss 1.11454, it/sec: 4.508629555384982
epoch 1 iter 56340: train loss 1.10118. lr 3.000000e-04, running loss 1.11452, it/sec: 4.451080252926597
epoch 1 iter 56360: train loss 1.12635. lr 3.000000e-04, running loss 1.11452, it/sec: 4.506536120432224
epoch 1 iter 56380: train loss 1.08873. lr 3.000000e-04, running loss 1.11444, it/sec: 4.461287848912646
epoch 1 iter 56400: train loss 1.12611. lr 3.000000e-04, running loss 1.11447, it/sec: 4.513372325552228
epoch 1 iter 56420: train loss 1.09335. lr 3.000000e-04, running loss 1.11436, it/sec: 4.45735357419616
epoch 1 iter 56440: train loss 1.11347. lr 3.000000e-04, running loss 1.11439, it/sec: 4.523799855132501
epoch 1 iter 56460: train loss 1.12769. lr 3.000000e-04, running loss 1.11443, it/sec: 4.458693696434963
epoch 1 iter 56480: train loss 1.11919. lr 3.000000e-04, running loss 1.11438, it/sec: 4.521654977897326
epoch 1 iter 56500: train loss 1.10881. lr 3.000000e-04, running loss 1.11443, it/sec: 4.447917872339693
epoch 1 iter 56520: train loss 1.12257. lr 3.000000e-04, running loss 1.11442, it/sec: 4.5171436762538075
epoch 1 iter 56540: train loss 1.13036. lr 3.000000e-04, running loss 1.11442, it/sec: 4.47585439961658
epoch 1 iter 56560: train loss 1.12360. lr 3.000000e-04, running loss 1.11440, it/sec: 4.481086828852206
epoch 1 iter 56580: train loss 1.11957. lr 3.000000e-04, running loss 1.11457, it/sec: 4.484349334857631
epoch 1 iter 56600: train loss 1.11672. lr 3.000000e-04, running loss 1.11452, it/sec: 4.481003638611528
epoch 1 iter 56620: train loss 1.11526. lr 3.000000e-04, running loss 1.11458, it/sec: 4.5173160399552446
epoch 1 iter 56640: train loss 1.10279. lr 3.000000e-04, running loss 1.11453, it/sec: 4.489344775491011
epoch 1 iter 56660: train loss 1.14874. lr 3.000000e-04, running loss 1.11461, it/sec: 4.450490049055913
epoch 1 iter 56680: train loss 1.09528. lr 3.000000e-04, running loss 1.11453, it/sec: 4.504642747628468
epoch 1 iter 56700: train loss 1.09373. lr 3.000000e-04, running loss 1.11449, it/sec: 4.458364767520447
epoch 1 iter 56720: train loss 1.10180. lr 3.000000e-04, running loss 1.11450, it/sec: 4.509765837356659
epoch 1 iter 56740: train loss 1.10877. lr 3.000000e-04, running loss 1.11438, it/sec: 4.474714115468424
epoch 1 iter 56760: train loss 1.12359. lr 3.000000e-04, running loss 1.11440, it/sec: 4.486681163092416
epoch 1 iter 56780: train loss 1.11293. lr 3.000000e-04, running loss 1.11455, it/sec: 4.466700424449828
epoch 1 iter 56800: train loss 1.07400. lr 3.000000e-04, running loss 1.11439, it/sec: 4.464666167476805
epoch 1 iter 56820: train loss 1.12378. lr 3.000000e-04, running loss 1.11443, it/sec: 4.538135729452305
epoch 1 iter 56840: train loss 1.07645. lr 3.000000e-04, running loss 1.11442, it/sec: 4.453590951817051
epoch 1 iter 56860: train loss 1.09537. lr 3.000000e-04, running loss 1.11528, it/sec: 4.4950080316044065
epoch 1 iter 56880: train loss 1.12778. lr 3.000000e-04, running loss 1.11539, it/sec: 4.444580330172257
epoch 1 iter 56900: train loss 1.10066. lr 3.000000e-04, running loss 1.11538, it/sec: 4.511540922713473
epoch 1 iter 56920: train loss 1.10601. lr 3.000000e-04, running loss 1.11537, it/sec: 4.453566575452315
epoch 1 iter 56940: train loss 1.14544. lr 3.000000e-04, running loss 1.11540, it/sec: 4.486913318691647
epoch 1 iter 56960: train loss 1.12678. lr 3.000000e-04, running loss 1.11533, it/sec: 4.4613765382656485
epoch 1 iter 56980: train loss 1.10914. lr 3.000000e-04, running loss 1.11527, it/sec: 4.514836666604766
epoch 1 iter 57000: train loss 1.12082. lr 3.000000e-04, running loss 1.11516, it/sec: 4.468232740017444
epoch 1 iter 57020: train loss 1.12124. lr 3.000000e-04, running loss 1.11521, it/sec: 4.5207125014605065
epoch 1 iter 57040: train loss 1.11033. lr 3.000000e-04, running loss 1.11526, it/sec: 4.454108076704438
epoch 1 iter 57060: train loss 1.10753. lr 3.000000e-04, running loss 1.11523, it/sec: 4.531871393106906
epoch 1 iter 57080: train loss 1.13698. lr 3.000000e-04, running loss 1.11521, it/sec: 4.45507853845538
epoch 1 iter 57100: train loss 1.12181. lr 3.000000e-04, running loss 1.11522, it/sec: 4.524265559832362
epoch 1 iter 57120: train loss 1.11075. lr 3.000000e-04, running loss 1.11525, it/sec: 4.448534209012883
epoch 1 iter 57140: train loss 1.11511. lr 3.000000e-04, running loss 1.11523, it/sec: 4.518496823256351
epoch 1 iter 57160: train loss 1.10263. lr 3.000000e-04, running loss 1.11516, it/sec: 4.472155030893003
epoch 1 iter 57180: train loss 1.10121. lr 3.000000e-04, running loss 1.11505, it/sec: 4.510033195606555
epoch 1 iter 57200: train loss 1.11690. lr 3.000000e-04, running loss 1.11508, it/sec: 4.473086151270727
epoch 1 iter 57220: train loss 1.11099. lr 3.000000e-04, running loss 1.11503, it/sec: 4.48929469279228
epoch 1 iter 57240: train loss 1.12107. lr 3.000000e-04, running loss 1.11496, it/sec: 4.464647929306368
epoch 1 iter 57260: train loss 1.13435. lr 3.000000e-04, running loss 1.11496, it/sec: 4.513658713103587
epoch 1 iter 57280: train loss 1.08243. lr 3.000000e-04, running loss 1.11491, it/sec: 4.454360087288937
epoch 1 iter 57300: train loss 1.13018. lr 3.000000e-04, running loss 1.11482, it/sec: 4.5305663833144045
epoch 1 iter 57320: train loss 1.09883. lr 3.000000e-04, running loss 1.11487, it/sec: 4.47139703189981
epoch 1 iter 57340: train loss 1.11683. lr 3.000000e-04, running loss 1.11511, it/sec: 4.513291293655856
epoch 1 iter 57360: train loss 1.13060. lr 3.000000e-04, running loss 1.11512, it/sec: 4.462556891952105
epoch 1 iter 57380: train loss 1.11334. lr 3.000000e-04, running loss 1.11515, it/sec: 4.4883783269794675
epoch 1 iter 57400: train loss 1.10432. lr 3.000000e-04, running loss 1.11509, it/sec: 4.444981713952427
epoch 1 iter 57420: train loss 1.10842. lr 3.000000e-04, running loss 1.11503, it/sec: 4.47971475178779
epoch 1 iter 57440: train loss 1.10466. lr 3.000000e-04, running loss 1.11496, it/sec: 4.4704833205204135
epoch 1 iter 57460: train loss 1.13308. lr 3.000000e-04, running loss 1.11506, it/sec: 4.495680678964198
epoch 1 iter 57480: train loss 1.10492. lr 3.000000e-04, running loss 1.11501, it/sec: 4.44861457493109
epoch 1 iter 57500: train loss 1.11213. lr 3.000000e-04, running loss 1.11496, it/sec: 4.524101894885553
epoch 1 iter 57520: train loss 1.17820. lr 3.000000e-04, running loss 1.11497, it/sec: 4.476910601542047
epoch 1 iter 57540: train loss 1.11785. lr 3.000000e-04, running loss 1.11505, it/sec: 4.510288319422731
epoch 1 iter 57560: train loss 1.13665. lr 3.000000e-04, running loss 1.11500, it/sec: 4.472581514999695
epoch 1 iter 57580: train loss 1.09931. lr 3.000000e-04, running loss 1.11491, it/sec: 4.502214337295787
epoch 1 iter 57600: train loss 1.09587. lr 3.000000e-04, running loss 1.11487, it/sec: 4.468218884079887
epoch 1 iter 57620: train loss 1.09832. lr 3.000000e-04, running loss 1.11478, it/sec: 4.491239527211719
epoch 1 iter 57640: train loss 1.09316. lr 3.000000e-04, running loss 1.11479, it/sec: 4.48659752364015
epoch 1 iter 57660: train loss 1.14958. lr 3.000000e-04, running loss 1.11481, it/sec: 4.532579588277861
epoch 1 iter 57680: train loss 1.10522. lr 3.000000e-04, running loss 1.11485, it/sec: 4.468710096592554
epoch 1 iter 57700: train loss 1.11324. lr 3.000000e-04, running loss 1.11475, it/sec: 4.489402820357774
epoch 1 iter 57720: train loss 1.10647. lr 3.000000e-04, running loss 1.11496, it/sec: 4.498280666535106
epoch 1 iter 57740: train loss 1.11589. lr 3.000000e-04, running loss 1.11495, it/sec: 4.516390117550966
epoch 1 iter 57760: train loss 1.10664. lr 3.000000e-04, running loss 1.11491, it/sec: 4.46461840789632
epoch 1 iter 57780: train loss 1.11782. lr 3.000000e-04, running loss 1.11497, it/sec: 4.504677244225994
epoch 1 iter 57800: train loss 1.12027. lr 3.000000e-04, running loss 1.11493, it/sec: 4.470023668138553
epoch 1 iter 57820: train loss 1.10833. lr 3.000000e-04, running loss 1.11491, it/sec: 4.493178255100352
epoch 1 iter 57840: train loss 1.11300. lr 3.000000e-04, running loss 1.11483, it/sec: 4.491182342412522
epoch 1 iter 57860: train loss 1.11876. lr 3.000000e-04, running loss 1.11483, it/sec: 4.449330560707998
epoch 1 iter 57880: train loss 1.11023. lr 3.000000e-04, running loss 1.11497, it/sec: 4.51902525315064
epoch 1 iter 57900: train loss 1.09200. lr 3.000000e-04, running loss 1.11496, it/sec: 4.487703312084764
epoch 1 iter 57920: train loss 1.11681. lr 3.000000e-04, running loss 1.11515, it/sec: 4.495272834783688
epoch 1 iter 57940: train loss 1.08443. lr 3.000000e-04, running loss 1.11505, it/sec: 4.456188462515991
n_blocks=12
writing to model-#.pth
device: cuda:0
save interval: 100000
batch size: 32
epoch 1 iter 0: train loss 0.75617. lr 3.000000e-04, running loss 0.75617, it/sec: 0.03404220265475945
epoch 1 iter 20: train loss 1.42166. lr 3.000000e-04, running loss 1.40336, it/sec: 4.3581438215555695
epoch 1 iter 40: train loss 1.42443. lr 3.000000e-04, running loss 1.41525, it/sec: 4.378876681861282
epoch 1 iter 60: train loss 1.41827. lr 3.000000e-04, running loss 1.42003, it/sec: 4.368321950293407
epoch 1 iter 80: train loss 1.45090. lr 3.000000e-04, running loss 1.42199, it/sec: 4.338999806649363
epoch 1 iter 100: train loss 1.42855. lr 3.000000e-04, running loss 1.42216, it/sec: 4.366068737395169
epoch 1 iter 120: train loss 1.43360. lr 3.000000e-04, running loss 1.42301, it/sec: 4.378122638583089
epoch 1 iter 140: train loss 1.41822. lr 3.000000e-04, running loss 1.42371, it/sec: 4.365169913046442
epoch 1 iter 160: train loss 1.42905. lr 3.000000e-04, running loss 1.42411, it/sec: 4.389133636616329
epoch 1 iter 180: train loss 1.43731. lr 3.000000e-04, running loss 1.42464, it/sec: 4.399737286356961
epoch 1 iter 200: train loss 1.41976. lr 3.000000e-04, running loss 1.42486, it/sec: 4.392193234620931
epoch 1 iter 220: train loss 1.43783. lr 3.000000e-04, running loss 1.42524, it/sec: 4.410395330117723
epoch 1 iter 240: train loss 1.41463. lr 3.000000e-04, running loss 1.42611, it/sec: 4.396554882291836
epoch 1 iter 260: train loss 1.42352. lr 3.000000e-04, running loss 1.42575, it/sec: 4.4150534722685
epoch 1 iter 280: train loss 1.43349. lr 3.000000e-04, running loss 1.42616, it/sec: 4.400338463088045
epoch 1 iter 300: train loss 1.42942. lr 3.000000e-04, running loss 1.42642, it/sec: 4.382530545547046
epoch 1 iter 320: train loss 1.40692. lr 3.000000e-04, running loss 1.42652, it/sec: 4.43323496021871
epoch 1 iter 340: train loss 1.43826. lr 3.000000e-04, running loss 1.42651, it/sec: 4.374501492158659
epoch 1 iter 360: train loss 1.42263. lr 3.000000e-04, running loss 1.42647, it/sec: 4.413167347091446
epoch 1 iter 380: train loss 1.41978. lr 3.000000e-04, running loss 1.42649, it/sec: 4.383767280670131
epoch 1 iter 400: train loss 1.42504. lr 3.000000e-04, running loss 1.42666, it/sec: 4.363445191099155
epoch 1 iter 420: train loss 1.44539. lr 3.000000e-04, running loss 1.42662, it/sec: 4.335933940227994
epoch 1 iter 440: train loss 1.42670. lr 3.000000e-04, running loss 1.42672, it/sec: 4.39064936984693
epoch 1 iter 460: train loss 1.42357. lr 3.000000e-04, running loss 1.42675, it/sec: 4.425368979926136
epoch 1 iter 480: train loss 1.42437. lr 3.000000e-04, running loss 1.42707, it/sec: 4.389850237879504
epoch 1 iter 500: train loss 1.42297. lr 3.000000e-04, running loss 1.42700, it/sec: 4.351245731754453
epoch 1 iter 520: train loss 1.41825. lr 3.000000e-04, running loss 1.42706, it/sec: 4.349023884128954
epoch 1 iter 540: train loss 1.43162. lr 3.000000e-04, running loss 1.42701, it/sec: 4.3216849289314485
epoch 1 iter 560: train loss 1.42624. lr 3.000000e-04, running loss 1.42699, it/sec: 4.315459800611137
epoch 1 iter 580: train loss 1.43085. lr 3.000000e-04, running loss 1.42716, it/sec: 4.376121881133036
epoch 1 iter 600: train loss 1.43182. lr 3.000000e-04, running loss 1.42722, it/sec: 4.366262039303791
epoch 1 iter 620: train loss 1.43061. lr 3.000000e-04, running loss 1.42720, it/sec: 4.394026099552596
epoch 1 iter 640: train loss 1.43516. lr 3.000000e-04, running loss 1.42763, it/sec: 4.3850946439248295
epoch 1 iter 660: train loss 1.42023. lr 3.000000e-04, running loss 1.42770, it/sec: 4.345359491424398
epoch 1 iter 680: train loss 1.42235. lr 3.000000e-04, running loss 1.42802, it/sec: 4.334480091427446
epoch 1 iter 700: train loss 1.43799. lr 3.000000e-04, running loss 1.42796, it/sec: 4.369502543480601
epoch 1 iter 720: train loss 1.44042. lr 3.000000e-04, running loss 1.42810, it/sec: 4.3963176229731085
epoch 1 iter 740: train loss 1.43713. lr 3.000000e-04, running loss 1.42818, it/sec: 4.38103833947833
epoch 1 iter 760: train loss 1.42685. lr 3.000000e-04, running loss 1.42831, it/sec: 4.359163551157888
epoch 1 iter 780: train loss 1.43339. lr 3.000000e-04, running loss 1.42831, it/sec: 4.344646920438807
epoch 1 iter 800: train loss 1.40462. lr 3.000000e-04, running loss 1.42839, it/sec: 4.331833991889108
epoch 1 iter 820: train loss 1.41113. lr 3.000000e-04, running loss 1.42830, it/sec: 4.409214254754113
epoch 1 iter 840: train loss 1.43192. lr 3.000000e-04, running loss 1.42823, it/sec: 4.387442441517083
epoch 1 iter 860: train loss 1.42450. lr 3.000000e-04, running loss 1.42825, it/sec: 4.4012644993749905
epoch 1 iter 880: train loss 1.42308. lr 3.000000e-04, running loss 1.42826, it/sec: 4.39791723682484
epoch 1 iter 900: train loss 1.61267. lr 3.000000e-04, running loss 1.42841, it/sec: 4.345696412601889
epoch 1 iter 920: train loss 1.42681. lr 3.000000e-04, running loss 1.42842, it/sec: 4.351334245937661
epoch 1 iter 940: train loss 1.42329. lr 3.000000e-04, running loss 1.42833, it/sec: 4.367723002247079
epoch 1 iter 960: train loss 1.42716. lr 3.000000e-04, running loss 1.42827, it/sec: 4.3972136020196615
epoch 1 iter 980: train loss 1.40954. lr 3.000000e-04, running loss 1.42824, it/sec: 4.3748156559156985
epoch 1 iter 1000: train loss 1.45694. lr 3.000000e-04, running loss 1.42828, it/sec: 4.338948145716792
epoch 1 iter 1020: train loss 1.41994. lr 3.000000e-04, running loss 1.42828, it/sec: 4.355121065427713
epoch 1 iter 1040: train loss 1.44168. lr 3.000000e-04, running loss 1.42833, it/sec: 4.393825524193879
epoch 1 iter 1060: train loss 1.43024. lr 3.000000e-04, running loss 1.42829, it/sec: 4.387699130961785
epoch 1 iter 1080: train loss 1.42611. lr 3.000000e-04, running loss 1.42830, it/sec: 4.348012012068388
epoch 1 iter 1100: train loss 1.42848. lr 3.000000e-04, running loss 1.42833, it/sec: 4.3355799403747275
epoch 1 iter 1120: train loss 1.42211. lr 3.000000e-04, running loss 1.42846, it/sec: 4.3741753587720025
epoch 1 iter 1140: train loss 1.43754. lr 3.000000e-04, running loss 1.42839, it/sec: 4.402154259429122
epoch 1 iter 1160: train loss 1.42838. lr 3.000000e-04, running loss 1.42839, it/sec: 4.386541434642965
epoch 1 iter 1180: train loss 1.42226. lr 3.000000e-04, running loss 1.42835, it/sec: 4.36400767576371
epoch 1 iter 1200: train loss 1.41803. lr 3.000000e-04, running loss 1.42835, it/sec: 4.349681945188527
epoch 1 iter 1220: train loss 1.43262. lr 3.000000e-04, running loss 1.42831, it/sec: 4.370526219645198
epoch 1 iter 1240: train loss 1.40863. lr 3.000000e-04, running loss 1.42840, it/sec: 4.399185061303851
epoch 1 iter 1260: train loss 1.41380. lr 3.000000e-04, running loss 1.42838, it/sec: 4.378287547388841
epoch 1 iter 1280: train loss 1.43137. lr 3.000000e-04, running loss 1.42835, it/sec: 4.345543239950537
epoch 1 iter 1300: train loss 1.42387. lr 3.000000e-04, running loss 1.42828, it/sec: 4.35297558184315
epoch 1 iter 1320: train loss 1.43436. lr 3.000000e-04, running loss 1.42827, it/sec: 4.399670503964636
epoch 1 iter 1340: train loss 1.42441. lr 3.000000e-04, running loss 1.42830, it/sec: 4.3841556977575475
epoch 1 iter 1360: train loss 1.42226. lr 3.000000e-04, running loss 1.42829, it/sec: 4.364141068177669
epoch 1 iter 1380: train loss 1.43388. lr 3.000000e-04, running loss 1.42829, it/sec: 4.31220846382671
epoch 1 iter 1400: train loss 1.43327. lr 3.000000e-04, running loss 1.42835, it/sec: 4.3803910183623636
epoch 1 iter 1420: train loss 1.43982. lr 3.000000e-04, running loss 1.42833, it/sec: 4.40705471823026
epoch 1 iter 1440: train loss 1.42501. lr 3.000000e-04, running loss 1.42829, it/sec: 4.382797937740822
epoch 1 iter 1460: train loss 1.41529. lr 3.000000e-04, running loss 1.42827, it/sec: 4.3639526374734166
epoch 1 iter 1480: train loss 1.42461. lr 3.000000e-04, running loss 1.42828, it/sec: 4.345232304549884
epoch 1 iter 1500: train loss 1.43524. lr 3.000000e-04, running loss 1.42816, it/sec: 4.314836647365213
epoch 1 iter 1520: train loss 2.03619. lr 3.000000e-04, running loss 1.42884, it/sec: 4.398061723190257
epoch 1 iter 1540: train loss 1.42385. lr 3.000000e-04, running loss 1.42879, it/sec: 4.393844984082158
epoch 1 iter 1560: train loss 1.45685. lr 3.000000e-04, running loss 1.42895, it/sec: 4.367170830780552
epoch 1 iter 1580: train loss 1.42175. lr 3.000000e-04, running loss 1.42897, it/sec: 4.343232873080517
epoch 1 iter 1600: train loss 1.41939. lr 3.000000e-04, running loss 1.42895, it/sec: 4.36867616379477
epoch 1 iter 1620: train loss 1.42424. lr 3.000000e-04, running loss 1.42917, it/sec: 4.404038254615562
epoch 1 iter 1640: train loss 1.43576. lr 3.000000e-04, running loss 1.42914, it/sec: 4.377861567396138
epoch 1 iter 1660: train loss 1.41623. lr 3.000000e-04, running loss 1.42937, it/sec: 4.326755333566053
epoch 1 iter 1680: train loss 1.42729. lr 3.000000e-04, running loss 1.42933, it/sec: 4.340438717281834
epoch 1 iter 1700: train loss 1.44028. lr 3.000000e-04, running loss 1.42938, it/sec: 4.388053549715299
epoch 1 iter 1720: train loss 1.42420. lr 3.000000e-04, running loss 1.42948, it/sec: 4.393066072557184
epoch 1 iter 1740: train loss 1.42167. lr 3.000000e-04, running loss 1.42941, it/sec: 4.344451694516255
epoch 1 iter 1760: train loss 1.42992. lr 3.000000e-04, running loss 1.42948, it/sec: 4.348531248277889
epoch 1 iter 1780: train loss 1.44003. lr 3.000000e-04, running loss 1.42957, it/sec: 4.31787354282028
epoch 1 iter 1800: train loss 1.40834. lr 3.000000e-04, running loss 1.42948, it/sec: 4.369332702864852
epoch 1 iter 1820: train loss 1.43244. lr 3.000000e-04, running loss 1.42938, it/sec: 4.3789191746743805
epoch 1 iter 1840: train loss 1.41303. lr 3.000000e-04, running loss 1.42931, it/sec: 4.39634715593738
epoch 1 iter 1860: train loss 1.42295. lr 3.000000e-04, running loss 1.42931, it/sec: 4.356560118593833
epoch 1 iter 1880: train loss 1.42425. lr 3.000000e-04, running loss 1.42925, it/sec: 4.339912558836577
epoch 1 iter 1900: train loss 1.40740. lr 3.000000e-04, running loss 1.42920, it/sec: 4.39120529597623
epoch 1 iter 1920: train loss 1.43434. lr 3.000000e-04, running loss 1.42908, it/sec: 4.396838177241987
epoch 1 iter 1940: train loss 1.43618. lr 3.000000e-04, running loss 1.42918, it/sec: 4.374485686299356
epoch 1 iter 1960: train loss 1.42940. lr 3.000000e-04, running loss 1.42908, it/sec: 4.3482452955045785
epoch 1 iter 1980: train loss 1.42871. lr 3.000000e-04, running loss 1.42910, it/sec: 4.370095924636115
epoch 1 iter 2000: train loss 1.45452. lr 3.000000e-04, running loss 1.42911, it/sec: 4.394554010390547
epoch 1 iter 2020: train loss 1.43907. lr 3.000000e-04, running loss 1.42923, it/sec: 4.376005373302762
epoch 1 iter 2040: train loss 1.42426. lr 3.000000e-04, running loss 1.42920, it/sec: 4.363363569739476
epoch 1 iter 2060: train loss 1.42288. lr 3.000000e-04, running loss 1.42922, it/sec: 4.34502739837702
epoch 1 iter 2080: train loss 1.42949. lr 3.000000e-04, running loss 1.42918, it/sec: 4.34300086164327
epoch 1 iter 2100: train loss 1.43339. lr 3.000000e-04, running loss 1.42916, it/sec: 4.389966077408268
epoch 1 iter 2120: train loss 1.43161. lr 3.000000e-04, running loss 1.42943, it/sec: 4.386715848386719
epoch 1 iter 2140: train loss 1.42365. lr 3.000000e-04, running loss 1.42939, it/sec: 4.387086929475088
epoch 1 iter 2160: train loss 1.45585. lr 3.000000e-04, running loss 1.42941, it/sec: 4.327087298552079
epoch 1 iter 2180: train loss 1.42973. lr 3.000000e-04, running loss 1.42938, it/sec: 4.376346987926994
epoch 1 iter 2200: train loss 1.42305. lr 3.000000e-04, running loss 1.42947, it/sec: 4.400129876431252
epoch 1 iter 2220: train loss 1.44075. lr 3.000000e-04, running loss 1.42942, it/sec: 4.373968689382424
epoch 1 iter 2240: train loss 1.41375. lr 3.000000e-04, running loss 1.42945, it/sec: 4.353973405967178
epoch 1 iter 2260: train loss 1.41927. lr 3.000000e-04, running loss 1.42946, it/sec: 4.341959721447333
epoch 1 iter 2280: train loss 1.42511. lr 3.000000e-04, running loss 1.42940, it/sec: 4.368117550670848
epoch 1 iter 2300: train loss 1.45150. lr 3.000000e-04, running loss 1.42933, it/sec: 4.378480130592717
epoch 1 iter 2320: train loss 1.41230. lr 3.000000e-04, running loss 1.42928, it/sec: 4.408824727245699
epoch 1 iter 2340: train loss 1.42340. lr 3.000000e-04, running loss 1.42925, it/sec: 4.38868246990038
epoch 1 iter 2360: train loss 1.43184. lr 3.000000e-04, running loss 1.42972, it/sec: 4.360366899922421
epoch 1 iter 2380: train loss 1.41812. lr 3.000000e-04, running loss 1.42982, it/sec: 4.344151840700422
epoch 1 iter 2400: train loss 1.43746. lr 3.000000e-04, running loss 1.42977, it/sec: 4.37204330993841
epoch 1 iter 2420: train loss 1.41752. lr 3.000000e-04, running loss 1.42964, it/sec: 4.405383960740047
epoch 1 iter 2440: train loss 1.43683. lr 3.000000e-04, running loss 1.42958, it/sec: 4.392288980081968
epoch 1 iter 2460: train loss 1.43713. lr 3.000000e-04, running loss 1.43267, it/sec: 4.353778478548286
epoch 1 iter 2480: train loss 1.43300. lr 3.000000e-04, running loss 1.43257, it/sec: 4.354107587116448
epoch 1 iter 2500: train loss 1.40777. lr 3.000000e-04, running loss 1.43255, it/sec: 4.3877496295139125
epoch 1 iter 2520: train loss 1.41672. lr 3.000000e-04, running loss 1.43240, it/sec: 4.398599716163675
epoch 1 iter 2540: train loss 1.43078. lr 3.000000e-04, running loss 1.43233, it/sec: 4.345473447621565
epoch 1 iter 2560: train loss 1.42934. lr 3.000000e-04, running loss 1.43223, it/sec: 4.345333245415629
epoch 1 iter 2580: train loss 1.41936. lr 3.000000e-04, running loss 1.43214, it/sec: 4.364381305503146
epoch 1 iter 2600: train loss 1.42631. lr 3.000000e-04, running loss 1.43207, it/sec: 4.396513556423165
epoch 1 iter 2620: train loss 1.42206. lr 3.000000e-04, running loss 1.43204, it/sec: 4.385490260552106
epoch 1 iter 2640: train loss 1.42500. lr 3.000000e-04, running loss 1.43190, it/sec: 4.379029393973058
epoch 1 iter 2660: train loss 1.41494. lr 3.000000e-04, running loss 1.43176, it/sec: 4.382901590870485
epoch 1 iter 2680: train loss 1.41395. lr 3.000000e-04, running loss 1.43164, it/sec: 4.357907879672481
epoch 1 iter 2700: train loss 1.40618. lr 3.000000e-04, running loss 1.43159, it/sec: 4.33967832259807
epoch 1 iter 2720: train loss 1.43453. lr 3.000000e-04, running loss 1.43143, it/sec: 4.385545766206552
epoch 1 iter 2740: train loss 1.44750. lr 3.000000e-04, running loss 1.43136, it/sec: 4.382652376332876
epoch 1 iter 2760: train loss 1.42302. lr 3.000000e-04, running loss 1.43130, it/sec: 4.3463216363375485
epoch 1 iter 2780: train loss 1.43486. lr 3.000000e-04, running loss 1.43129, it/sec: 4.349274697175823
epoch 1 iter 2800: train loss 1.43565. lr 3.000000e-04, running loss 1.43112, it/sec: 4.394056355845944
epoch 1 iter 2820: train loss 1.42003. lr 3.000000e-04, running loss 1.43094, it/sec: 4.404245118688809
epoch 1 iter 2840: train loss 1.42009. lr 3.000000e-04, running loss 1.43089, it/sec: 4.346636506339036
epoch 1 iter 2860: train loss 1.41581. lr 3.000000e-04, running loss 1.43086, it/sec: 4.365662952613988
epoch 1 iter 2880: train loss 1.43941. lr 3.000000e-04, running loss 1.43085, it/sec: 4.33306765877541
epoch 1 iter 2900: train loss 1.42067. lr 3.000000e-04, running loss 1.43079, it/sec: 4.364024434727701
epoch 1 iter 2920: train loss 1.42083. lr 3.000000e-04, running loss 1.43080, it/sec: 4.393526479180788
epoch 1 iter 2940: train loss 1.42400. lr 3.000000e-04, running loss 1.43072, it/sec: 4.388647742652558
epoch 1 iter 2960: train loss 1.45003. lr 3.000000e-04, running loss 1.43063, it/sec: 4.400922372622504
epoch 1 iter 2980: train loss 1.42584. lr 3.000000e-04, running loss 1.43065, it/sec: 4.345674750767654
epoch 1 iter 3000: train loss 1.42681. lr 3.000000e-04, running loss 1.43056, it/sec: 4.337281402404445
epoch 1 iter 3020: train loss 1.43136. lr 3.000000e-04, running loss 1.43048, it/sec: 4.367025944745464
epoch 1 iter 3040: train loss 1.44951. lr 3.000000e-04, running loss 1.43041, it/sec: 4.401976250667384
epoch 1 iter 3060: train loss 1.42360. lr 3.000000e-04, running loss 1.43046, it/sec: 4.389514123825692
epoch 1 iter 3080: train loss 1.42077. lr 3.000000e-04, running loss 1.43045, it/sec: 4.345376447086158
epoch 1 iter 3100: train loss 1.46746. lr 3.000000e-04, running loss 1.43041, it/sec: 4.341477902601014
epoch 1 iter 3120: train loss 1.43923. lr 3.000000e-04, running loss 1.43034, it/sec: 4.3670581748540975
epoch 1 iter 3140: train loss 1.41452. lr 3.000000e-04, running loss 1.43027, it/sec: 4.369126853051313
epoch 1 iter 3160: train loss 1.38786. lr 3.000000e-04, running loss 1.43024, it/sec: 4.412213051054524
epoch 1 iter 3180: train loss 1.43708. lr 3.000000e-04, running loss 1.43011, it/sec: 4.387818266062752
epoch 1 iter 3200: train loss 1.42556. lr 3.000000e-04, running loss 1.43003, it/sec: 4.326817150647928
epoch 1 iter 3220: train loss 1.42278. lr 3.000000e-04, running loss 1.42999, it/sec: 4.35102997678275
epoch 1 iter 3240: train loss 1.43478. lr 3.000000e-04, running loss 1.42990, it/sec: 4.339556721923673
epoch 1 iter 3260: train loss 1.43364. lr 3.000000e-04, running loss 1.42984, it/sec: 4.391466417643097
epoch 1 iter 3280: train loss 1.41712. lr 3.000000e-04, running loss 1.42975, it/sec: 4.384664744411262
epoch 1 iter 3300: train loss 1.41971. lr 3.000000e-04, running loss 1.42973, it/sec: 4.354964024022266
epoch 1 iter 3320: train loss 1.44370. lr 3.000000e-04, running loss 1.42981, it/sec: 4.3113164536117345
epoch 1 iter 3340: train loss 1.43304. lr 3.000000e-04, running loss 1.42979, it/sec: 4.352219368669639
epoch 1 iter 3360: train loss 1.43411. lr 3.000000e-04, running loss 1.42976, it/sec: 4.40838893745109
epoch 1 iter 3380: train loss 1.42873. lr 3.000000e-04, running loss 1.42974, it/sec: 4.381373137724164
epoch 1 iter 3400: train loss 1.41928. lr 3.000000e-04, running loss 1.43006, it/sec: 4.328419804433207
epoch 1 iter 3420: train loss 1.44336. lr 3.000000e-04, running loss 1.43001, it/sec: 4.33533979997626
epoch 1 iter 3440: train loss 1.40285. lr 3.000000e-04, running loss 1.42994, it/sec: 4.395492122829117
epoch 1 iter 3460: train loss 1.43174. lr 3.000000e-04, running loss 1.42989, it/sec: 4.389130533867591
epoch 1 iter 3480: train loss 1.42929. lr 3.000000e-04, running loss 1.42985, it/sec: 4.3702638957573985
epoch 1 iter 3500: train loss 1.42516. lr 3.000000e-04, running loss 1.42978, it/sec: 4.406311674550379
epoch 1 iter 3520: train loss 1.42567. lr 3.000000e-04, running loss 1.43056, it/sec: 4.393635621499669
epoch 1 iter 3540: train loss 1.43816. lr 3.000000e-04, running loss 1.43046, it/sec: 4.346713838657435
epoch 1 iter 3560: train loss 1.42301. lr 3.000000e-04, running loss 1.43044, it/sec: 4.353881692649761
epoch 1 iter 3580: train loss 1.45071. lr 3.000000e-04, running loss 1.43042, it/sec: 4.3899062398321345
epoch 1 iter 3600: train loss 1.43016. lr 3.000000e-04, running loss 1.43064, it/sec: 4.397120909976349
epoch 1 iter 3620: train loss 1.43199. lr 3.000000e-04, running loss 1.43069, it/sec: 4.337163848248129
epoch 1 iter 3640: train loss 1.43252. lr 3.000000e-04, running loss 1.43062, it/sec: 4.34994697066628
epoch 1 iter 3660: train loss 1.41589. lr 3.000000e-04, running loss 1.43062, it/sec: 4.3822991961574616
epoch 1 iter 3680: train loss 1.43584. lr 3.000000e-04, running loss 1.43056, it/sec: 4.376641802231788
epoch 1 iter 3700: train loss 1.42435. lr 3.000000e-04, running loss 1.43070, it/sec: 4.348423655111949
epoch 1 iter 3720: train loss 1.43509. lr 3.000000e-04, running loss 1.43064, it/sec: 4.368848053161459
epoch 1 iter 3740: train loss 1.43158. lr 3.000000e-04, running loss 1.43057, it/sec: 4.354997556752987
epoch 1 iter 3760: train loss 1.43028. lr 3.000000e-04, running loss 1.43054, it/sec: 4.383778675886195
epoch 1 iter 3780: train loss 1.42238. lr 3.000000e-04, running loss 1.43050, it/sec: 4.398237015990252
epoch 1 iter 3800: train loss 1.41149. lr 3.000000e-04, running loss 1.43043, it/sec: 4.340407010482127
epoch 1 iter 3820: train loss 1.43897. lr 3.000000e-04, running loss 1.43033, it/sec: 4.358906236874103
epoch 1 iter 3840: train loss 1.39981. lr 3.000000e-04, running loss 1.43031, it/sec: 4.334108576376902
epoch 1 iter 3860: train loss 1.45295. lr 3.000000e-04, running loss 1.43037, it/sec: 4.392549150240476
epoch 1 iter 3880: train loss 1.42777. lr 3.000000e-04, running loss 1.43028, it/sec: 4.395879066170834
epoch 1 iter 3900: train loss 1.42122. lr 3.000000e-04, running loss 1.43027, it/sec: 4.352540285411839
epoch 1 iter 3920: train loss 1.43244. lr 3.000000e-04, running loss 1.43024, it/sec: 4.337515548606255
epoch 1 iter 3940: train loss 1.41320. lr 3.000000e-04, running loss 1.43017, it/sec: 4.38829324808808
epoch 1 iter 3960: train loss 1.43336. lr 3.000000e-04, running loss 1.43010, it/sec: 4.401106223244659
epoch 1 iter 3980: train loss 1.43370. lr 3.000000e-04, running loss 1.43007, it/sec: 4.364261631436374
epoch 1 iter 4000: train loss 1.43639. lr 3.000000e-04, running loss 1.42993, it/sec: 4.3433749020516625
epoch 1 iter 4020: train loss 1.43376. lr 3.000000e-04, running loss 1.42987, it/sec: 4.34387608717844
epoch 1 iter 4040: train loss 1.41842. lr 3.000000e-04, running loss 1.42978, it/sec: 4.372979827246927
epoch 1 iter 4060: train loss 1.41898. lr 3.000000e-04, running loss 1.42974, it/sec: 4.368645111789287
epoch 1 iter 4080: train loss 1.43382. lr 3.000000e-04, running loss 1.42973, it/sec: 4.378835130732463
epoch 1 iter 4100: train loss 1.42579. lr 3.000000e-04, running loss 1.42965, it/sec: 4.358912829447602
epoch 1 iter 4120: train loss 1.42478. lr 3.000000e-04, running loss 1.42962, it/sec: 4.34308144079226
epoch 1 iter 4140: train loss 1.44257. lr 3.000000e-04, running loss 1.42962, it/sec: 4.317584691398054
epoch 1 iter 4160: train loss 1.42978. lr 3.000000e-04, running loss 1.42985, it/sec: 4.392795863637052
epoch 1 iter 4180: train loss 1.45251. lr 3.000000e-04, running loss 1.42988, it/sec: 4.392738263774031
epoch 1 iter 4200: train loss 1.44191. lr 3.000000e-04, running loss 1.42987, it/sec: 4.318595765045058
epoch 1 iter 4220: train loss 1.41902. lr 3.000000e-04, running loss 1.42980, it/sec: 4.3181124043962384
epoch 1 iter 4240: train loss 1.42202. lr 3.000000e-04, running loss 1.42984, it/sec: 4.4011487201420865
epoch 1 iter 4260: train loss 1.44122. lr 3.000000e-04, running loss 1.42977, it/sec: 4.393004645193368
epoch 1 iter 4280: train loss 1.42705. lr 3.000000e-04, running loss 1.42977, it/sec: 4.344828740714699
epoch 1 iter 4300: train loss 1.41336. lr 3.000000e-04, running loss 1.42971, it/sec: 4.343583765099887
epoch 1 iter 4320: train loss 1.42241. lr 3.000000e-04, running loss 1.42971, it/sec: 4.3786336006975235
epoch 1 iter 4340: train loss 1.43080. lr 3.000000e-04, running loss 1.42967, it/sec: 4.363482832153213
epoch 1 iter 4360: train loss 1.41225. lr 3.000000e-04, running loss 1.42965, it/sec: 4.349898946520488
epoch 1 iter 4380: train loss 1.43529. lr 3.000000e-04, running loss 1.42955, it/sec: 4.324217139630586
epoch 1 iter 4400: train loss 1.43237. lr 3.000000e-04, running loss 1.42948, it/sec: 4.3298856392532725
epoch 1 iter 4420: train loss 1.42495. lr 3.000000e-04, running loss 1.42946, it/sec: 4.3263749600508925
epoch 1 iter 4440: train loss 1.42948. lr 3.000000e-04, running loss 1.42954, it/sec: 4.391875587763907
epoch 1 iter 4460: train loss 1.43880. lr 3.000000e-04, running loss 1.42957, it/sec: 4.384446221011081
epoch 1 iter 4480: train loss 1.42205. lr 3.000000e-04, running loss 1.42945, it/sec: 4.3613680970856
epoch 1 iter 4500: train loss 1.42631. lr 3.000000e-04, running loss 1.42953, it/sec: 4.357905827971783
epoch 1 iter 4520: train loss 1.43703. lr 3.000000e-04, running loss 1.42943, it/sec: 4.391623172682457
epoch 1 iter 4540: train loss 1.43456. lr 3.000000e-04, running loss 1.42945, it/sec: 4.402328773086559
epoch 1 iter 4560: train loss 1.43568. lr 3.000000e-04, running loss 1.42950, it/sec: 4.350960026835264
epoch 1 iter 4580: train loss 1.43375. lr 3.000000e-04, running loss 1.42949, it/sec: 4.351974464482412
epoch 1 iter 4600: train loss 1.42991. lr 3.000000e-04, running loss 1.42938, it/sec: 4.3976272265481935
epoch 1 iter 4620: train loss 1.42246. lr 3.000000e-04, running loss 1.42946, it/sec: 4.395135207297325
epoch 1 iter 4640: train loss 1.44283. lr 3.000000e-04, running loss 1.42940, it/sec: 4.362486298041648
epoch 1 iter 4660: train loss 1.41348. lr 3.000000e-04, running loss 1.42932, it/sec: 4.376810774254461
epoch 1 iter 4680: train loss 1.41830. lr 3.000000e-04, running loss 1.42928, it/sec: 4.331238347394248
epoch 1 iter 4700: train loss 1.44216. lr 3.000000e-04, running loss 1.42940, it/sec: 4.321318070242206
epoch 1 iter 4720: train loss 1.43829. lr 3.000000e-04, running loss 1.42942, it/sec: 4.382625428575593
epoch 1 iter 4740: train loss 1.41359. lr 3.000000e-04, running loss 1.42936, it/sec: 4.408086701144764
epoch 1 iter 4760: train loss 1.39499. lr 3.000000e-04, running loss 1.42942, it/sec: 4.37298514343016
epoch 1 iter 4780: train loss 1.43340. lr 3.000000e-04, running loss 1.42944, it/sec: 4.3297953329251015
epoch 1 iter 4800: train loss 1.42334. lr 3.000000e-04, running loss 1.42932, it/sec: 4.314327341640805
epoch 1 iter 4820: train loss 1.43345. lr 3.000000e-04, running loss 1.42933, it/sec: 4.370388520915075
epoch 1 iter 4840: train loss 1.44433. lr 3.000000e-04, running loss 1.42944, it/sec: 4.401014238515094
epoch 1 iter 4860: train loss 1.43356. lr 3.000000e-04, running loss 1.42944, it/sec: 4.353014406844914
epoch 1 iter 4880: train loss 1.44227. lr 3.000000e-04, running loss 1.42971, it/sec: 4.331468746640888
epoch 1 iter 4900: train loss 1.42806. lr 3.000000e-04, running loss 1.42958, it/sec: 4.384353719587336
epoch 1 iter 4920: train loss 1.42065. lr 3.000000e-04, running loss 1.42965, it/sec: 4.394836854163798
epoch 1 iter 4940: train loss 1.42867. lr 3.000000e-04, running loss 1.42966, it/sec: 4.364210090047084
epoch 1 iter 4960: train loss 1.43720. lr 3.000000e-04, running loss 1.42962, it/sec: 4.323939890911557
epoch 1 iter 4980: train loss 1.41894. lr 3.000000e-04, running loss 1.42957, it/sec: 4.339511658475807
epoch 1 iter 5000: train loss 1.42431. lr 3.000000e-04, running loss 1.42957, it/sec: 4.3310820300038095
epoch 1 iter 5020: train loss 1.43102. lr 3.000000e-04, running loss 1.42960, it/sec: 4.372734437571299
epoch 1 iter 5040: train loss 1.42335. lr 3.000000e-04, running loss 1.42951, it/sec: 4.408668355132612
epoch 1 iter 5060: train loss 1.42607. lr 3.000000e-04, running loss 1.42941, it/sec: 4.3564445925838475
epoch 1 iter 5080: train loss 1.42790. lr 3.000000e-04, running loss 1.42940, it/sec: 4.336723226337899
epoch 1 iter 5100: train loss 1.43644. lr 3.000000e-04, running loss 1.42941, it/sec: 4.375228906913353
epoch 1 iter 5120: train loss 1.41990. lr 3.000000e-04, running loss 1.42939, it/sec: 4.373559751431833
epoch 1 iter 5140: train loss 1.42485. lr 3.000000e-04, running loss 1.42932, it/sec: 4.345825968201551
epoch 1 iter 5160: train loss 1.42315. lr 3.000000e-04, running loss 1.42928, it/sec: 4.3308998200920685
epoch 1 iter 5180: train loss 1.43147. lr 3.000000e-04, running loss 1.42923, it/sec: 4.403896399617506
epoch 1 iter 5200: train loss 1.44053. lr 3.000000e-04, running loss 1.42926, it/sec: 4.375474175472459
epoch 1 iter 5220: train loss 1.41737. lr 3.000000e-04, running loss 1.42918, it/sec: 4.38663320067651
epoch 1 iter 5240: train loss 1.43294. lr 3.000000e-04, running loss 1.42919, it/sec: 4.332664042438997
epoch 1 iter 5260: train loss 1.41917. lr 3.000000e-04, running loss 1.42911, it/sec: 4.34212896830703
epoch 1 iter 5280: train loss 1.42589. lr 3.000000e-04, running loss 1.42912, it/sec: 4.353094902464275
epoch 1 iter 5300: train loss 1.44230. lr 3.000000e-04, running loss 1.42904, it/sec: 4.355879634769409
epoch 1 iter 5320: train loss 1.44099. lr 3.000000e-04, running loss 1.42911, it/sec: 4.4020059770108215
epoch 1 iter 5340: train loss 1.41493. lr 3.000000e-04, running loss 1.42900, it/sec: 4.378548918240351
epoch 1 iter 5360: train loss 1.41209. lr 3.000000e-04, running loss 1.42897, it/sec: 4.377418194708848
epoch 1 iter 5380: train loss 1.41909. lr 3.000000e-04, running loss 1.42891, it/sec: 4.343993815454932
epoch 1 iter 5400: train loss 1.40664. lr 3.000000e-04, running loss 1.42887, it/sec: 4.382696631836609
epoch 1 iter 5420: train loss 1.45283. lr 3.000000e-04, running loss 1.42884, it/sec: 4.425044126442022
epoch 1 iter 5440: train loss 1.44795. lr 3.000000e-04, running loss 1.42881, it/sec: 4.391601937427053
epoch 1 iter 5460: train loss 1.42615. lr 3.000000e-04, running loss 1.42882, it/sec: 4.358221734142544
epoch 1 iter 5480: train loss 1.42318. lr 3.000000e-04, running loss 1.42886, it/sec: 4.342774478046347
epoch 1 iter 5500: train loss 1.40633. lr 3.000000e-04, running loss 1.42880, it/sec: 4.39846742177536
epoch 1 iter 5520: train loss 1.43833. lr 3.000000e-04, running loss 1.42875, it/sec: 4.386010945626914
epoch 1 iter 5540: train loss 1.43549. lr 3.000000e-04, running loss 1.42867, it/sec: 4.37873132499625
epoch 1 iter 5560: train loss 1.42630. lr 3.000000e-04, running loss 1.42857, it/sec: 4.34729550946097
epoch 1 iter 5580: train loss 1.43102. lr 3.000000e-04, running loss 1.42859, it/sec: 4.31737144350455
epoch 1 iter 5600: train loss 1.43052. lr 3.000000e-04, running loss 1.42868, it/sec: 4.314663508395993
epoch 1 iter 5620: train loss 1.42146. lr 3.000000e-04, running loss 1.42868, it/sec: 4.404898498880414
epoch 1 iter 5640: train loss 1.43136. lr 3.000000e-04, running loss 1.42878, it/sec: 4.414991370615222
epoch 1 iter 5660: train loss 1.44164. lr 3.000000e-04, running loss 1.42887, it/sec: 4.3620355699810816
epoch 1 iter 5680: train loss 1.42778. lr 3.000000e-04, running loss 1.42893, it/sec: 4.3575576306159505
epoch 1 iter 5700: train loss 1.43252. lr 3.000000e-04, running loss 1.42891, it/sec: 4.39500105355987
epoch 1 iter 5720: train loss 1.43053. lr 3.000000e-04, running loss 1.42894, it/sec: 4.381611857591976
epoch 1 iter 5740: train loss 1.44752. lr 3.000000e-04, running loss 1.42889, it/sec: 4.393557269046731
epoch 1 iter 5760: train loss 1.41940. lr 3.000000e-04, running loss 1.42883, it/sec: 4.319858665056821
epoch 1 iter 5780: train loss 1.44212. lr 3.000000e-04, running loss 1.42882, it/sec: 4.351520244909598
epoch 1 iter 5800: train loss 1.42729. lr 3.000000e-04, running loss 1.42883, it/sec: 4.377896448793176
epoch 1 iter 5820: train loss 1.42484. lr 3.000000e-04, running loss 1.42880, it/sec: 4.378361657133185
epoch 1 iter 5840: train loss 1.41285. lr 3.000000e-04, running loss 1.42877, it/sec: 4.392648172808852
epoch 1 iter 5860: train loss 1.43731. lr 3.000000e-04, running loss 1.42878, it/sec: 4.378803417402233
epoch 1 iter 5880: train loss 1.43551. lr 3.000000e-04, running loss 1.42873, it/sec: 4.377024896179287
epoch 1 iter 5900: train loss 1.42763. lr 3.000000e-04, running loss 1.42875, it/sec: 4.340615004745127
epoch 1 iter 5920: train loss 1.42294. lr 3.000000e-04, running loss 1.42869, it/sec: 4.347036815894826
epoch 1 iter 5940: train loss 1.41244. lr 3.000000e-04, running loss 1.42863, it/sec: 4.4089888839076545
epoch 1 iter 5960: train loss 1.42127. lr 3.000000e-04, running loss 1.42899, it/sec: 4.396628762234499
epoch 1 iter 5980: train loss 1.40962. lr 3.000000e-04, running loss 1.42901, it/sec: 4.365813828038203
epoch 1 iter 6000: train loss 1.42441. lr 3.000000e-04, running loss 1.42898, it/sec: 4.332778686529123
epoch 1 iter 6020: train loss 1.42989. lr 3.000000e-04, running loss 1.42904, it/sec: 4.360106365112335
epoch 1 iter 6040: train loss 1.42303. lr 3.000000e-04, running loss 1.42899, it/sec: 4.374731370604954
epoch 1 iter 6060: train loss 1.43372. lr 3.000000e-04, running loss 1.42894, it/sec: 4.370784029228224
epoch 1 iter 6080: train loss 1.41955. lr 3.000000e-04, running loss 1.42895, it/sec: 4.350978863148936
epoch 1 iter 6100: train loss 1.43299. lr 3.000000e-04, running loss 1.42896, it/sec: 4.379302015493815
epoch 1 iter 6120: train loss 1.41211. lr 3.000000e-04, running loss 1.42894, it/sec: 4.399928704016343
epoch 1 iter 6140: train loss 1.41655. lr 3.000000e-04, running loss 1.42884, it/sec: 4.358155806990612
epoch 1 iter 6160: train loss 1.42988. lr 3.000000e-04, running loss 1.42881, it/sec: 4.370928190577641
epoch 1 iter 6180: train loss 1.43294. lr 3.000000e-04, running loss 1.42879, it/sec: 4.353153798068926
epoch 1 iter 6200: train loss 1.42245. lr 3.000000e-04, running loss 1.42876, it/sec: 4.329589385732918
epoch 1 iter 6220: train loss 1.42329. lr 3.000000e-04, running loss 1.42883, it/sec: 4.399243874636894
epoch 1 iter 6240: train loss 1.41338. lr 3.000000e-04, running loss 1.42891, it/sec: 4.353621154426339
epoch 1 iter 6260: train loss 1.43137. lr 3.000000e-04, running loss 1.42891, it/sec: 4.37240244937172
epoch 1 iter 6280: train loss 1.42173. lr 3.000000e-04, running loss 1.42892, it/sec: 4.343196485326981
epoch 1 iter 6300: train loss 1.42900. lr 3.000000e-04, running loss 1.42888, it/sec: 4.38624007109076
epoch 1 iter 6320: train loss 1.43971. lr 3.000000e-04, running loss 1.42890, it/sec: 4.379077123645594
epoch 1 iter 6340: train loss 1.43045. lr 3.000000e-04, running loss 1.42891, it/sec: 4.339235099610346
epoch 1 iter 6360: train loss 1.41408. lr 3.000000e-04, running loss 1.42878, it/sec: 4.373007384495185
epoch 1 iter 6380: train loss 1.41055. lr 3.000000e-04, running loss 1.42882, it/sec: 4.383676767598744
epoch 1 iter 6400: train loss 1.43476. lr 3.000000e-04, running loss 1.42879, it/sec: 4.401435630624174
epoch 1 iter 6420: train loss 1.44221. lr 3.000000e-04, running loss 1.42892, it/sec: 4.366989939811956
epoch 1 iter 6440: train loss 1.40503. lr 3.000000e-04, running loss 1.42880, it/sec: 4.355037118863518
epoch 1 iter 6460: train loss 1.43326. lr 3.000000e-04, running loss 1.42878, it/sec: 4.3483612950520545
epoch 1 iter 6480: train loss 1.44558. lr 3.000000e-04, running loss 1.42878, it/sec: 4.3180373182400436
epoch 1 iter 6500: train loss 1.43138. lr 3.000000e-04, running loss 1.42878, it/sec: 4.3744955217854145
epoch 1 iter 6520: train loss 1.42292. lr 3.000000e-04, running loss 1.42874, it/sec: 4.3968072265970175
epoch 1 iter 6540: train loss 1.39215. lr 3.000000e-04, running loss 1.42868, it/sec: 4.341733652659105
epoch 1 iter 6560: train loss 1.41886. lr 3.000000e-04, running loss 1.42887, it/sec: 4.341280491965899
epoch 1 iter 6580: train loss 1.42093. lr 3.000000e-04, running loss 1.42885, it/sec: 4.401674274979299
epoch 1 iter 6600: train loss 1.41640. lr 3.000000e-04, running loss 1.42871, it/sec: 4.364209785170472
epoch 1 iter 6620: train loss 1.42129. lr 3.000000e-04, running loss 1.42867, it/sec: 4.320988484176439
epoch 1 iter 6640: train loss 1.43550. lr 3.000000e-04, running loss 1.42864, it/sec: 4.3511016535968805
epoch 1 iter 6660: train loss 1.41950. lr 3.000000e-04, running loss 1.42868, it/sec: 4.393340232333103
epoch 1 iter 6680: train loss 1.42633. lr 3.000000e-04, running loss 1.42867, it/sec: 4.390250818168249
epoch 1 iter 6700: train loss 1.43036. lr 3.000000e-04, running loss 1.42865, it/sec: 4.324745166667143
epoch 1 iter 6720: train loss 1.44429. lr 3.000000e-04, running loss 1.42865, it/sec: 4.364987775989862
epoch 1 iter 6740: train loss 1.43788. lr 3.000000e-04, running loss 1.42873, it/sec: 4.336380549826769
epoch 1 iter 6760: train loss 1.42489. lr 3.000000e-04, running loss 1.42871, it/sec: 4.379065674724679
epoch 1 iter 6780: train loss 1.42557. lr 3.000000e-04, running loss 1.42869, it/sec: 4.40110521528937
epoch 1 iter 6800: train loss 1.43246. lr 3.000000e-04, running loss 1.42869, it/sec: 4.2858414708993795
epoch 1 iter 6820: train loss 1.41865. lr 3.000000e-04, running loss 1.42873, it/sec: 4.362391067634309
epoch 1 iter 6840: train loss 1.43278. lr 3.000000e-04, running loss 1.42864, it/sec: 4.384640539301621
epoch 1 iter 6860: train loss 1.42256. lr 3.000000e-04, running loss 1.42861, it/sec: 4.392061807012373
epoch 1 iter 6880: train loss 1.41438. lr 3.000000e-04, running loss 1.42856, it/sec: 4.34770858583187
epoch 1 iter 6900: train loss 1.41839. lr 3.000000e-04, running loss 1.42851, it/sec: 4.338594933271364
epoch 1 iter 6920: train loss 1.42872. lr 3.000000e-04, running loss 1.42850, it/sec: 4.373251218438877
epoch 1 iter 6940: train loss 1.41917. lr 3.000000e-04, running loss 1.42846, it/sec: 4.399088571146317
epoch 1 iter 6960: train loss 1.42156. lr 3.000000e-04, running loss 1.42840, it/sec: 4.381351829404133
epoch 1 iter 6980: train loss 1.43058. lr 3.000000e-04, running loss 1.42836, it/sec: 4.3484277395772315
epoch 1 iter 7000: train loss 1.42550. lr 3.000000e-04, running loss 1.42830, it/sec: 4.337362821793099
epoch 1 iter 7020: train loss 1.42164. lr 3.000000e-04, running loss 1.42827, it/sec: 4.365094400345587
epoch 1 iter 7040: train loss 1.41235. lr 3.000000e-04, running loss 1.42824, it/sec: 4.364729260254602
epoch 1 iter 7060: train loss 1.43740. lr 3.000000e-04, running loss 1.42816, it/sec: 4.38486929105624
epoch 1 iter 7080: train loss 1.44229. lr 3.000000e-04, running loss 1.42811, it/sec: 4.40307262597416
epoch 1 iter 7100: train loss 1.43476. lr 3.000000e-04, running loss 1.42806, it/sec: 4.339020290765601
epoch 1 iter 7120: train loss 1.42218. lr 3.000000e-04, running loss 1.42807, it/sec: 4.348305269488154
epoch 1 iter 7140: train loss 1.42278. lr 3.000000e-04, running loss 1.42805, it/sec: 4.3628027157714095
epoch 1 iter 7160: train loss 1.43385. lr 3.000000e-04, running loss 1.42837, it/sec: 4.383809001521571
epoch 1 iter 7180: train loss 1.42942. lr 3.000000e-04, running loss 1.42834, it/sec: 4.3256540067653
epoch 1 iter 7200: train loss 1.42061. lr 3.000000e-04, running loss 1.42835, it/sec: 4.34743623547172
epoch 1 iter 7220: train loss 1.42218. lr 3.000000e-04, running loss 1.42833, it/sec: 4.392743087853424
epoch 1 iter 7240: train loss 1.44019. lr 3.000000e-04, running loss 1.42834, it/sec: 4.398397774891483
epoch 1 iter 7260: train loss 1.43453. lr 3.000000e-04, running loss 1.42836, it/sec: 4.350207201490945
epoch 1 iter 7280: train loss 1.42819. lr 3.000000e-04, running loss 1.42832, it/sec: 4.314702602238778
epoch 1 iter 7300: train loss 1.42643. lr 3.000000e-04, running loss 1.42834, it/sec: 4.382445366851744
epoch 1 iter 7320: train loss 1.43301. lr 3.000000e-04, running loss 1.42822, it/sec: 4.385373329476405
epoch 1 iter 7340: train loss 1.44617. lr 3.000000e-04, running loss 1.42833, it/sec: 4.3968398212786175
epoch 1 iter 7360: train loss 1.44130. lr 3.000000e-04, running loss 1.42841, it/sec: 4.362517814231462
epoch 1 iter 7380: train loss 1.43529. lr 3.000000e-04, running loss 1.42841, it/sec: 4.333313931726742
epoch 1 iter 7400: train loss 1.41576. lr 3.000000e-04, running loss 1.42837, it/sec: 4.351801779368736
epoch 1 iter 7420: train loss 1.42669. lr 3.000000e-04, running loss 1.42830, it/sec: 4.40221540035414
epoch 1 iter 7440: train loss 1.42941. lr 3.000000e-04, running loss 1.42827, it/sec: 4.386648653132151
epoch 1 iter 7460: train loss 1.42254. lr 3.000000e-04, running loss 1.42828, it/sec: 4.34551916477314
epoch 1 iter 7480: train loss 1.44860. lr 3.000000e-04, running loss 1.42877, it/sec: 4.379690678921143
epoch 1 iter 7500: train loss 1.65948. lr 3.000000e-04, running loss 1.43036, it/sec: 4.393962386479433
epoch 1 iter 7520: train loss 1.42090. lr 3.000000e-04, running loss 1.43034, it/sec: 4.375842149937694
epoch 1 iter 7540: train loss 1.40518. lr 3.000000e-04, running loss 1.43029, it/sec: 4.333348163687156
epoch 1 iter 7560: train loss 1.44328. lr 3.000000e-04, running loss 1.43018, it/sec: 4.360297409714586
epoch 1 iter 7580: train loss 1.44150. lr 3.000000e-04, running loss 1.43009, it/sec: 4.381958035371659
epoch 1 iter 7600: train loss 1.42530. lr 3.000000e-04, running loss 1.43002, it/sec: 4.404048495524153
epoch 1 iter 7620: train loss 1.41437. lr 3.000000e-04, running loss 1.42992, it/sec: 4.377552906818039
epoch 1 iter 7640: train loss 1.43799. lr 3.000000e-04, running loss 1.42993, it/sec: 4.344882108726907
epoch 1 iter 7660: train loss 1.45071. lr 3.000000e-04, running loss 1.42996, it/sec: 4.317985781928252
epoch 1 iter 7680: train loss 1.44356. lr 3.000000e-04, running loss 1.42984, it/sec: 4.374752900876747
epoch 1 iter 7700: train loss 1.43071. lr 3.000000e-04, running loss 1.42973, it/sec: 4.394422961896105
epoch 1 iter 7720: train loss 1.42654. lr 3.000000e-04, running loss 1.42970, it/sec: 4.338811753484533
epoch 1 iter 7740: train loss 1.43376. lr 3.000000e-04, running loss 1.43023, it/sec: 4.356983102300681
epoch 1 iter 7760: train loss 1.42860. lr 3.000000e-04, running loss 1.43011, it/sec: 4.383076060818777
epoch 1 iter 7780: train loss 1.41379. lr 3.000000e-04, running loss 1.43002, it/sec: 4.355331573146731
epoch 1 iter 7800: train loss 1.44502. lr 3.000000e-04, running loss 1.43003, it/sec: 4.323620747860392
epoch 1 iter 7820: train loss 1.42366. lr 3.000000e-04, running loss 1.42997, it/sec: 4.406827529974865
epoch 1 iter 7840: train loss 1.42086. lr 3.000000e-04, running loss 1.43072, it/sec: 4.405632952965141
epoch 1 iter 7860: train loss 1.44159. lr 3.000000e-04, running loss 1.43073, it/sec: 4.385219637297019
epoch 1 iter 7880: train loss 1.43034. lr 3.000000e-04, running loss 1.43066, it/sec: 4.305348432866611
epoch 1 iter 7900: train loss 1.43909. lr 3.000000e-04, running loss 1.43058, it/sec: 4.364171960845542
epoch 1 iter 7920: train loss 1.42278. lr 3.000000e-04, running loss 1.43053, it/sec: 4.363431425523123
epoch 1 iter 7940: train loss 1.42617. lr 3.000000e-04, running loss 1.43051, it/sec: 4.386768267737453
epoch 1 iter 7960: train loss 1.42351. lr 3.000000e-04, running loss 1.43037, it/sec: 4.373036930142555
epoch 1 iter 7980: train loss 1.42908. lr 3.000000e-04, running loss 1.43031, it/sec: 4.340434101706232
epoch 1 iter 8000: train loss 1.41515. lr 3.000000e-04, running loss 1.43033, it/sec: 4.3728577700775695
epoch 1 iter 8020: train loss 1.42498. lr 3.000000e-04, running loss 1.43026, it/sec: 4.374429445545024
epoch 1 iter 8040: train loss 1.41839. lr 3.000000e-04, running loss 1.43018, it/sec: 4.330204826002187
epoch 1 iter 8060: train loss 1.41489. lr 3.000000e-04, running loss 1.43015, it/sec: 4.315135557805766
epoch 1 iter 8080: train loss 1.42722. lr 3.000000e-04, running loss 1.43018, it/sec: 4.3869966075025415
epoch 1 iter 8100: train loss 1.43517. lr 3.000000e-04, running loss 1.43010, it/sec: 4.384665052152473
epoch 1 iter 8120: train loss 1.42842. lr 3.000000e-04, running loss 1.43010, it/sec: 4.376996753002653
epoch 1 iter 8140: train loss 1.44068. lr 3.000000e-04, running loss 1.43005, it/sec: 4.341994656393361
epoch 1 iter 8160: train loss 1.41188. lr 3.000000e-04, running loss 1.42998, it/sec: 4.302787427128948
epoch 1 iter 8180: train loss 1.43549. lr 3.000000e-04, running loss 1.42992, it/sec: 4.370953333244592
epoch 1 iter 8200: train loss 1.42819. lr 3.000000e-04, running loss 1.42990, it/sec: 4.4091131825847185
epoch 1 iter 8220: train loss 1.43980. lr 3.000000e-04, running loss 1.42977, it/sec: 4.398916537685039
epoch 1 iter 8240: train loss 1.43360. lr 3.000000e-04, running loss 1.42976, it/sec: 4.3596371021000095
epoch 1 iter 8260: train loss 1.43197. lr 3.000000e-04, running loss 1.42984, it/sec: 4.313045705265067
epoch 1 iter 8280: train loss 1.44678. lr 3.000000e-04, running loss 1.42977, it/sec: 4.378934072801694
epoch 1 iter 8300: train loss 1.41385. lr 3.000000e-04, running loss 1.42967, it/sec: 4.396522061185193
epoch 1 iter 8320: train loss 1.44309. lr 3.000000e-04, running loss 1.42962, it/sec: 4.377878892560805
epoch 1 iter 8340: train loss 1.41627. lr 3.000000e-04, running loss 1.42960, it/sec: 4.341941227607199
epoch 1 iter 8360: train loss 1.42715. lr 3.000000e-04, running loss 1.42988, it/sec: 4.297516758064692
epoch 1 iter 8380: train loss 1.43988. lr 3.000000e-04, running loss 1.42992, it/sec: 4.369085028966062
epoch 1 iter 8400: train loss 1.42779. lr 3.000000e-04, running loss 1.42984, it/sec: 4.40424816380843
epoch 1 iter 8420: train loss 1.42369. lr 3.000000e-04, running loss 1.42983, it/sec: 4.335172190106191
epoch 1 iter 8440: train loss 1.43046. lr 3.000000e-04, running loss 1.42981, it/sec: 4.340774573081096
epoch 1 iter 8460: train loss 1.43471. lr 3.000000e-04, running loss 1.42977, it/sec: 4.3848259921757675
epoch 1 iter 8480: train loss 1.42854. lr 3.000000e-04, running loss 1.42974, it/sec: 4.393933735135255
epoch 1 iter 8500: train loss 1.44032. lr 3.000000e-04, running loss 1.42972, it/sec: 4.391293902259294
epoch 1 iter 8520: train loss 1.45237. lr 3.000000e-04, running loss 1.42960, it/sec: 4.3666711585939915
epoch 1 iter 8540: train loss 1.43342. lr 3.000000e-04, running loss 1.42967, it/sec: 4.329735735321767
epoch 1 iter 8560: train loss 1.42040. lr 3.000000e-04, running loss 1.42965, it/sec: 4.342727102836371
epoch 1 iter 8580: train loss 1.41656. lr 3.000000e-04, running loss 1.42967, it/sec: 4.386149766067899
epoch 1 iter 8600: train loss 1.42674. lr 3.000000e-04, running loss 1.42962, it/sec: 4.398852256965797
epoch 1 iter 8620: train loss 1.42400. lr 3.000000e-04, running loss 1.42949, it/sec: 4.346678034809037
epoch 1 iter 8640: train loss 1.42943. lr 3.000000e-04, running loss 1.42950, it/sec: 4.345252809088791
epoch 1 iter 8660: train loss 1.45783. lr 3.000000e-04, running loss 1.42957, it/sec: 4.375064955980052
epoch 1 iter 8680: train loss 1.42794. lr 3.000000e-04, running loss 1.42952, it/sec: 4.389821716412653
epoch 1 iter 8700: train loss 1.42686. lr 3.000000e-04, running loss 1.42957, it/sec: 4.341728356228676
epoch 1 iter 8720: train loss 1.43945. lr 3.000000e-04, running loss 1.42952, it/sec: 4.341304654037949
epoch 1 iter 8740: train loss 1.42552. lr 3.000000e-04, running loss 1.42949, it/sec: 4.379164953158765
epoch 1 iter 8760: train loss 1.42456. lr 3.000000e-04, running loss 1.42948, it/sec: 4.394238106915642
epoch 1 iter 8780: train loss 1.44856. lr 3.000000e-04, running loss 1.42950, it/sec: 4.299711455729774
epoch 1 iter 8800: train loss 1.40869. lr 3.000000e-04, running loss 1.42946, it/sec: 4.33218051106406
epoch 1 iter 8820: train loss 1.42562. lr 3.000000e-04, running loss 1.42945, it/sec: 4.322853955540099
epoch 1 iter 8840: train loss 1.42552. lr 3.000000e-04, running loss 1.42947, it/sec: 4.361242672776323
epoch 1 iter 8860: train loss 1.42971. lr 3.000000e-04, running loss 1.42966, it/sec: 4.381908840223483
epoch 1 iter 8880: train loss 1.42625. lr 3.000000e-04, running loss 1.42969, it/sec: 4.367833289388943
epoch 1 iter 8900: train loss 1.44027. lr 3.000000e-04, running loss 1.42982, it/sec: 4.360639161698984
epoch 1 iter 8920: train loss 1.43321. lr 3.000000e-04, running loss 1.42978, it/sec: 4.37222334017759
epoch 1 iter 8940: train loss 1.41732. lr 3.000000e-04, running loss 1.42971, it/sec: 4.384729919368363
epoch 1 iter 8960: train loss 1.48420. lr 3.000000e-04, running loss 1.42972, it/sec: 4.374138412833844
epoch 1 iter 8980: train loss 1.42843. lr 3.000000e-04, running loss 1.42976, it/sec: 4.316775277777433
epoch 1 iter 9000: train loss 1.42528. lr 3.000000e-04, running loss 1.42994, it/sec: 4.367907924064268
epoch 1 iter 9020: train loss 1.43714. lr 3.000000e-04, running loss 1.42998, it/sec: 4.379144778089551
epoch 1 iter 9040: train loss 1.42932. lr 3.000000e-04, running loss 1.42997, it/sec: 4.3508891888137615
epoch 1 iter 9060: train loss 1.43031. lr 3.000000e-04, running loss 1.42993, it/sec: 4.369432550638093
epoch 1 iter 9080: train loss 1.44220. lr 3.000000e-04, running loss 1.42987, it/sec: 4.308872233523448
epoch 1 iter 9100: train loss 1.43418. lr 3.000000e-04, running loss 1.42986, it/sec: 4.327877544754094
epoch 1 iter 9120: train loss 1.44143. lr 3.000000e-04, running loss 1.42979, it/sec: 4.3294699438325015
epoch 1 iter 9140: train loss 1.43463. lr 3.000000e-04, running loss 1.42964, it/sec: 4.40904848583798
epoch 1 iter 9160: train loss 1.41486. lr 3.000000e-04, running loss 1.42965, it/sec: 4.35631317033422
epoch 1 iter 9180: train loss 1.43225. lr 3.000000e-04, running loss 1.42973, it/sec: 4.3395927856329415
epoch 1 iter 9200: train loss 1.42444. lr 3.000000e-04, running loss 1.42972, it/sec: 4.41638975034416
epoch 1 iter 9220: train loss 1.41768. lr 3.000000e-04, running loss 1.42970, it/sec: 4.363998248764464
epoch 1 iter 9240: train loss 1.42422. lr 3.000000e-04, running loss 1.42963, it/sec: 4.357469355992137
epoch 1 iter 9260: train loss 1.43505. lr 3.000000e-04, running loss 1.42957, it/sec: 4.296995840058407
epoch 1 iter 9280: train loss 1.43405. lr 3.000000e-04, running loss 1.42970, it/sec: 4.3986381024518675
epoch 1 iter 9300: train loss 1.42363. lr 3.000000e-04, running loss 1.42957, it/sec: 4.404777562627369
epoch 1 iter 9320: train loss 1.42166. lr 3.000000e-04, running loss 1.42964, it/sec: 4.357801378397733
epoch 1 iter 9340: train loss 1.41500. lr 3.000000e-04, running loss 1.42962, it/sec: 4.324834252114739
epoch 1 iter 9360: train loss 1.43969. lr 3.000000e-04, running loss 1.42964, it/sec: 4.318059766863589
epoch 1 iter 9380: train loss 1.43242. lr 3.000000e-04, running loss 1.42956, it/sec: 4.335893933248646
epoch 1 iter 9400: train loss 1.42096. lr 3.000000e-04, running loss 1.42949, it/sec: 4.3440809979135
epoch 1 iter 9420: train loss 1.42192. lr 3.000000e-04, running loss 1.42935, it/sec: 4.341828681177839
epoch 1 iter 9440: train loss 1.43823. lr 3.000000e-04, running loss 1.42929, it/sec: 4.399004740183139
epoch 1 iter 9460: train loss 1.42826. lr 3.000000e-04, running loss 1.42923, it/sec: 4.381680551460682
epoch 1 iter 9480: train loss 1.43719. lr 3.000000e-04, running loss 1.42918, it/sec: 4.352630576998637
epoch 1 iter 9500: train loss 1.41078. lr 3.000000e-04, running loss 1.42947, it/sec: 4.345571150475016
epoch 1 iter 9520: train loss 1.41453. lr 3.000000e-04, running loss 1.42943, it/sec: 4.376457365413274
epoch 1 iter 9540: train loss 1.43120. lr 3.000000e-04, running loss 1.42945, it/sec: 4.371916277373441
epoch 1 iter 9560: train loss 1.43504. lr 3.000000e-04, running loss 1.42930, it/sec: 4.355444554591806
epoch 1 iter 9580: train loss 1.43304. lr 3.000000e-04, running loss 1.42922, it/sec: 4.352552220196726
epoch 1 iter 9600: train loss 1.41731. lr 3.000000e-04, running loss 1.42936, it/sec: 4.403423792438726
epoch 1 iter 9620: train loss 1.43274. lr 3.000000e-04, running loss 1.42940, it/sec: 4.3742519890029286
epoch 1 iter 9640: train loss 1.43646. lr 3.000000e-04, running loss 1.42927, it/sec: 4.379897198211584
epoch 1 iter 9660: train loss 1.40634. lr 3.000000e-04, running loss 1.42925, it/sec: 4.332910156953425
epoch 1 iter 9680: train loss 1.42584. lr 3.000000e-04, running loss 1.42911, it/sec: 4.339409594777888
epoch 1 iter 9700: train loss 1.40647. lr 3.000000e-04, running loss 1.42901, it/sec: 4.356321368941824
epoch 1 iter 9720: train loss 1.42912. lr 3.000000e-04, running loss 1.42899, it/sec: 4.399886868468846
epoch 1 iter 9740: train loss 1.44446. lr 3.000000e-04, running loss 1.42899, it/sec: 4.402205420647671
epoch 1 iter 9760: train loss 1.44331. lr 3.000000e-04, running loss 1.42901, it/sec: 4.3196475243768715
epoch 1 iter 9780: train loss 1.42750. lr 3.000000e-04, running loss 1.42900, it/sec: 4.352051834716189
epoch 1 iter 9800: train loss 1.41770. lr 3.000000e-04, running loss 1.42902, it/sec: 4.342243114010981
epoch 1 iter 9820: train loss 1.44192. lr 3.000000e-04, running loss 1.42901, it/sec: 4.344948276641317
epoch 1 iter 9840: train loss 1.42721. lr 3.000000e-04, running loss 1.42895, it/sec: 4.348473404443534
epoch 1 iter 9860: train loss 1.42280. lr 3.000000e-04, running loss 1.42887, it/sec: 4.383125339211007
epoch 1 iter 9880: train loss 1.41220. lr 3.000000e-04, running loss 1.42883, it/sec: 4.3528848204462465
epoch 1 iter 9900: train loss 1.43637. lr 3.000000e-04, running loss 1.42878, it/sec: 4.386830676827062
epoch 1 iter 9920: train loss 1.42382. lr 3.000000e-04, running loss 1.42872, it/sec: 4.334368400582935
epoch 1 iter 9940: train loss 1.41133. lr 3.000000e-04, running loss 1.42868, it/sec: 4.319023625939277
epoch 1 iter 9960: train loss 1.41870. lr 3.000000e-04, running loss 1.42874, it/sec: 4.338070539487689
epoch 1 iter 9980: train loss 1.40837. lr 3.000000e-04, running loss 1.42870, it/sec: 4.363312697848832
epoch 1 iter 10000: train loss 1.43560. lr 3.000000e-04, running loss 1.42866, it/sec: 4.381568085511722
epoch 1 iter 10020: train loss 1.41315. lr 3.000000e-04, running loss 1.42865, it/sec: 4.366085149355347
epoch 1 iter 10040: train loss 1.41841. lr 3.000000e-04, running loss 1.42858, it/sec: 4.3272392840392575
epoch 1 iter 10060: train loss 1.42325. lr 3.000000e-04, running loss 1.42859, it/sec: 4.348071658283076
epoch 1 iter 10080: train loss 1.41971. lr 3.000000e-04, running loss 1.42858, it/sec: 4.396643471685197
epoch 1 iter 10100: train loss 1.42791. lr 3.000000e-04, running loss 1.42856, it/sec: 4.334307588986903
epoch 1 iter 10120: train loss 1.41271. lr 3.000000e-04, running loss 1.42848, it/sec: 4.346954221190315
epoch 1 iter 10140: train loss 1.41696. lr 3.000000e-04, running loss 1.42906, it/sec: 4.331038116324025
epoch 1 iter 10160: train loss 1.41970. lr 3.000000e-04, running loss 1.42910, it/sec: 4.389216185696804
epoch 1 iter 10180: train loss 1.42420. lr 3.000000e-04, running loss 1.42911, it/sec: 4.383539527481015
epoch 1 iter 10200: train loss 1.42649. lr 3.000000e-04, running loss 1.42912, it/sec: 4.337717489619237
epoch 1 iter 10220: train loss 1.42890. lr 3.000000e-04, running loss 1.42916, it/sec: 4.31863082875452
epoch 1 iter 10240: train loss 1.45111. lr 3.000000e-04, running loss 1.42919, it/sec: 4.391970238128517
epoch 1 iter 10260: train loss 1.42845. lr 3.000000e-04, running loss 1.42915, it/sec: 4.400572265532982
epoch 1 iter 10280: train loss 1.43537. lr 3.000000e-04, running loss 1.42922, it/sec: 4.386706458557545
epoch 1 iter 10300: train loss 1.42445. lr 3.000000e-04, running loss 1.42923, it/sec: 4.392070467477581
epoch 1 iter 10320: train loss 1.42285. lr 3.000000e-04, running loss 1.42919, it/sec: 4.344147161202356
epoch 1 iter 10340: train loss 1.44131. lr 3.000000e-04, running loss 1.42920, it/sec: 4.335987803466995
epoch 1 iter 10360: train loss 1.40855. lr 3.000000e-04, running loss 1.42918, it/sec: 4.376176767541945
epoch 1 iter 10380: train loss 1.43465. lr 3.000000e-04, running loss 1.42909, it/sec: 4.393385957792874
epoch 1 iter 10400: train loss 1.42308. lr 3.000000e-04, running loss 1.42909, it/sec: 4.387758851231097
epoch 1 iter 10420: train loss 1.43207. lr 3.000000e-04, running loss 1.42906, it/sec: 4.347117223686585
epoch 1 iter 10440: train loss 1.46312. lr 3.000000e-04, running loss 1.42906, it/sec: 4.373453152891469
epoch 1 iter 10460: train loss 1.44059. lr 3.000000e-04, running loss 1.42908, it/sec: 4.400941974341194
epoch 1 iter 10480: train loss 1.41868. lr 3.000000e-04, running loss 1.42902, it/sec: 4.354035073006455
epoch 1 iter 10500: train loss 1.43701. lr 3.000000e-04, running loss 1.42893, it/sec: 4.336103281262412
epoch 1 iter 10520: train loss 1.43220. lr 3.000000e-04, running loss 1.42886, it/sec: 4.3625368648535545
epoch 1 iter 10540: train loss 1.40608. lr 3.000000e-04, running loss 1.42891, it/sec: 4.388336961840861
epoch 1 iter 10560: train loss 1.43872. lr 3.000000e-04, running loss 1.42883, it/sec: 4.418435745232803
epoch 1 iter 10580: train loss 1.42392. lr 3.000000e-04, running loss 1.42884, it/sec: 4.414412587675659
epoch 1 iter 10600: train loss 1.42242. lr 3.000000e-04, running loss 1.42877, it/sec: 4.39117637192058
epoch 1 iter 10620: train loss 1.42729. lr 3.000000e-04, running loss 1.42892, it/sec: 4.318064298075584
epoch 1 iter 10640: train loss 1.41874. lr 3.000000e-04, running loss 1.42890, it/sec: 4.389843358459259
epoch 1 iter 10660: train loss 1.42083. lr 3.000000e-04, running loss 1.42888, it/sec: 4.395177512499319
epoch 1 iter 10680: train loss 1.42323. lr 3.000000e-04, running loss 1.42881, it/sec: 4.36845373488851
epoch 1 iter 10700: train loss 1.43128. lr 3.000000e-04, running loss 1.42885, it/sec: 4.3263959995470795
epoch 1 iter 10720: train loss 1.43004. lr 3.000000e-04, running loss 1.42892, it/sec: 4.387898416662663
epoch 1 iter 10740: train loss 1.43980. lr 3.000000e-04, running loss 1.42895, it/sec: 4.395896554482301
epoch 1 iter 10760: train loss 1.42563. lr 3.000000e-04, running loss 1.42897, it/sec: 4.3356559387714695
epoch 1 iter 10780: train loss 1.42761. lr 3.000000e-04, running loss 1.42891, it/sec: 4.346972001026932
epoch 1 iter 10800: train loss 1.42194. lr 3.000000e-04, running loss 1.42892, it/sec: 4.383476347434622
epoch 1 iter 10820: train loss 1.43054. lr 3.000000e-04, running loss 1.42887, it/sec: 4.392679817440217
epoch 1 iter 10840: train loss 1.44476. lr 3.000000e-04, running loss 1.42882, it/sec: 4.394549955720659
epoch 1 iter 10860: train loss 1.40713. lr 3.000000e-04, running loss 1.42880, it/sec: 4.366009795830028
epoch 1 iter 10880: train loss 1.42008. lr 3.000000e-04, running loss 1.42876, it/sec: 4.360869675407609
epoch 1 iter 10900: train loss 1.43345. lr 3.000000e-04, running loss 1.42870, it/sec: 4.354775702760981
epoch 1 iter 10920: train loss 1.44662. lr 3.000000e-04, running loss 1.42867, it/sec: 4.404006426929846
epoch 1 iter 10940: train loss 1.42842. lr 3.000000e-04, running loss 1.42883, it/sec: 4.364672946294446
epoch 1 iter 10960: train loss 1.42301. lr 3.000000e-04, running loss 1.42884, it/sec: 4.346045474260123
epoch 1 iter 10980: train loss 1.44726. lr 3.000000e-04, running loss 1.42879, it/sec: 4.3352547905897945
epoch 1 iter 11000: train loss 1.41651. lr 3.000000e-04, running loss 1.42878, it/sec: 4.360673027602618
epoch 1 iter 11020: train loss 1.42267. lr 3.000000e-04, running loss 1.42874, it/sec: 4.395246921376059
epoch 1 iter 11040: train loss 1.44081. lr 3.000000e-04, running loss 1.42876, it/sec: 4.384379843822791
epoch 1 iter 11060: train loss 1.43780. lr 3.000000e-04, running loss 1.42874, it/sec: 4.3400377197116615
epoch 1 iter 11080: train loss 1.42852. lr 3.000000e-04, running loss 1.42869, it/sec: 4.303752110504209
epoch 1 iter 11100: train loss 1.42332. lr 3.000000e-04, running loss 1.42868, it/sec: 4.388231683993551
epoch 1 iter 11120: train loss 1.43022. lr 3.000000e-04, running loss 1.42867, it/sec: 4.398055456396954
epoch 1 iter 11140: train loss 1.43837. lr 3.000000e-04, running loss 1.42871, it/sec: 4.3788381028692696
epoch 1 iter 11160: train loss 1.42744. lr 3.000000e-04, running loss 1.42898, it/sec: 4.373062574614342
epoch 1 iter 11180: train loss 1.44227. lr 3.000000e-04, running loss 1.42905, it/sec: 4.352734534019012
epoch 1 iter 11200: train loss 1.42864. lr 3.000000e-04, running loss 1.42898, it/sec: 4.3395384370847605
epoch 1 iter 11220: train loss 1.44416. lr 3.000000e-04, running loss 1.42901, it/sec: 4.415064388515946
epoch 1 iter 11240: train loss 1.42864. lr 3.000000e-04, running loss 1.42897, it/sec: 4.410560091789394
epoch 1 iter 11260: train loss 1.43210. lr 3.000000e-04, running loss 1.42887, it/sec: 4.349678729472563
epoch 1 iter 11280: train loss 1.42161. lr 3.000000e-04, running loss 1.42882, it/sec: 4.3466747663460525
epoch 1 iter 11300: train loss 1.45293. lr 3.000000e-04, running loss 1.42890, it/sec: 4.361417972536072
epoch 1 iter 11320: train loss 1.42386. lr 3.000000e-04, running loss 1.42882, it/sec: 4.384489973407032
epoch 1 iter 11340: train loss 1.40381. lr 3.000000e-04, running loss 1.42876, it/sec: 4.3265760524427845
epoch 1 iter 11360: train loss 1.41598. lr 3.000000e-04, running loss 1.42870, it/sec: 4.33391673989669
epoch 1 iter 11380: train loss 1.42044. lr 3.000000e-04, running loss 1.42861, it/sec: 4.369593596875958
epoch 1 iter 11400: train loss 1.44097. lr 3.000000e-04, running loss 1.42858, it/sec: 4.3999893571693836
epoch 1 iter 11420: train loss 1.43838. lr 3.000000e-04, running loss 1.42857, it/sec: 4.369715568584032
epoch 1 iter 11440: train loss 1.41738. lr 3.000000e-04, running loss 1.42847, it/sec: 4.405196240741726
epoch 1 iter 11460: train loss 1.43690. lr 3.000000e-04, running loss 1.42851, it/sec: 4.353096741169887
epoch 1 iter 11480: train loss 1.40890. lr 3.000000e-04, running loss 1.42842, it/sec: 4.348281862555109
epoch 1 iter 11500: train loss 1.42099. lr 3.000000e-04, running loss 1.42845, it/sec: 4.392899392635765
epoch 1 iter 11520: train loss 1.43790. lr 3.000000e-04, running loss 1.42842, it/sec: 4.40887390518689
epoch 1 iter 11540: train loss 1.50975. lr 3.000000e-04, running loss 1.42846, it/sec: 4.341713992181281
epoch 1 iter 11560: train loss 1.43462. lr 3.000000e-04, running loss 1.42845, it/sec: 4.35301541164308
epoch 1 iter 11580: train loss 1.42538. lr 3.000000e-04, running loss 1.42848, it/sec: 4.3958113177118925
epoch 1 iter 11600: train loss 1.43434. lr 3.000000e-04, running loss 1.42853, it/sec: 4.372768147691238
epoch 1 iter 11620: train loss 1.40651. lr 3.000000e-04, running loss 1.42857, it/sec: 4.34115588137821
epoch 1 iter 11640: train loss 1.42779. lr 3.000000e-04, running loss 1.42862, it/sec: 4.320873698099052
epoch 1 iter 11660: train loss 1.42701. lr 3.000000e-04, running loss 1.42864, it/sec: 4.3789246381277875
epoch 1 iter 11680: train loss 1.43504. lr 3.000000e-04, running loss 1.42865, it/sec: 4.4076754978397386
epoch 1 iter 11700: train loss 1.43581. lr 3.000000e-04, running loss 1.42864, it/sec: 4.374239073995278
epoch 1 iter 11720: train loss 1.41535. lr 3.000000e-04, running loss 1.42858, it/sec: 4.369718241601714
epoch 1 iter 11740: train loss 1.41050. lr 3.000000e-04, running loss 1.42858, it/sec: 4.344051011832018
epoch 1 iter 11760: train loss 1.40846. lr 3.000000e-04, running loss 1.42847, it/sec: 4.343475586428361
epoch 1 iter 11780: train loss 1.41216. lr 3.000000e-04, running loss 1.42846, it/sec: 4.39012557596762
epoch 1 iter 11800: train loss 1.48023. lr 3.000000e-04, running loss 1.42846, it/sec: 4.404708666649519
epoch 1 iter 11820: train loss 1.43787. lr 3.000000e-04, running loss 1.42856, it/sec: 4.4022662539049895
epoch 1 iter 11840: train loss 1.44542. lr 3.000000e-04, running loss 1.42852, it/sec: 4.347867901497582
epoch 1 iter 11860: train loss 1.43885. lr 3.000000e-04, running loss 1.42847, it/sec: 4.334694788911809
epoch 1 iter 11880: train loss 1.43336. lr 3.000000e-04, running loss 1.42844, it/sec: 4.398378892867108
epoch 1 iter 11900: train loss 1.42493. lr 3.000000e-04, running loss 1.42844, it/sec: 4.399792785904064
epoch 1 iter 11920: train loss 1.42140. lr 3.000000e-04, running loss 1.42844, it/sec: 4.337127637665968
epoch 1 iter 11940: train loss 1.42201. lr 3.000000e-04, running loss 1.42839, it/sec: 4.346063908527665
epoch 1 iter 11960: train loss 1.41399. lr 3.000000e-04, running loss 1.42832, it/sec: 4.390635141465259
epoch 1 iter 11980: train loss 1.43205. lr 3.000000e-04, running loss 1.42873, it/sec: 4.410103205466594
epoch 1 iter 12000: train loss 1.42706. lr 3.000000e-04, running loss 1.42864, it/sec: 4.387426040033605
epoch 1 iter 12020: train loss 1.40378. lr 3.000000e-04, running loss 1.42862, it/sec: 4.376836979282689
epoch 1 iter 12040: train loss 1.43031. lr 3.000000e-04, running loss 1.42867, it/sec: 4.363037571349375
epoch 1 iter 12060: train loss 1.42061. lr 3.000000e-04, running loss 1.42863, it/sec: 4.34176228784144
epoch 1 iter 12080: train loss 1.41877. lr 3.000000e-04, running loss 1.42845, it/sec: 4.40236431847141
epoch 1 iter 12100: train loss 1.40996. lr 3.000000e-04, running loss 1.42835, it/sec: 4.404168324806155
epoch 1 iter 12120: train loss 1.41833. lr 3.000000e-04, running loss 1.42825, it/sec: 4.384363600616487
epoch 1 iter 12140: train loss 1.42994. lr 3.000000e-04, running loss 1.42822, it/sec: 4.2904014325612625
epoch 1 iter 12160: train loss 1.40834. lr 3.000000e-04, running loss 1.42825, it/sec: 4.3568662821000075
epoch 1 iter 12180: train loss 1.41993. lr 3.000000e-04, running loss 1.42824, it/sec: 4.391034206242105
epoch 1 iter 12200: train loss 1.42254. lr 3.000000e-04, running loss 1.42823, it/sec: 4.3757841712961465
epoch 1 iter 12220: train loss 1.41721. lr 3.000000e-04, running loss 1.42813, it/sec: 4.34307957211
epoch 1 iter 12240: train loss 1.42130. lr 3.000000e-04, running loss 1.42807, it/sec: 4.385952024257418
epoch 1 iter 12260: train loss 1.42603. lr 3.000000e-04, running loss 1.42805, it/sec: 4.3946049366473146
epoch 1 iter 12280: train loss 1.43369. lr 3.000000e-04, running loss 1.42809, it/sec: 4.385840030879041
epoch 1 iter 12300: train loss 1.43420. lr 3.000000e-04, running loss 1.42815, it/sec: 4.3499060229196385
epoch 1 iter 12320: train loss 1.42663. lr 3.000000e-04, running loss 1.42808, it/sec: 4.334533523755904
epoch 1 iter 12340: train loss 1.43274. lr 3.000000e-04, running loss 1.42813, it/sec: 4.32968519496771
epoch 1 iter 12360: train loss 1.42316. lr 3.000000e-04, running loss 1.42810, it/sec: 4.380405889649264
epoch 1 iter 12380: train loss 1.43109. lr 3.000000e-04, running loss 1.42811, it/sec: 4.393105481199977
epoch 1 iter 12400: train loss 1.42850. lr 3.000000e-04, running loss 1.42818, it/sec: 4.367855171583257
epoch 1 iter 12420: train loss 1.43443. lr 3.000000e-04, running loss 1.42822, it/sec: 4.36205815500859
epoch 1 iter 12440: train loss 1.41928. lr 3.000000e-04, running loss 1.42820, it/sec: 4.3367111154702265
epoch 1 iter 12460: train loss 1.41937. lr 3.000000e-04, running loss 1.42819, it/sec: 4.376314850213935
epoch 1 iter 12480: train loss 1.44125. lr 3.000000e-04, running loss 1.42815, it/sec: 4.391005786107398
epoch 1 iter 12500: train loss 1.41538. lr 3.000000e-04, running loss 1.42812, it/sec: 1.1340749288172531
epoch 1 iter 12520: train loss 1.43523. lr 3.000000e-04, running loss 1.42815, it/sec: 4.338542229069164
epoch 1 iter 12540: train loss 1.42974. lr 3.000000e-04, running loss 1.42809, it/sec: 4.338694191964361
epoch 1 iter 12560: train loss 1.44521. lr 3.000000e-04, running loss 1.42812, it/sec: 4.389827672609235
epoch 1 iter 12580: train loss 1.42795. lr 3.000000e-04, running loss 1.42819, it/sec: 4.395095337211459
epoch 1 iter 12600: train loss 1.42360. lr 3.000000e-04, running loss 1.42820, it/sec: 4.394999026373671
epoch 1 iter 12620: train loss 1.41457. lr 3.000000e-04, running loss 1.42827, it/sec: 4.394618244239296
epoch 1 iter 12640: train loss 1.42848. lr 3.000000e-04, running loss 1.42823, it/sec: 4.352239390124329
epoch 1 iter 12660: train loss 1.44472. lr 3.000000e-04, running loss 1.42822, it/sec: 4.341057603639326
epoch 1 iter 12680: train loss 1.42492. lr 3.000000e-04, running loss 1.42820, it/sec: 4.409772889989814
epoch 1 iter 12700: train loss 1.43116. lr 3.000000e-04, running loss 1.42825, it/sec: 4.416425931874189
epoch 1 iter 12720: train loss 1.41225. lr 3.000000e-04, running loss 1.42849, it/sec: 4.358484304992462
epoch 1 iter 12740: train loss 1.42900. lr 3.000000e-04, running loss 1.42846, it/sec: 4.321477026250881
epoch 1 iter 12760: train loss 1.43668. lr 3.000000e-04, running loss 1.42854, it/sec: 4.373937772562923
epoch 1 iter 12780: train loss 1.84254. lr 3.000000e-04, running loss 1.42890, it/sec: 4.393945647396142
epoch 1 iter 12800: train loss 1.40643. lr 3.000000e-04, running loss 1.42877, it/sec: 4.363386510962072
epoch 1 iter 12820: train loss 1.43451. lr 3.000000e-04, running loss 1.42902, it/sec: 4.316043065160476
epoch 1 iter 12840: train loss 1.41826. lr 3.000000e-04, running loss 1.42899, it/sec: 4.341925693473858
epoch 1 iter 12860: train loss 1.42264. lr 3.000000e-04, running loss 1.42899, it/sec: 4.384515079872988
epoch 1 iter 12880: train loss 1.43302. lr 3.000000e-04, running loss 1.42898, it/sec: 4.349752819894573
epoch 1 iter 12900: train loss 1.44128. lr 3.000000e-04, running loss 1.42917, it/sec: 4.411564661033633
epoch 1 iter 12920: train loss 1.42357. lr 3.000000e-04, running loss 1.42908, it/sec: 4.368997832195612
epoch 1 iter 12940: train loss 1.41321. lr 3.000000e-04, running loss 1.42905, it/sec: 4.314815962892404
epoch 1 iter 12960: train loss 1.44163. lr 3.000000e-04, running loss 1.42902, it/sec: 4.360797981919622
epoch 1 iter 12980: train loss 1.43654. lr 3.000000e-04, running loss 1.42896, it/sec: 4.380065866692977
epoch 1 iter 13000: train loss 1.43377. lr 3.000000e-04, running loss 1.42892, it/sec: 4.4025831187146345
epoch 1 iter 13020: train loss 1.43701. lr 3.000000e-04, running loss 1.42894, it/sec: 4.359580976988095
epoch 1 iter 13040: train loss 1.43217. lr 3.000000e-04, running loss 1.42896, it/sec: 4.321807658811503
epoch 1 iter 13060: train loss 1.42408. lr 3.000000e-04, running loss 1.42900, it/sec: 4.398809957580808
epoch 1 iter 13080: train loss 1.43017. lr 3.000000e-04, running loss 1.42897, it/sec: 4.3915451602842195
epoch 1 iter 13100: train loss 1.44139. lr 3.000000e-04, running loss 1.42901, it/sec: 4.3843037993083795
epoch 1 iter 13120: train loss 1.41504. lr 3.000000e-04, running loss 1.42903, it/sec: 4.318579726619293
epoch 1 iter 13140: train loss 1.43935. lr 3.000000e-04, running loss 1.42900, it/sec: 4.312234608867137
epoch 1 iter 13160: train loss 1.43928. lr 3.000000e-04, running loss 1.42902, it/sec: 4.381698367294042
epoch 1 iter 13180: train loss 1.41290. lr 3.000000e-04, running loss 1.42900, it/sec: 4.403083521424814
epoch 1 iter 13200: train loss 1.42490. lr 3.000000e-04, running loss 1.42898, it/sec: 4.385490721778031
epoch 1 iter 13220: train loss 1.40896. lr 3.000000e-04, running loss 1.42895, it/sec: 4.417686340702222
epoch 1 iter 13240: train loss 1.42816. lr 3.000000e-04, running loss 1.42894, it/sec: 4.373657593433615
epoch 1 iter 13260: train loss 1.42569. lr 3.000000e-04, running loss 1.42899, it/sec: 4.35713058811823
epoch 1 iter 13280: train loss 1.42977. lr 3.000000e-04, running loss 1.42902, it/sec: 4.345679887546628
epoch 1 iter 13300: train loss 1.43256. lr 3.000000e-04, running loss 1.42910, it/sec: 4.39547614355323
epoch 1 iter 13320: train loss 1.42721. lr 3.000000e-04, running loss 1.42917, it/sec: 4.377289182244882
epoch 1 iter 13340: train loss 1.41377. lr 3.000000e-04, running loss 1.42946, it/sec: 4.35303511835466
epoch 1 iter 13360: train loss 1.41781. lr 3.000000e-04, running loss 1.42941, it/sec: 4.377589757642944
epoch 1 iter 13380: train loss 1.43428. lr 3.000000e-04, running loss 1.42932, it/sec: 4.407223308479923
epoch 1 iter 13400: train loss 1.41317. lr 3.000000e-04, running loss 1.42931, it/sec: 4.354565231111419
epoch 1 iter 13420: train loss 1.42091. lr 3.000000e-04, running loss 1.42933, it/sec: 4.342476760084256
epoch 1 iter 13440: train loss 1.42829. lr 3.000000e-04, running loss 1.43001, it/sec: 4.39071223503799
epoch 1 iter 13460: train loss 1.42807. lr 3.000000e-04, running loss 1.42994, it/sec: 4.403098818028869
epoch 1 iter 13480: train loss 1.43014. lr 3.000000e-04, running loss 1.42991, it/sec: 4.398092575529355
epoch 1 iter 13500: train loss 1.43186. lr 3.000000e-04, running loss 1.43004, it/sec: 4.402452327609581
epoch 1 iter 13520: train loss 1.43302. lr 3.000000e-04, running loss 1.42997, it/sec: 4.357554194349704
epoch 1 iter 13540: train loss 1.43054. lr 3.000000e-04, running loss 1.42991, it/sec: 4.348092644121379
epoch 1 iter 13560: train loss 1.43426. lr 3.000000e-04, running loss 1.42983, it/sec: 4.389425858508772
epoch 1 iter 13580: train loss 1.41244. lr 3.000000e-04, running loss 1.42968, it/sec: 4.379370060913999
epoch 1 iter 13600: train loss 1.42095. lr 3.000000e-04, running loss 1.42962, it/sec: 4.38640060934503
epoch 1 iter 13620: train loss 1.43504. lr 3.000000e-04, running loss 1.42958, it/sec: 4.330160556846353
epoch 1 iter 13640: train loss 1.41686. lr 3.000000e-04, running loss 1.42945, it/sec: 4.335870226405876
epoch 1 iter 13660: train loss 1.43325. lr 3.000000e-04, running loss 1.42943, it/sec: 4.393201130503844
epoch 1 iter 13680: train loss 1.42649. lr 3.000000e-04, running loss 1.42941, it/sec: 4.376084538918737
epoch 1 iter 13700: train loss 1.44180. lr 3.000000e-04, running loss 1.42942, it/sec: 4.353958959679893
epoch 1 iter 13720: train loss 1.43407. lr 3.000000e-04, running loss 1.42941, it/sec: 4.319922692368074
epoch 1 iter 13740: train loss 1.44769. lr 3.000000e-04, running loss 1.42939, it/sec: 4.3864658168779185
epoch 1 iter 13760: train loss 1.44291. lr 3.000000e-04, running loss 1.42944, it/sec: 4.399679988811227
epoch 1 iter 13780: train loss 1.40924. lr 3.000000e-04, running loss 1.42944, it/sec: 4.387433567355859
epoch 1 iter 13800: train loss 1.43464. lr 3.000000e-04, running loss 1.42938, it/sec: 4.3964283542276
epoch 1 iter 13820: train loss 1.43716. lr 3.000000e-04, running loss 1.42933, it/sec: 4.3589236224657855
epoch 1 iter 13840: train loss 1.42681. lr 3.000000e-04, running loss 1.42933, it/sec: 4.334508122484508
epoch 1 iter 13860: train loss 1.41201. lr 3.000000e-04, running loss 1.42930, it/sec: 4.379192452231514
epoch 1 iter 13880: train loss 1.42445. lr 3.000000e-04, running loss 1.42932, it/sec: 4.397250319773652
epoch 1 iter 13900: train loss 1.43109. lr 3.000000e-04, running loss 1.42923, it/sec: 4.284063280721695
epoch 1 iter 13920: train loss 1.42221. lr 3.000000e-04, running loss 1.42923, it/sec: 4.328754985050655
epoch 1 iter 13940: train loss 1.43104. lr 3.000000e-04, running loss 1.42922, it/sec: 4.380849406431797
epoch 1 iter 13960: train loss 1.42492. lr 3.000000e-04, running loss 1.42919, it/sec: 4.355706525865145
epoch 1 iter 13980: train loss 1.43759. lr 3.000000e-04, running loss 1.42919, it/sec: 4.368831122699269
epoch 1 iter 14000: train loss 1.42472. lr 3.000000e-04, running loss 1.42919, it/sec: 4.348459638513468
epoch 1 iter 14020: train loss 1.41659. lr 3.000000e-04, running loss 1.42912, it/sec: 4.355789113360215
epoch 1 iter 14040: train loss 1.40914. lr 3.000000e-04, running loss 1.42907, it/sec: 4.402655863277402
epoch 1 iter 14060: train loss 1.42213. lr 3.000000e-04, running loss 1.42906, it/sec: 4.390223082433785
epoch 1 iter 14080: train loss 1.40619. lr 3.000000e-04, running loss 1.42905, it/sec: 4.420563206148922
epoch 1 iter 14100: train loss 1.42164. lr 3.000000e-04, running loss 1.42894, it/sec: 4.378465810240898
epoch 1 iter 14120: train loss 1.41874. lr 3.000000e-04, running loss 1.42892, it/sec: 4.345674637545399
epoch 1 iter 14140: train loss 1.41914. lr 3.000000e-04, running loss 1.42895, it/sec: 4.355449658179439
epoch 1 iter 14160: train loss 1.41252. lr 3.000000e-04, running loss 1.42894, it/sec: 4.397175491566384
epoch 1 iter 14180: train loss 1.41699. lr 3.000000e-04, running loss 1.42888, it/sec: 4.405662163815022
epoch 1 iter 14200: train loss 1.43636. lr 3.000000e-04, running loss 1.42889, it/sec: 4.376376999010721
epoch 1 iter 14220: train loss 1.42526. lr 3.000000e-04, running loss 1.42895, it/sec: 4.321640627254166
epoch 1 iter 14240: train loss 1.41469. lr 3.000000e-04, running loss 1.42917, it/sec: 4.394425897904687
epoch 1 iter 14260: train loss 1.42823. lr 3.000000e-04, running loss 1.42918, it/sec: 4.395015829701236
epoch 1 iter 14280: train loss 1.41136. lr 3.000000e-04, running loss 1.42913, it/sec: 4.352490177232087
epoch 1 iter 14300: train loss 1.43820. lr 3.000000e-04, running loss 1.42903, it/sec: 4.3398232819698475
epoch 1 iter 14320: train loss 1.42544. lr 3.000000e-04, running loss 1.42898, it/sec: 4.337066109202704
epoch 1 iter 14340: train loss 1.42594. lr 3.000000e-04, running loss 1.42896, it/sec: 4.38108112116353
epoch 1 iter 14360: train loss 1.41201. lr 3.000000e-04, running loss 1.42890, it/sec: 4.388296253254603
epoch 1 iter 14380: train loss 1.42985. lr 3.000000e-04, running loss 1.42885, it/sec: 4.390013024961187
epoch 1 iter 14400: train loss 1.43106. lr 3.000000e-04, running loss 1.42881, it/sec: 4.357653201662712
epoch 1 iter 14420: train loss 1.42330. lr 3.000000e-04, running loss 1.42877, it/sec: 4.3409259026727005
epoch 1 iter 14440: train loss 1.43158. lr 3.000000e-04, running loss 1.42881, it/sec: 4.394166682124165
epoch 1 iter 14460: train loss 1.43747. lr 3.000000e-04, running loss 1.42884, it/sec: 4.404973066106313
epoch 1 iter 14480: train loss 1.40253. lr 3.000000e-04, running loss 1.42884, it/sec: 4.3954619637159995
epoch 1 iter 14500: train loss 1.45796. lr 3.000000e-04, running loss 1.42887, it/sec: 4.346663090319066
epoch 1 iter 14520: train loss 1.42063. lr 3.000000e-04, running loss 1.42885, it/sec: 4.330821812492829
epoch 1 iter 14540: train loss 1.42532. lr 3.000000e-04, running loss 1.42883, it/sec: 4.365045013054065
epoch 1 iter 14560: train loss 1.41973. lr 3.000000e-04, running loss 1.42879, it/sec: 4.402545962104022
epoch 1 iter 14580: train loss 1.42436. lr 3.000000e-04, running loss 1.42877, it/sec: 4.365440888108235
epoch 1 iter 14600: train loss 1.40243. lr 3.000000e-04, running loss 1.42878, it/sec: 4.341572561149725
epoch 1 iter 14620: train loss 1.42920. lr 3.000000e-04, running loss 1.42880, it/sec: 4.336776114062515
epoch 1 iter 14640: train loss 1.43777. lr 3.000000e-04, running loss 1.42885, it/sec: 4.347879321011966
epoch 1 iter 14660: train loss 1.42059. lr 3.000000e-04, running loss 1.42875, it/sec: 4.3844432211153554
epoch 1 iter 14680: train loss 1.41636. lr 3.000000e-04, running loss 1.42908, it/sec: 4.390703694387023
epoch 1 iter 14700: train loss 1.42121. lr 3.000000e-04, running loss 1.42901, it/sec: 4.4044938647550875
epoch 1 iter 14720: train loss 1.42293. lr 3.000000e-04, running loss 1.42894, it/sec: 4.328669728726615
epoch 1 iter 14740: train loss 1.41827. lr 3.000000e-04, running loss 1.42895, it/sec: 4.34065800078354
epoch 1 iter 14760: train loss 1.42559. lr 3.000000e-04, running loss 1.42887, it/sec: 4.383160842436968
epoch 1 iter 14780: train loss 1.42630. lr 3.000000e-04, running loss 1.42884, it/sec: 4.373174508607833
epoch 1 iter 14800: train loss 1.43418. lr 3.000000e-04, running loss 1.43470, it/sec: 4.324398694044304
epoch 1 iter 14820: train loss 1.41149. lr 3.000000e-04, running loss 1.43471, it/sec: 4.3624137704394235
epoch 1 iter 14840: train loss 1.40604. lr 3.000000e-04, running loss 1.43456, it/sec: 4.397163872410105
epoch 1 iter 14860: train loss 1.44734. lr 3.000000e-04, running loss 1.43441, it/sec: 4.3709607274166915
epoch 1 iter 14880: train loss 1.42364. lr 3.000000e-04, running loss 1.43427, it/sec: 4.316594382427578
epoch 1 iter 14900: train loss 1.42298. lr 3.000000e-04, running loss 1.43406, it/sec: 4.309849418301479
epoch 1 iter 14920: train loss 1.41551. lr 3.000000e-04, running loss 1.43395, it/sec: 4.3409994692514156
epoch 1 iter 14940: train loss 1.42386. lr 3.000000e-04, running loss 1.43390, it/sec: 4.415990900443708
epoch 1 iter 14960: train loss 1.41599. lr 3.000000e-04, running loss 1.43385, it/sec: 4.383043729150493
epoch 1 iter 14980: train loss 1.41839. lr 3.000000e-04, running loss 1.43376, it/sec: 4.408715062068561
epoch 1 iter 15000: train loss 1.45327. lr 3.000000e-04, running loss 1.43363, it/sec: 4.341578875377361
epoch 1 iter 15020: train loss 1.43078. lr 3.000000e-04, running loss 1.43361, it/sec: 4.294448205646755
epoch 1 iter 15040: train loss 1.41610. lr 3.000000e-04, running loss 1.43343, it/sec: 4.381136151037146
epoch 1 iter 15060: train loss 1.43348. lr 3.000000e-04, running loss 1.43333, it/sec: 4.368839215174964
epoch 1 iter 15080: train loss 1.43020. lr 3.000000e-04, running loss 1.43328, it/sec: 4.323335537651478
epoch 1 iter 15100: train loss 1.42774. lr 3.000000e-04, running loss 1.43317, it/sec: 4.359121461908332
epoch 1 iter 15120: train loss 1.43584. lr 3.000000e-04, running loss 1.43309, it/sec: 4.384915205210455
epoch 1 iter 15140: train loss 1.41998. lr 3.000000e-04, running loss 1.43300, it/sec: 4.356172437904949
epoch 1 iter 15160: train loss 1.42231. lr 3.000000e-04, running loss 1.43291, it/sec: 4.345253450923438
epoch 1 iter 15180: train loss 1.38492. lr 3.000000e-04, running loss 1.43287, it/sec: 4.331527041035432
epoch 1 iter 15200: train loss 1.41172. lr 3.000000e-04, running loss 1.43273, it/sec: 4.330313658782673
epoch 1 iter 15220: train loss 1.42046. lr 3.000000e-04, running loss 1.43258, it/sec: 4.397723905899345
epoch 1 iter 15240: train loss 2.00983. lr 3.000000e-04, running loss 1.43302, it/sec: 4.380808508218728
epoch 1 iter 15260: train loss 1.43806. lr 3.000000e-04, running loss 1.43296, it/sec: 4.396713043893629
epoch 1 iter 15280: train loss 1.43015. lr 3.000000e-04, running loss 1.43299, it/sec: 4.344417173746892
epoch 1 iter 15300: train loss 1.42527. lr 3.000000e-04, running loss 1.43285, it/sec: 4.3418803536658
epoch 1 iter 15320: train loss 1.42982. lr 3.000000e-04, running loss 1.43271, it/sec: 4.343562861198841
epoch 1 iter 15340: train loss 1.43219. lr 3.000000e-04, running loss 1.43257, it/sec: 4.383416743682988
epoch 1 iter 15360: train loss 1.42760. lr 3.000000e-04, running loss 1.43248, it/sec: 4.372450204954081
epoch 1 iter 15380: train loss 1.41983. lr 3.000000e-04, running loss 1.43236, it/sec: 4.3201976335977506
epoch 1 iter 15400: train loss 1.42844. lr 3.000000e-04, running loss 1.43223, it/sec: 4.339165188643897
epoch 1 iter 15420: train loss 1.42288. lr 3.000000e-04, running loss 1.43211, it/sec: 4.3935393161979395
epoch 1 iter 15440: train loss 1.44835. lr 3.000000e-04, running loss 1.43204, it/sec: 4.344665872434118
epoch 1 iter 15460: train loss 1.44053. lr 3.000000e-04, running loss 1.43200, it/sec: 4.3477153338251275
epoch 1 iter 15480: train loss 1.44619. lr 3.000000e-04, running loss 1.43190, it/sec: 4.360649201766095
epoch 1 iter 15500: train loss 1.42626. lr 3.000000e-04, running loss 1.43180, it/sec: 4.396739237982632
epoch 1 iter 15520: train loss 1.44157. lr 3.000000e-04, running loss 1.43179, it/sec: 4.402812990530167
epoch 1 iter 15540: train loss 1.41677. lr 3.000000e-04, running loss 1.43163, it/sec: 4.402847088151062
epoch 1 iter 15560: train loss 1.42257. lr 3.000000e-04, running loss 1.43167, it/sec: 4.3725599664019095
epoch 1 iter 15580: train loss 1.41655. lr 3.000000e-04, running loss 1.43158, it/sec: 4.339318343823944
epoch 1 iter 15600: train loss 1.43410. lr 3.000000e-04, running loss 1.43178, it/sec: 4.389442082049706
epoch 1 iter 15620: train loss 1.43832. lr 3.000000e-04, running loss 1.43163, it/sec: 4.38913962682887
epoch 1 iter 15640: train loss 1.41818. lr 3.000000e-04, running loss 1.43157, it/sec: 4.3982566512213985
epoch 1 iter 15660: train loss 1.42694. lr 3.000000e-04, running loss 1.43155, it/sec: 4.35156976090621
epoch 1 iter 15680: train loss 1.43322. lr 3.000000e-04, running loss 1.43156, it/sec: 4.343027325415104
epoch 1 iter 15700: train loss 1.42577. lr 3.000000e-04, running loss 1.43142, it/sec: 4.393490364025872
epoch 1 iter 15720: train loss 1.41599. lr 3.000000e-04, running loss 1.43143, it/sec: 4.396833672745458
epoch 1 iter 15740: train loss 1.43251. lr 3.000000e-04, running loss 1.43130, it/sec: 4.350899752608996
epoch 1 iter 15760: train loss 1.44417. lr 3.000000e-04, running loss 1.43122, it/sec: 4.336046894796114
epoch 1 iter 15780: train loss 1.43043. lr 3.000000e-04, running loss 1.43118, it/sec: 4.36612821316788
epoch 1 iter 15800: train loss 1.44723. lr 3.000000e-04, running loss 1.43113, it/sec: 4.373486358607905
epoch 1 iter 15820: train loss 1.41999. lr 3.000000e-04, running loss 1.43107, it/sec: 4.398690575119704
epoch 1 iter 15840: train loss 1.44529. lr 3.000000e-04, running loss 1.43098, it/sec: 4.360579226636725
epoch 1 iter 15860: train loss 1.41853. lr 3.000000e-04, running loss 1.43089, it/sec: 4.335441990692752
epoch 1 iter 15880: train loss 1.42753. lr 3.000000e-04, running loss 1.43088, it/sec: 4.3405482701758675
epoch 1 iter 15900: train loss 1.43418. lr 3.000000e-04, running loss 1.43085, it/sec: 4.412428490943993
epoch 1 iter 15920: train loss 1.41647. lr 3.000000e-04, running loss 1.43081, it/sec: 4.412073901665452
epoch 1 iter 15940: train loss 1.41979. lr 3.000000e-04, running loss 1.43077, it/sec: 4.360081136834731
epoch 1 iter 15960: train loss 1.41628. lr 3.000000e-04, running loss 1.43077, it/sec: 4.314654684557359
epoch 1 iter 15980: train loss 1.41235. lr 3.000000e-04, running loss 1.43062, it/sec: 4.338877773894158
epoch 1 iter 16000: train loss 1.43052. lr 3.000000e-04, running loss 1.43050, it/sec: 4.401427126011369
epoch 1 iter 16020: train loss 1.44768. lr 3.000000e-04, running loss 1.43052, it/sec: 4.377465256840298
epoch 1 iter 16040: train loss 1.43519. lr 3.000000e-04, running loss 1.43058, it/sec: 4.338508272170191
epoch 1 iter 16060: train loss 1.44441. lr 3.000000e-04, running loss 1.43053, it/sec: 4.358691357102806
epoch 1 iter 16080: train loss 1.45395. lr 3.000000e-04, running loss 1.43077, it/sec: 4.3909510871094435
epoch 1 iter 16100: train loss 1.43041. lr 3.000000e-04, running loss 1.43069, it/sec: 4.394522725439492
epoch 1 iter 16120: train loss 1.44312. lr 3.000000e-04, running loss 1.43072, it/sec: 4.3853234441848565
epoch 1 iter 16140: train loss 1.42344. lr 3.000000e-04, running loss 1.43061, it/sec: 4.408266700948908
epoch 1 iter 16160: train loss 1.41665. lr 3.000000e-04, running loss 1.43058, it/sec: 4.349267698803986
epoch 1 iter 16180: train loss 1.40778. lr 3.000000e-04, running loss 1.43051, it/sec: 4.311164598990492
epoch 1 iter 16200: train loss 1.42781. lr 3.000000e-04, running loss 1.43047, it/sec: 4.391595863032231
epoch 1 iter 16220: train loss 1.41825. lr 3.000000e-04, running loss 1.43036, it/sec: 4.384942605310827
epoch 1 iter 16240: train loss 1.41385. lr 3.000000e-04, running loss 1.43030, it/sec: 4.33357228987602
epoch 1 iter 16260: train loss 1.42128. lr 3.000000e-04, running loss 1.43009, it/sec: 4.319720986262188
epoch 1 iter 16280: train loss 1.42325. lr 3.000000e-04, running loss 1.43004, it/sec: 4.4115152859778295
epoch 1 iter 16300: train loss 1.44790. lr 3.000000e-04, running loss 1.43003, it/sec: 4.408012494442857
epoch 1 iter 16320: train loss 1.41690. lr 3.000000e-04, running loss 1.43003, it/sec: 4.3479865472140276
epoch 1 iter 16340: train loss 1.42211. lr 3.000000e-04, running loss 1.42992, it/sec: 4.3049586001545626
epoch 1 iter 16360: train loss 1.42148. lr 3.000000e-04, running loss 1.42988, it/sec: 4.36564597028233
epoch 1 iter 16380: train loss 1.43296. lr 3.000000e-04, running loss 1.42990, it/sec: 4.403696880620613
epoch 1 iter 16400: train loss 1.40804. lr 3.000000e-04, running loss 1.42983, it/sec: 4.38268591419254
epoch 1 iter 16420: train loss 1.43221. lr 3.000000e-04, running loss 1.42976, it/sec: 4.396523452958485
epoch 1 iter 16440: train loss 1.43867. lr 3.000000e-04, running loss 1.42977, it/sec: 4.378037956921972
epoch 1 iter 16460: train loss 1.43716. lr 3.000000e-04, running loss 1.42968, it/sec: 4.351858140900478
epoch 1 iter 16480: train loss 1.41953. lr 3.000000e-04, running loss 1.42955, it/sec: 4.353004478008064
epoch 1 iter 16500: train loss 1.43084. lr 3.000000e-04, running loss 1.42947, it/sec: 4.409900945760257
epoch 1 iter 16520: train loss 1.44310. lr 3.000000e-04, running loss 1.42948, it/sec: 4.390213831283409
epoch 1 iter 16540: train loss 1.40845. lr 3.000000e-04, running loss 1.42940, it/sec: 4.36422469978424
epoch 1 iter 16560: train loss 1.42473. lr 3.000000e-04, running loss 1.42934, it/sec: 4.3468998574786175
epoch 1 iter 16580: train loss 1.43268. lr 3.000000e-04, running loss 1.42933, it/sec: 4.381926198554434
epoch 1 iter 16600: train loss 1.41877. lr 3.000000e-04, running loss 1.42926, it/sec: 4.404924771392477
epoch 1 iter 16620: train loss 1.43404. lr 3.000000e-04, running loss 1.42933, it/sec: 4.372998472903706
epoch 1 iter 16640: train loss 1.43068. lr 3.000000e-04, running loss 1.42929, it/sec: 4.341477091827236
epoch 1 iter 16660: train loss 1.43835. lr 3.000000e-04, running loss 1.42934, it/sec: 4.3359747369671
epoch 1 iter 16680: train loss 1.40948. lr 3.000000e-04, running loss 1.42932, it/sec: 4.410597887688205
epoch 1 iter 16700: train loss 1.44214. lr 3.000000e-04, running loss 1.42936, it/sec: 4.390934641426749
epoch 1 iter 16720: train loss 1.42528. lr 3.000000e-04, running loss 1.42935, it/sec: 4.3801740911051965
epoch 1 iter 16740: train loss 1.43426. lr 3.000000e-04, running loss 1.42918, it/sec: 4.358655165464545
epoch 1 iter 16760: train loss 1.42079. lr 3.000000e-04, running loss 1.42920, it/sec: 4.335450356002728
epoch 1 iter 16780: train loss 1.45497. lr 3.000000e-04, running loss 1.42922, it/sec: 4.39887210943928
epoch 1 iter 16800: train loss 1.45368. lr 3.000000e-04, running loss 1.42922, it/sec: 4.390120007121013
epoch 1 iter 16820: train loss 1.42996. lr 3.000000e-04, running loss 1.42918, it/sec: 4.380723223306103
epoch 1 iter 16840: train loss 1.43944. lr 3.000000e-04, running loss 1.42935, it/sec: 4.343786629597841
epoch 1 iter 16860: train loss 1.43186. lr 3.000000e-04, running loss 1.42939, it/sec: 4.355145534167265
epoch 1 iter 16880: train loss 1.43316. lr 3.000000e-04, running loss 1.42931, it/sec: 4.379544883952899
epoch 1 iter 16900: train loss 1.44244. lr 3.000000e-04, running loss 1.42926, it/sec: 4.377290332116632
epoch 1 iter 16920: train loss 1.41988. lr 3.000000e-04, running loss 1.42925, it/sec: 4.344107946086642
epoch 1 iter 16940: train loss 1.42550. lr 3.000000e-04, running loss 1.42919, it/sec: 4.362481254385771
epoch 1 iter 16960: train loss 1.43293. lr 3.000000e-04, running loss 1.42908, it/sec: 4.3544345855385815
epoch 1 iter 16980: train loss 1.41696. lr 3.000000e-04, running loss 1.42896, it/sec: 4.3911884432059765
epoch 1 iter 17000: train loss 1.41280. lr 3.000000e-04, running loss 1.42892, it/sec: 4.393951168638589
epoch 1 iter 17020: train loss 1.42763. lr 3.000000e-04, running loss 1.42935, it/sec: 4.378291438207969
epoch 1 iter 17040: train loss 1.43908. lr 3.000000e-04, running loss 1.42938, it/sec: 4.344526437564035
epoch 1 iter 17060: train loss 1.43632. lr 3.000000e-04, running loss 1.42938, it/sec: 4.353547917352096
epoch 1 iter 17080: train loss 1.45638. lr 3.000000e-04, running loss 1.42940, it/sec: 4.391368202655671
epoch 1 iter 17100: train loss 1.42400. lr 3.000000e-04, running loss 1.42935, it/sec: 4.416516260563148
epoch 1 iter 17120: train loss 1.42076. lr 3.000000e-04, running loss 1.42936, it/sec: 4.35880681209177
epoch 1 iter 17140: train loss 1.42375. lr 3.000000e-04, running loss 1.42936, it/sec: 4.337193042929631
epoch 1 iter 17160: train loss 1.42672. lr 3.000000e-04, running loss 1.42934, it/sec: 4.3718388867771685
epoch 1 iter 17180: train loss 1.42575. lr 3.000000e-04, running loss 1.42927, it/sec: 4.409482927964727
epoch 1 iter 17200: train loss 1.43178. lr 3.000000e-04, running loss 1.42924, it/sec: 4.389358869541581
epoch 1 iter 17220: train loss 1.42607. lr 3.000000e-04, running loss 1.42920, it/sec: 4.360923780269561
epoch 1 iter 17240: train loss 1.42440. lr 3.000000e-04, running loss 1.42914, it/sec: 4.3200376321494485
epoch 1 iter 17260: train loss 1.40101. lr 3.000000e-04, running loss 1.42903, it/sec: 4.383817130563452
epoch 1 iter 17280: train loss 1.42170. lr 3.000000e-04, running loss 1.42892, it/sec: 4.366671978805092
epoch 1 iter 17300: train loss 1.43708. lr 3.000000e-04, running loss 1.42888, it/sec: 4.401103666150933
epoch 1 iter 17320: train loss 1.43986. lr 3.000000e-04, running loss 1.42887, it/sec: 4.385540322042884
epoch 1 iter 17340: train loss 1.42275. lr 3.000000e-04, running loss 1.42880, it/sec: 4.375976113001811
epoch 1 iter 17360: train loss 1.41831. lr 3.000000e-04, running loss 1.42890, it/sec: 4.333615464610919
epoch 1 iter 17380: train loss 1.43960. lr 3.000000e-04, running loss 1.42929, it/sec: 4.336249732268299
epoch 1 iter 17400: train loss 1.41393. lr 3.000000e-04, running loss 1.42946, it/sec: 4.400797278379717
epoch 1 iter 17420: train loss 1.41508. lr 3.000000e-04, running loss 1.42951, it/sec: 4.409126032767104
epoch 1 iter 17440: train loss 1.42892. lr 3.000000e-04, running loss 1.42944, it/sec: 4.402295032068807
epoch 1 iter 17460: train loss 1.43921. lr 3.000000e-04, running loss 1.42939, it/sec: 4.332529751637105
epoch 1 iter 17480: train loss 1.42083. lr 3.000000e-04, running loss 1.42936, it/sec: 4.355857359958117
epoch 1 iter 17500: train loss 1.43181. lr 3.000000e-04, running loss 1.42932, it/sec: 4.4078249394046845
epoch 1 iter 17520: train loss 1.42288. lr 3.000000e-04, running loss 1.42924, it/sec: 4.400620426528024
epoch 1 iter 17540: train loss 1.42104. lr 3.000000e-04, running loss 1.42921, it/sec: 4.406345089524447
epoch 1 iter 17560: train loss 1.43647. lr 3.000000e-04, running loss 1.42936, it/sec: 4.351860128484663
epoch 1 iter 17580: train loss 1.42730. lr 3.000000e-04, running loss 1.42947, it/sec: 4.374872250847669
epoch 1 iter 17600: train loss 1.40537. lr 3.000000e-04, running loss 1.42944, it/sec: 4.356565471158038
epoch 1 iter 17620: train loss 1.43454. lr 3.000000e-04, running loss 1.42945, it/sec: 4.3163636801799266
epoch 1 iter 17640: train loss 1.42681. lr 3.000000e-04, running loss 1.42941, it/sec: 4.341788546691597
epoch 1 iter 17660: train loss 1.43093. lr 3.000000e-04, running loss 1.42942, it/sec: 4.376727981986237
epoch 1 iter 17680: train loss 1.42827. lr 3.000000e-04, running loss 1.42937, it/sec: 4.3791657200249166
epoch 1 iter 17700: train loss 1.43007. lr 3.000000e-04, running loss 1.42940, it/sec: 4.379734566464961
epoch 1 iter 17720: train loss 1.43860. lr 3.000000e-04, running loss 1.42941, it/sec: 4.3074700184828725
epoch 1 iter 17740: train loss 1.43239. lr 3.000000e-04, running loss 1.42943, it/sec: 4.345089927696523
epoch 1 iter 17760: train loss 1.43058. lr 3.000000e-04, running loss 1.42948, it/sec: 4.38076268002569
epoch 1 iter 17780: train loss 1.43263. lr 3.000000e-04, running loss 1.42947, it/sec: 4.395497725555794
epoch 1 iter 17800: train loss 1.40660. lr 3.000000e-04, running loss 1.42940, it/sec: 4.325526923109899
epoch 1 iter 17820: train loss 1.43079. lr 3.000000e-04, running loss 1.42935, it/sec: 4.368329621024563
epoch 1 iter 17840: train loss 1.43420. lr 3.000000e-04, running loss 1.42952, it/sec: 4.385392695538074
epoch 1 iter 17860: train loss 1.43929. lr 3.000000e-04, running loss 1.42951, it/sec: 4.400431001165394
epoch 1 iter 17880: train loss 1.42740. lr 3.000000e-04, running loss 1.42947, it/sec: 4.372066285778327
epoch 1 iter 17900: train loss 1.43137. lr 3.000000e-04, running loss 1.42944, it/sec: 4.413487985048078
epoch 1 iter 17920: train loss 1.42527. lr 3.000000e-04, running loss 1.42939, it/sec: 4.43061880654848
epoch 1 iter 17940: train loss 1.42195. lr 3.000000e-04, running loss 1.42931, it/sec: 4.380268429865107
epoch 1 iter 17960: train loss 1.41725. lr 3.000000e-04, running loss 1.42960, it/sec: 4.3807323384355366
epoch 1 iter 17980: train loss 1.43665. lr 3.000000e-04, running loss 1.42954, it/sec: 4.3579503821481795
epoch 1 iter 18000: train loss 1.43537. lr 3.000000e-04, running loss 1.42942, it/sec: 4.369205502189404
epoch 1 iter 18020: train loss 1.41931. lr 3.000000e-04, running loss 1.42945, it/sec: 4.368963283557473
epoch 1 iter 18040: train loss 1.41715. lr 3.000000e-04, running loss 1.42941, it/sec: 4.355004005032107
epoch 1 iter 18060: train loss 1.44218. lr 3.000000e-04, running loss 1.42936, it/sec: 4.392955974525759
epoch 1 iter 18080: train loss 1.42766. lr 3.000000e-04, running loss 1.42934, it/sec: 4.4108613231308365
epoch 1 iter 18100: train loss 1.42905. lr 3.000000e-04, running loss 1.42928, it/sec: 4.424316635972642
epoch 1 iter 18120: train loss 1.44560. lr 3.000000e-04, running loss 1.42948, it/sec: 4.437699830282855
epoch 1 iter 18140: train loss 1.43083. lr 3.000000e-04, running loss 1.42943, it/sec: 4.4324961674793135
epoch 1 iter 18160: train loss 1.41434. lr 3.000000e-04, running loss 1.42936, it/sec: 4.386755047969394
epoch 1 iter 18180: train loss 1.42438. lr 3.000000e-04, running loss 1.42933, it/sec: 4.437863782736832
epoch 1 iter 18200: train loss 1.43384. lr 3.000000e-04, running loss 1.42935, it/sec: 4.411106283340762
epoch 1 iter 18220: train loss 1.43597. lr 3.000000e-04, running loss 1.42938, it/sec: 4.406337012337281
epoch 1 iter 18240: train loss 1.40252. lr 3.000000e-04, running loss 1.42927, it/sec: 4.4377900867174676
epoch 1 iter 18260: train loss 1.41718. lr 3.000000e-04, running loss 1.42934, it/sec: 4.3596193501589715
epoch 1 iter 18280: train loss 1.41910. lr 3.000000e-04, running loss 1.42928, it/sec: 4.367177181941051
epoch 1 iter 18300: train loss 1.44613. lr 3.000000e-04, running loss 1.42933, it/sec: 4.36400200004903
epoch 1 iter 18320: train loss 1.42650. lr 3.000000e-04, running loss 1.42929, it/sec: 4.3963367190588
epoch 1 iter 18340: train loss 1.42120. lr 3.000000e-04, running loss 1.42927, it/sec: 4.407284765542059
epoch 1 iter 18360: train loss 1.43556. lr 3.000000e-04, running loss 1.42926, it/sec: 4.406469451656556
epoch 1 iter 18380: train loss 1.42419. lr 3.000000e-04, running loss 1.42919, it/sec: 4.4005537514582205
epoch 1 iter 18400: train loss 1.44402. lr 3.000000e-04, running loss 1.42912, it/sec: 4.399691622395346
epoch 1 iter 18420: train loss 1.41586. lr 3.000000e-04, running loss 1.42901, it/sec: 4.328346495669873
epoch 1 iter 18440: train loss 1.42259. lr 3.000000e-04, running loss 1.42895, it/sec: 4.350529639359795
epoch 1 iter 18460: train loss 1.43272. lr 3.000000e-04, running loss 1.42890, it/sec: 4.335727108852839
epoch 1 iter 18480: train loss 1.42453. lr 3.000000e-04, running loss 1.42891, it/sec: 4.370090005189461
epoch 1 iter 18500: train loss 1.43223. lr 3.000000e-04, running loss 1.42890, it/sec: 4.375059711615952
epoch 1 iter 18520: train loss 1.42764. lr 3.000000e-04, running loss 1.42890, it/sec: 4.383273601535435
epoch 1 iter 18540: train loss 1.43472. lr 3.000000e-04, running loss 1.42898, it/sec: 4.350161593832018
epoch 1 iter 18560: train loss 1.42795. lr 3.000000e-04, running loss 1.42894, it/sec: 4.339642200464938
epoch 1 iter 18580: train loss 1.42109. lr 3.000000e-04, running loss 1.42890, it/sec: 4.390371381025922
epoch 1 iter 18600: train loss 1.41360. lr 3.000000e-04, running loss 1.42897, it/sec: 4.408365985493059
epoch 1 iter 18620: train loss 1.43111. lr 3.000000e-04, running loss 1.42894, it/sec: 4.405847614013406
epoch 1 iter 18640: train loss 1.48505. lr 3.000000e-04, running loss 1.42900, it/sec: 4.408368415289823
epoch 1 iter 18660: train loss 1.41820. lr 3.000000e-04, running loss 1.42914, it/sec: 4.428508016390558
epoch 1 iter 18680: train loss 1.41715. lr 3.000000e-04, running loss 1.42904, it/sec: 4.4079186668628685
epoch 1 iter 18700: train loss 1.41705. lr 3.000000e-04, running loss 1.42895, it/sec: 4.391357249491838
epoch 1 iter 18720: train loss 1.41849. lr 3.000000e-04, running loss 1.42882, it/sec: 4.392783051181677
epoch 1 iter 18740: train loss 1.43096. lr 3.000000e-04, running loss 1.42876, it/sec: 4.404152730609828
epoch 1 iter 18760: train loss 1.42682. lr 3.000000e-04, running loss 1.42885, it/sec: 4.431099253650265
epoch 1 iter 18780: train loss 1.40529. lr 3.000000e-04, running loss 1.42879, it/sec: 4.432905454257593
epoch 1 iter 18800: train loss 1.43946. lr 3.000000e-04, running loss 1.42882, it/sec: 4.43089755593585
epoch 1 iter 18820: train loss 1.44053. lr 3.000000e-04, running loss 1.42882, it/sec: 4.437330949172559
epoch 1 iter 18840: train loss 1.41557. lr 3.000000e-04, running loss 1.42878, it/sec: 4.395172181576239
epoch 1 iter 18860: train loss 1.44413. lr 3.000000e-04, running loss 1.42880, it/sec: 4.430180444601343
epoch 1 iter 18880: train loss 1.42173. lr 3.000000e-04, running loss 1.42881, it/sec: 4.390472888330606
epoch 1 iter 18900: train loss 1.41713. lr 3.000000e-04, running loss 1.42872, it/sec: 4.3729427490093045
epoch 1 iter 18920: train loss 1.43212. lr 3.000000e-04, running loss 1.42873, it/sec: 4.345020034541696
epoch 1 iter 18940: train loss 1.43049. lr 3.000000e-04, running loss 1.42866, it/sec: 4.34634711907972
epoch 1 iter 18960: train loss 1.41222. lr 3.000000e-04, running loss 1.42854, it/sec: 4.347669059934316
epoch 1 iter 18980: train loss 1.42313. lr 3.000000e-04, running loss 1.42855, it/sec: 4.3896187308188805
epoch 1 iter 19000: train loss 1.43403. lr 3.000000e-04, running loss 1.42857, it/sec: 4.393484978769285
epoch 1 iter 19020: train loss 1.42646. lr 3.000000e-04, running loss 1.42858, it/sec: 4.406934188083145
epoch 1 iter 19040: train loss 1.42797. lr 3.000000e-04, running loss 1.42851, it/sec: 4.379853441400902
epoch 1 iter 19060: train loss 1.42423. lr 3.000000e-04, running loss 1.42841, it/sec: 4.4117575567444325
epoch 1 iter 19080: train loss 1.42202. lr 3.000000e-04, running loss 1.42838, it/sec: 4.377616586620578
epoch 1 iter 19100: train loss 1.42531. lr 3.000000e-04, running loss 1.42833, it/sec: 4.405098749140326
epoch 1 iter 19120: train loss 1.41090. lr 3.000000e-04, running loss 1.42824, it/sec: 4.393989416519163
epoch 1 iter 19140: train loss 1.45412. lr 3.000000e-04, running loss 1.42830, it/sec: 4.423356592333009
epoch 1 iter 19160: train loss 1.42614. lr 3.000000e-04, running loss 1.42826, it/sec: 4.43299476768663
epoch 1 iter 19180: train loss 1.43934. lr 3.000000e-04, running loss 1.42835, it/sec: 4.418439687280929
epoch 1 iter 19200: train loss 1.43218. lr 3.000000e-04, running loss 1.42837, it/sec: 4.398657856138076
epoch 1 iter 19220: train loss 1.43728. lr 3.000000e-04, running loss 1.42830, it/sec: 4.359227418249336
epoch 1 iter 19240: train loss 1.43308. lr 3.000000e-04, running loss 1.42821, it/sec: 4.369818108108052
epoch 1 iter 19260: train loss 1.42199. lr 3.000000e-04, running loss 1.42820, it/sec: 4.384138610406381
epoch 1 iter 19280: train loss 1.42373. lr 3.000000e-04, running loss 1.42821, it/sec: 4.381542954676061
epoch 1 iter 19300: train loss 1.43575. lr 3.000000e-04, running loss 1.42831, it/sec: 4.362989353015308
epoch 1 iter 19320: train loss 1.43110. lr 3.000000e-04, running loss 1.42832, it/sec: 4.347759679873788
epoch 1 iter 19340: train loss 1.43293. lr 3.000000e-04, running loss 1.42840, it/sec: 4.346117306887778
epoch 1 iter 19360: train loss 1.42383. lr 3.000000e-04, running loss 1.42835, it/sec: 4.366282266736802
epoch 1 iter 19380: train loss 1.42994. lr 3.000000e-04, running loss 1.42830, it/sec: 4.383382850168652
epoch 1 iter 19400: train loss 1.43648. lr 3.000000e-04, running loss 1.42827, it/sec: 4.365439400580743
epoch 1 iter 19420: train loss 1.44520. lr 3.000000e-04, running loss 1.42829, it/sec: 4.352179230392039
epoch 1 iter 19440: train loss 1.42433. lr 3.000000e-04, running loss 1.42837, it/sec: 4.380105963743936
epoch 1 iter 19460: train loss 1.42548. lr 3.000000e-04, running loss 1.42833, it/sec: 4.365436084984336
epoch 1 iter 19480: train loss 1.42106. lr 3.000000e-04, running loss 1.42828, it/sec: 4.396866306079219
epoch 1 iter 19500: train loss 1.42792. lr 3.000000e-04, running loss 1.42840, it/sec: 4.387390679444017
epoch 1 iter 19520: train loss 1.43189. lr 3.000000e-04, running loss 1.42854, it/sec: 4.436951458668364
epoch 1 iter 19540: train loss 1.42510. lr 3.000000e-04, running loss 1.42847, it/sec: 4.413448053409111
epoch 1 iter 19560: train loss 1.43905. lr 3.000000e-04, running loss 1.42843, it/sec: 4.430557030649717
epoch 1 iter 19580: train loss 1.45077. lr 3.000000e-04, running loss 1.42841, it/sec: 4.422827295696588
epoch 1 iter 19600: train loss 1.42990. lr 3.000000e-04, running loss 1.42841, it/sec: 4.398806263350004
epoch 1 iter 19620: train loss 1.43453. lr 3.000000e-04, running loss 1.42845, it/sec: 4.4225333272260725
epoch 1 iter 19640: train loss 1.44224. lr 3.000000e-04, running loss 1.42840, it/sec: 4.4016788277434085
epoch 1 iter 19660: train loss 1.42250. lr 3.000000e-04, running loss 1.42840, it/sec: 4.385562825345283
epoch 1 iter 19680: train loss 1.41397. lr 3.000000e-04, running loss 1.42835, it/sec: 4.428300710303528
epoch 1 iter 19700: train loss 1.42518. lr 3.000000e-04, running loss 1.42832, it/sec: 4.408243070456497
epoch 1 iter 19720: train loss 1.41547. lr 3.000000e-04, running loss 1.42830, it/sec: 4.368563353543521
epoch 1 iter 19740: train loss 1.42270. lr 3.000000e-04, running loss 1.42822, it/sec: 4.356630477415375
epoch 1 iter 19760: train loss 1.42034. lr 3.000000e-04, running loss 1.42818, it/sec: 4.392043443076559
epoch 1 iter 19780: train loss 1.43771. lr 3.000000e-04, running loss 1.42816, it/sec: 4.349426581486598
epoch 1 iter 19800: train loss 1.42810. lr 3.000000e-04, running loss 1.42836, it/sec: 4.339697136995352
epoch 1 iter 19820: train loss 1.43150. lr 3.000000e-04, running loss 1.42832, it/sec: 4.420968140738353
epoch 1 iter 19840: train loss 1.42903. lr 3.000000e-04, running loss 1.42834, it/sec: 4.406026284059108
epoch 1 iter 19860: train loss 1.43043. lr 3.000000e-04, running loss 1.42831, it/sec: 4.417083946725185
epoch 1 iter 19880: train loss 1.43884. lr 3.000000e-04, running loss 1.42836, it/sec: 4.420646121565097
epoch 1 iter 19900: train loss 1.43050. lr 3.000000e-04, running loss 1.42833, it/sec: 4.408797513245379
epoch 1 iter 19920: train loss 1.42697. lr 3.000000e-04, running loss 1.42827, it/sec: 4.407625782355397
epoch 1 iter 19940: train loss 1.42235. lr 3.000000e-04, running loss 1.42825, it/sec: 4.398139019237913
epoch 1 iter 19960: train loss 1.44928. lr 3.000000e-04, running loss 1.42827, it/sec: 4.379895684066394
epoch 1 iter 19980: train loss 1.43448. lr 3.000000e-04, running loss 1.42837, it/sec: 4.422097014428093
epoch 1 iter 20000: train loss 1.43075. lr 3.000000e-04, running loss 1.42855, it/sec: 4.41265386222471
epoch 1 iter 20020: train loss 1.42713. lr 3.000000e-04, running loss 1.42853, it/sec: 4.421017629779151
epoch 1 iter 20040: train loss 1.41666. lr 3.000000e-04, running loss 1.42867, it/sec: 4.4202882769091705
epoch 1 iter 20060: train loss 1.42060. lr 3.000000e-04, running loss 1.42853, it/sec: 4.443627706695191
epoch 1 iter 20080: train loss 1.42338. lr 3.000000e-04, running loss 1.42852, it/sec: 4.422125583484995
epoch 1 iter 20100: train loss 1.40394. lr 3.000000e-04, running loss 1.42847, it/sec: 4.390891126165482
epoch 1 iter 20120: train loss 1.42966. lr 3.000000e-04, running loss 1.42844, it/sec: 4.399542981931207
epoch 1 iter 20140: train loss 1.43297. lr 3.000000e-04, running loss 1.42845, it/sec: 4.362270323033267
epoch 1 iter 20160: train loss 1.43801. lr 3.000000e-04, running loss 1.42837, it/sec: 4.36108743248529
epoch 1 iter 20180: train loss 1.41176. lr 3.000000e-04, running loss 1.42835, it/sec: 4.352169816940251
epoch 1 iter 20200: train loss 1.40885. lr 3.000000e-04, running loss 1.42834, it/sec: 4.374186571643551
epoch 1 iter 20220: train loss 1.43774. lr 3.000000e-04, running loss 1.42835, it/sec: 4.325318539546344
epoch 1 iter 20240: train loss 1.41246. lr 3.000000e-04, running loss 1.42834, it/sec: 4.368841239409568
epoch 1 iter 20260: train loss 1.42531. lr 3.000000e-04, running loss 1.42832, it/sec: 4.380631972450878
epoch 1 iter 20280: train loss 1.42758. lr 3.000000e-04, running loss 1.42835, it/sec: 4.37974333358372
epoch 1 iter 20300: train loss 1.42466. lr 3.000000e-04, running loss 1.42830, it/sec: 4.377628851182502
epoch 1 iter 20320: train loss 1.43353. lr 3.000000e-04, running loss 1.42828, it/sec: 4.332900638711997
epoch 1 iter 20340: train loss 1.43921. lr 3.000000e-04, running loss 1.42834, it/sec: 4.341401397170848
epoch 1 iter 20360: train loss 1.44820. lr 3.000000e-04, running loss 1.42843, it/sec: 4.3657861150815265
epoch 1 iter 20380: train loss 1.41354. lr 3.000000e-04, running loss 1.42854, it/sec: 4.378147710115504
epoch 1 iter 20400: train loss 1.43509. lr 3.000000e-04, running loss 1.42867, it/sec: 4.414703782659787
epoch 1 iter 20420: train loss 1.40861. lr 3.000000e-04, running loss 1.42874, it/sec: 4.410679418475095
epoch 1 iter 20440: train loss 1.40770. lr 3.000000e-04, running loss 1.42873, it/sec: 4.413359486186262
epoch 1 iter 20460: train loss 1.42546. lr 3.000000e-04, running loss 1.42867, it/sec: 4.411347885056066
epoch 1 iter 20480: train loss 1.42521. lr 3.000000e-04, running loss 1.42863, it/sec: 4.409930621883597
epoch 1 iter 20500: train loss 1.45469. lr 3.000000e-04, running loss 1.42868, it/sec: 4.424659842419247
epoch 1 iter 20520: train loss 1.42935. lr 3.000000e-04, running loss 1.42859, it/sec: 4.421768416569155
epoch 1 iter 20540: train loss 1.41411. lr 3.000000e-04, running loss 1.42860, it/sec: 4.425794362671885
epoch 1 iter 20560: train loss 1.43708. lr 3.000000e-04, running loss 1.42863, it/sec: 4.420168934688569
epoch 1 iter 20580: train loss 1.43895. lr 3.000000e-04, running loss 1.42862, it/sec: 4.425063374878958
epoch 1 iter 20600: train loss 1.43162. lr 3.000000e-04, running loss 1.42857, it/sec: 4.412544687609648
epoch 1 iter 20620: train loss 1.42760. lr 3.000000e-04, running loss 1.42853, it/sec: 4.422173904379134
epoch 1 iter 20640: train loss 1.42779. lr 3.000000e-04, running loss 1.42853, it/sec: 4.4134691297050175
epoch 1 iter 20660: train loss 1.43137. lr 3.000000e-04, running loss 1.42846, it/sec: 4.429198709462744
epoch 1 iter 20680: train loss 1.51780. lr 3.000000e-04, running loss 1.42848, it/sec: 4.387841985153209
epoch 1 iter 20700: train loss 1.44362. lr 3.000000e-04, running loss 1.42850, it/sec: 4.360170202291015
epoch 1 iter 20720: train loss 1.42687. lr 3.000000e-04, running loss 1.42849, it/sec: 4.3543678425379495
epoch 1 iter 20740: train loss 1.40442. lr 3.000000e-04, running loss 1.42843, it/sec: 4.351562148922291
epoch 1 iter 20760: train loss 1.43498. lr 3.000000e-04, running loss 1.42853, it/sec: 4.366921953326659
epoch 1 iter 20780: train loss 1.47335. lr 3.000000e-04, running loss 1.42849, it/sec: 4.3450901529811565
epoch 1 iter 20800: train loss 1.42531. lr 3.000000e-04, running loss 1.42843, it/sec: 4.3600244691613605
epoch 1 iter 20820: train loss 1.41113. lr 3.000000e-04, running loss 1.42865, it/sec: 4.351026853841368
epoch 1 iter 20840: train loss 1.44854. lr 3.000000e-04, running loss 1.42865, it/sec: 4.350714317909492
epoch 1 iter 20860: train loss 1.41731. lr 3.000000e-04, running loss 1.42858, it/sec: 4.359838313408749
epoch 1 iter 20880: train loss 1.43093. lr 3.000000e-04, running loss 1.42871, it/sec: 4.362557076618878
epoch 1 iter 20900: train loss 1.46056. lr 3.000000e-04, running loss 1.42876, it/sec: 4.379524206335662
epoch 1 iter 20920: train loss 1.42588. lr 3.000000e-04, running loss 1.42877, it/sec: 4.404608984401583
epoch 1 iter 20940: train loss 1.42375. lr 3.000000e-04, running loss 1.42874, it/sec: 4.409147339385504
epoch 1 iter 20960: train loss 1.43848. lr 3.000000e-04, running loss 1.42872, it/sec: 4.436622599615887
epoch 1 iter 20980: train loss 1.44535. lr 3.000000e-04, running loss 1.42874, it/sec: 4.433616626482943
epoch 1 iter 21000: train loss 1.42070. lr 3.000000e-04, running loss 1.42884, it/sec: 4.405394595383217
epoch 1 iter 21020: train loss 1.42782. lr 3.000000e-04, running loss 1.42895, it/sec: 4.405390364790847
epoch 1 iter 21040: train loss 1.41757. lr 3.000000e-04, running loss 1.42886, it/sec: 4.418374562002237
epoch 1 iter 21060: train loss 1.41030. lr 3.000000e-04, running loss 1.42890, it/sec: 4.396471959058644
epoch 1 iter 21080: train loss 1.43437. lr 3.000000e-04, running loss 1.42890, it/sec: 4.379985310758437
epoch 1 iter 21100: train loss 1.41850. lr 3.000000e-04, running loss 1.42903, it/sec: 4.410741829197658
epoch 1 iter 21120: train loss 1.41030. lr 3.000000e-04, running loss 1.42898, it/sec: 4.4055577409049205
epoch 1 iter 21140: train loss 1.41870. lr 3.000000e-04, running loss 1.42891, it/sec: 4.416462484120681
epoch 1 iter 21160: train loss 1.40993. lr 3.000000e-04, running loss 1.42893, it/sec: 4.430617845589561
epoch 1 iter 21180: train loss 1.42640. lr 3.000000e-04, running loss 1.42893, it/sec: 4.392256279905507
epoch 1 iter 21200: train loss 1.68025. lr 3.000000e-04, running loss 1.42919, it/sec: 4.377154028007441
epoch 1 iter 21220: train loss 1.43726. lr 3.000000e-04, running loss 1.42914, it/sec: 4.353167632342077
epoch 1 iter 21240: train loss 1.41609. lr 3.000000e-04, running loss 1.42904, it/sec: 4.354994635659022
epoch 1 iter 21260: train loss 1.43327. lr 3.000000e-04, running loss 1.42903, it/sec: 4.346881415019029
epoch 1 iter 21280: train loss 1.41881. lr 3.000000e-04, running loss 1.42897, it/sec: 4.375514227502126
epoch 1 iter 21300: train loss 1.41821. lr 3.000000e-04, running loss 1.42893, it/sec: 4.383380679346343
epoch 1 iter 21320: train loss 1.42838. lr 3.000000e-04, running loss 1.42888, it/sec: 4.392113774807392
epoch 1 iter 21340: train loss 1.41210. lr 3.000000e-04, running loss 1.42894, it/sec: 4.419371971777221
epoch 1 iter 21360: train loss 1.43405. lr 3.000000e-04, running loss 1.42890, it/sec: 4.402022835088255
epoch 1 iter 21380: train loss 1.41018. lr 3.000000e-04, running loss 1.42886, it/sec: 4.425736561159938
epoch 1 iter 21400: train loss 1.43070. lr 3.000000e-04, running loss 1.42884, it/sec: 4.399335767351557
epoch 1 iter 21420: train loss 1.41315. lr 3.000000e-04, running loss 1.42879, it/sec: 4.400771423705659
epoch 1 iter 21440: train loss 1.40558. lr 3.000000e-04, running loss 1.42865, it/sec: 4.42555256682566
epoch 1 iter 21460: train loss 1.41808. lr 3.000000e-04, running loss 1.42856, it/sec: 4.4279116264596174
epoch 1 iter 21480: train loss 1.43152. lr 3.000000e-04, running loss 1.42849, it/sec: 4.429680869083622
epoch 1 iter 21500: train loss 1.43400. lr 3.000000e-04, running loss 1.42851, it/sec: 4.375967170357908
epoch 1 iter 21520: train loss 1.40811. lr 3.000000e-04, running loss 1.42850, it/sec: 4.357802935890934
epoch 1 iter 21540: train loss 1.42468. lr 3.000000e-04, running loss 1.42841, it/sec: 4.356472377082463
epoch 1 iter 21560: train loss 1.43163. lr 3.000000e-04, running loss 1.42842, it/sec: 4.360326746237644
epoch 1 iter 21580: train loss 1.43066. lr 3.000000e-04, running loss 1.42844, it/sec: 4.373900371846815
epoch 1 iter 21600: train loss 1.45098. lr 3.000000e-04, running loss 1.42857, it/sec: 4.381649371307107
epoch 1 iter 21620: train loss 1.44700. lr 3.000000e-04, running loss 1.42855, it/sec: 4.403252855282285
epoch 1 iter 21640: train loss 1.43555. lr 3.000000e-04, running loss 1.42852, it/sec: 4.424090737158262
epoch 1 iter 21660: train loss 1.44788. lr 3.000000e-04, running loss 1.42854, it/sec: 4.398891150006143
epoch 1 iter 21680: train loss 1.40889. lr 3.000000e-04, running loss 1.42852, it/sec: 4.414835282316826
epoch 1 iter 21700: train loss 1.42476. lr 3.000000e-04, running loss 1.42848, it/sec: 4.439465396384853
epoch 1 iter 21720: train loss 1.42673. lr 3.000000e-04, running loss 1.42850, it/sec: 4.429699431463629
epoch 1 iter 21740: train loss 1.42285. lr 3.000000e-04, running loss 1.42855, it/sec: 4.427408347678471
epoch 1 iter 21760: train loss 1.42856. lr 3.000000e-04, running loss 1.42849, it/sec: 4.408627849964804
epoch 1 iter 21780: train loss 1.42591. lr 3.000000e-04, running loss 1.42846, it/sec: 4.420790288547216
epoch 1 iter 21800: train loss 1.43637. lr 3.000000e-04, running loss 1.42844, it/sec: 4.415162361444831
epoch 1 iter 21820: train loss 1.43429. lr 3.000000e-04, running loss 1.42847, it/sec: 4.398141281267145
epoch 1 iter 21840: train loss 1.43853. lr 3.000000e-04, running loss 1.42854, it/sec: 4.37341935538969
epoch 1 iter 21860: train loss 1.42698. lr 3.000000e-04, running loss 1.42855, it/sec: 4.380923333696492
epoch 1 iter 21880: train loss 1.44803. lr 3.000000e-04, running loss 1.42857, it/sec: 4.3745826318150955
epoch 1 iter 21900: train loss 1.42763. lr 3.000000e-04, running loss 1.42867, it/sec: 4.361833812509349
epoch 1 iter 21920: train loss 1.42311. lr 3.000000e-04, running loss 1.42872, it/sec: 4.382161964035587
epoch 1 iter 21940: train loss 1.40356. lr 3.000000e-04, running loss 1.42867, it/sec: 4.36115518095927
epoch 1 iter 21960: train loss 1.39562. lr 3.000000e-04, running loss 1.42866, it/sec: 4.359475287762407
epoch 1 iter 21980: train loss 1.43156. lr 3.000000e-04, running loss 1.42868, it/sec: 4.379021589627085
epoch 1 iter 22000: train loss 1.43511. lr 3.000000e-04, running loss 1.42870, it/sec: 4.353624774254949
epoch 1 iter 22020: train loss 1.42414. lr 3.000000e-04, running loss 1.42999, it/sec: 4.3575771132096115
epoch 1 iter 22040: train loss 1.41160. lr 3.000000e-04, running loss 1.42992, it/sec: 4.368027129619328
epoch 1 iter 22060: train loss 1.41487. lr 3.000000e-04, running loss 1.42993, it/sec: 4.384007799751271
epoch 1 iter 22080: train loss 1.44096. lr 3.000000e-04, running loss 1.42989, it/sec: 4.4162533798348305
epoch 1 iter 22100: train loss 1.42690. lr 3.000000e-04, running loss 1.42988, it/sec: 4.409015321782643
epoch 1 iter 22120: train loss 1.42715. lr 3.000000e-04, running loss 1.42987, it/sec: 4.4372452807457945
epoch 1 iter 22140: train loss 1.41561. lr 3.000000e-04, running loss 1.42987, it/sec: 4.417537030534901
epoch 1 iter 22160: train loss 1.41326. lr 3.000000e-04, running loss 1.42984, it/sec: 4.42734793571644
epoch 1 iter 22180: train loss 1.43103. lr 3.000000e-04, running loss 1.42980, it/sec: 4.414106545496632
epoch 1 iter 22200: train loss 1.43729. lr 3.000000e-04, running loss 1.42977, it/sec: 4.350050626606383
epoch 1 iter 22220: train loss 1.42931. lr 3.000000e-04, running loss 1.42974, it/sec: 4.349923015182701
epoch 1 iter 22240: train loss 1.43960. lr 3.000000e-04, running loss 1.42972, it/sec: 4.367711403859579
epoch 1 iter 22260: train loss 1.43226. lr 3.000000e-04, running loss 1.42973, it/sec: 4.38315990083586
epoch 1 iter 22280: train loss 1.43894. lr 3.000000e-04, running loss 1.42965, it/sec: 4.339211074551141
epoch 1 iter 22300: train loss 1.42166. lr 3.000000e-04, running loss 1.42962, it/sec: 4.352537272779451
epoch 1 iter 22320: train loss 1.43410. lr 3.000000e-04, running loss 1.42971, it/sec: 4.364529158454044
epoch 1 iter 22340: train loss 1.42157. lr 3.000000e-04, running loss 1.42964, it/sec: 4.358265762973363
epoch 1 iter 22360: train loss 1.43761. lr 3.000000e-04, running loss 1.42961, it/sec: 4.375067195451026
epoch 1 iter 22380: train loss 1.42601. lr 3.000000e-04, running loss 1.42960, it/sec: 4.360278606615256
epoch 1 iter 22400: train loss 1.43350. lr 3.000000e-04, running loss 1.42948, it/sec: 4.3545868481143035
epoch 1 iter 22420: train loss 1.46758. lr 3.000000e-04, running loss 1.42947, it/sec: 4.393282077808177
epoch 1 iter 22440: train loss 1.43022. lr 3.000000e-04, running loss 1.42940, it/sec: 4.414318739242756
epoch 1 iter 22460: train loss 1.43148. lr 3.000000e-04, running loss 1.42936, it/sec: 4.418416474866432
epoch 1 iter 22480: train loss 1.41668. lr 3.000000e-04, running loss 1.42933, it/sec: 4.429272473271869
epoch 1 iter 22500: train loss 1.42255. lr 3.000000e-04, running loss 1.42942, it/sec: 4.4389574394695295
epoch 1 iter 22520: train loss 1.44188. lr 3.000000e-04, running loss 1.42946, it/sec: 4.402774298906095
epoch 1 iter 22540: train loss 1.43299. lr 3.000000e-04, running loss 1.42946, it/sec: 4.4296047766016216
epoch 1 iter 22560: train loss 1.43638. lr 3.000000e-04, running loss 1.42947, it/sec: 4.3956959804073605
epoch 1 iter 22580: train loss 1.41342. lr 3.000000e-04, running loss 1.42942, it/sec: 4.386635586413859
epoch 1 iter 22600: train loss 1.41102. lr 3.000000e-04, running loss 1.42944, it/sec: 4.390530563445601
epoch 1 iter 22620: train loss 1.41986. lr 3.000000e-04, running loss 1.42937, it/sec: 4.385541956522022
epoch 1 iter 22640: train loss 1.42921. lr 3.000000e-04, running loss 1.42940, it/sec: 4.3551073909633455
epoch 1 iter 22660: train loss 1.43181. lr 3.000000e-04, running loss 1.42945, it/sec: 4.350148289758867
epoch 1 iter 22680: train loss 1.42481. lr 3.000000e-04, running loss 1.43000, it/sec: 4.37529332251541
epoch 1 iter 22700: train loss 1.42524. lr 3.000000e-04, running loss 1.43003, it/sec: 4.3563217279491955
epoch 1 iter 22720: train loss 1.41878. lr 3.000000e-04, running loss 1.42994, it/sec: 4.375542791808178
epoch 1 iter 22740: train loss 1.42883. lr 3.000000e-04, running loss 1.42996, it/sec: 4.415526904175291
epoch 1 iter 22760: train loss 1.42109. lr 3.000000e-04, running loss 1.42989, it/sec: 4.415430824965749
epoch 1 iter 22780: train loss 1.41672. lr 3.000000e-04, running loss 1.42985, it/sec: 4.420610242811221
epoch 1 iter 22800: train loss 1.43193. lr 3.000000e-04, running loss 1.42984, it/sec: 4.427453491144969
epoch 1 iter 22820: train loss 1.39663. lr 3.000000e-04, running loss 1.42970, it/sec: 4.4209382771980374
epoch 1 iter 22840: train loss 1.43516. lr 3.000000e-04, running loss 1.42974, it/sec: 4.386349352296676
epoch 1 iter 22860: train loss 1.43047. lr 3.000000e-04, running loss 1.42963, it/sec: 4.412111062909073
epoch 1 iter 22880: train loss 1.41850. lr 3.000000e-04, running loss 1.42957, it/sec: 4.413475421226413
epoch 1 iter 22900: train loss 1.41232. lr 3.000000e-04, running loss 1.42950, it/sec: 4.416593269937143
epoch 1 iter 22920: train loss 1.41593. lr 3.000000e-04, running loss 1.42935, it/sec: 4.436489070222588
epoch 1 iter 22940: train loss 1.42527. lr 3.000000e-04, running loss 1.42929, it/sec: 4.424127122452296
epoch 1 iter 22960: train loss 1.42739. lr 3.000000e-04, running loss 1.42934, it/sec: 4.390656000438942
epoch 1 iter 22980: train loss 1.41201. lr 3.000000e-04, running loss 1.42928, it/sec: 4.399343644273899
epoch 1 iter 23000: train loss 1.43183. lr 3.000000e-04, running loss 1.42917, it/sec: 4.375748498811254
epoch 1 iter 23020: train loss 1.42214. lr 3.000000e-04, running loss 1.42917, it/sec: 4.379256064765045
epoch 1 iter 23040: train loss 1.44694. lr 3.000000e-04, running loss 1.42919, it/sec: 4.379751236525998
epoch 1 iter 23060: train loss 1.43212. lr 3.000000e-04, running loss 1.42929, it/sec: 4.380406311832221
epoch 1 iter 23080: train loss 1.41931. lr 3.000000e-04, running loss 1.42931, it/sec: 4.362448807108225
epoch 1 iter 23100: train loss 1.40936. lr 3.000000e-04, running loss 1.42926, it/sec: 4.369983041050278
epoch 1 iter 23120: train loss 1.42091. lr 3.000000e-04, running loss 1.42923, it/sec: 4.405416721118297
epoch 1 iter 23140: train loss 1.43550. lr 3.000000e-04, running loss 1.42936, it/sec: 4.394609649041002
epoch 1 iter 23160: train loss 1.41554. lr 3.000000e-04, running loss 1.42937, it/sec: 4.4076194113819955
epoch 1 iter 23180: train loss 1.41554. lr 3.000000e-04, running loss 1.42927, it/sec: 4.377125365710972
epoch 1 iter 23200: train loss 1.42080. lr 3.000000e-04, running loss 1.42922, it/sec: 4.403694457099609
epoch 1 iter 23220: train loss 1.42499. lr 3.000000e-04, running loss 1.42913, it/sec: 4.426959666080806
epoch 1 iter 23240: train loss 1.42588. lr 3.000000e-04, running loss 1.42920, it/sec: 4.400009626720673
epoch 1 iter 23260: train loss 1.41778. lr 3.000000e-04, running loss 1.42919, it/sec: 4.36496782782444
epoch 1 iter 23280: train loss 1.49022. lr 3.000000e-04, running loss 1.42917, it/sec: 4.399821164398452
epoch 1 iter 23300: train loss 1.41638. lr 3.000000e-04, running loss 1.42912, it/sec: 4.430361506117215
epoch 1 iter 23320: train loss 1.48335. lr 3.000000e-04, running loss 1.42931, it/sec: 4.360601892878341
epoch 1 iter 23340: train loss 1.41456. lr 3.000000e-04, running loss 1.42925, it/sec: 4.358783348363102
epoch 1 iter 23360: train loss 1.40782. lr 3.000000e-04, running loss 1.42915, it/sec: 4.371077920989515
epoch 1 iter 23380: train loss 1.48854. lr 3.000000e-04, running loss 1.42910, it/sec: 4.369029251431526
epoch 1 iter 23400: train loss 1.43840. lr 3.000000e-04, running loss 1.42904, it/sec: 4.363404199395646
epoch 1 iter 23420: train loss 1.44214. lr 3.000000e-04, running loss 1.42899, it/sec: 4.380623069968486
epoch 1 iter 23440: train loss 1.42675. lr 3.000000e-04, running loss 1.42892, it/sec: 4.405104336850934
epoch 1 iter 23460: train loss 1.43999. lr 3.000000e-04, running loss 1.42908, it/sec: 4.384530689734045
epoch 1 iter 23480: train loss 1.42699. lr 3.000000e-04, running loss 1.42902, it/sec: 4.379945790991839
epoch 1 iter 23500: train loss 1.43859. lr 3.000000e-04, running loss 1.42899, it/sec: 4.335607082978658
epoch 1 iter 23520: train loss 1.42669. lr 3.000000e-04, running loss 1.42886, it/sec: 4.3566300785845105
epoch 1 iter 23540: train loss 1.43123. lr 3.000000e-04, running loss 1.42887, it/sec: 4.320448942022449
epoch 1 iter 23560: train loss 1.42652. lr 3.000000e-04, running loss 1.42879, it/sec: 4.370076693440773
epoch 1 iter 23580: train loss 1.44570. lr 3.000000e-04, running loss 1.42889, it/sec: 4.38550664644624
epoch 1 iter 23600: train loss 1.42952. lr 3.000000e-04, running loss 1.42891, it/sec: 4.40919041946451
epoch 1 iter 23620: train loss 1.43687. lr 3.000000e-04, running loss 1.42893, it/sec: 4.410256565941587
epoch 1 iter 23640: train loss 1.41828. lr 3.000000e-04, running loss 1.42930, it/sec: 4.428288139623762
epoch 1 iter 23660: train loss 1.42500. lr 3.000000e-04, running loss 1.42924, it/sec: 4.431181857312045
epoch 1 iter 23680: train loss 1.44035. lr 3.000000e-04, running loss 1.42932, it/sec: 4.444888794581326
epoch 1 iter 23700: train loss 1.43205. lr 3.000000e-04, running loss 1.42928, it/sec: 4.4213101053273824
epoch 1 iter 23720: train loss 1.41741. lr 3.000000e-04, running loss 1.42928, it/sec: 4.405081032791584
epoch 1 iter 23740: train loss 1.42728. lr 3.000000e-04, running loss 1.42928, it/sec: 4.3922984140386205
epoch 1 iter 23760: train loss 1.42733. lr 3.000000e-04, running loss 1.42917, it/sec: 4.395628470396691
epoch 1 iter 23780: train loss 1.41837. lr 3.000000e-04, running loss 1.42914, it/sec: 4.396878292761937
epoch 1 iter 23800: train loss 1.42544. lr 3.000000e-04, running loss 1.42909, it/sec: 4.421143328959866
epoch 1 iter 23820: train loss 1.43713. lr 3.000000e-04, running loss 1.42910, it/sec: 4.425325249129169
epoch 1 iter 23840: train loss 1.40602. lr 3.000000e-04, running loss 1.42908, it/sec: 4.392771183412509
epoch 1 iter 23860: train loss 1.44885. lr 3.000000e-04, running loss 1.42902, it/sec: 4.399654592319307
epoch 1 iter 23880: train loss 1.42385. lr 3.000000e-04, running loss 1.42899, it/sec: 4.371103982900641
epoch 1 iter 23900: train loss 1.42642. lr 3.000000e-04, running loss 1.42895, it/sec: 4.352202566843183
epoch 1 iter 23920: train loss 1.41613. lr 3.000000e-04, running loss 1.42892, it/sec: 4.384513600578659
epoch 1 iter 23940: train loss 1.41600. lr 3.000000e-04, running loss 1.42890, it/sec: 4.345138449051099
epoch 1 iter 23960: train loss 1.42865. lr 3.000000e-04, running loss 1.42893, it/sec: 4.383688028117003
epoch 1 iter 23980: train loss 1.41661. lr 3.000000e-04, running loss 1.42890, it/sec: 4.393874425399197
epoch 1 iter 24000: train loss 1.42417. lr 3.000000e-04, running loss 1.42886, it/sec: 4.384673798735242
epoch 1 iter 24020: train loss 1.41901. lr 3.000000e-04, running loss 1.42882, it/sec: 4.396192576624862
epoch 1 iter 24040: train loss 1.44377. lr 3.000000e-04, running loss 1.42884, it/sec: 4.412430827759354
epoch 1 iter 24060: train loss 1.42424. lr 3.000000e-04, running loss 1.42880, it/sec: 4.412775796962476
epoch 1 iter 24080: train loss 1.42526. lr 3.000000e-04, running loss 1.42882, it/sec: 4.417769656032427
epoch 1 iter 24100: train loss 1.42496. lr 3.000000e-04, running loss 1.42873, it/sec: 4.436528001526132
epoch 1 iter 24120: train loss 1.42952. lr 3.000000e-04, running loss 1.42873, it/sec: 4.419392752264946
epoch 1 iter 24140: train loss 1.43985. lr 3.000000e-04, running loss 1.42864, it/sec: 4.417652169613777
epoch 1 iter 24160: train loss 1.42209. lr 3.000000e-04, running loss 1.42862, it/sec: 4.403565965768205
epoch 1 iter 24180: train loss 1.41738. lr 3.000000e-04, running loss 1.42855, it/sec: 4.413536020981405
epoch 1 iter 24200: train loss 1.42740. lr 3.000000e-04, running loss 1.42857, it/sec: 4.403544945573689
epoch 1 iter 24220: train loss 1.42914. lr 3.000000e-04, running loss 1.42893, it/sec: 4.363394240837556
epoch 1 iter 24240: train loss 1.45452. lr 3.000000e-04, running loss 1.42891, it/sec: 4.385402099893648
epoch 1 iter 24260: train loss 1.42027. lr 3.000000e-04, running loss 1.42889, it/sec: 4.3724862254207535
epoch 1 iter 24280: train loss 1.43025. lr 3.000000e-04, running loss 1.42889, it/sec: 4.368637096684247
epoch 1 iter 24300: train loss 1.43843. lr 3.000000e-04, running loss 1.42877, it/sec: 4.359638394282342
epoch 1 iter 24320: train loss 1.43860. lr 3.000000e-04, running loss 1.42876, it/sec: 4.371913487072307
epoch 1 iter 24340: train loss 1.42351. lr 3.000000e-04, running loss 1.42885, it/sec: 4.340738717850885
epoch 1 iter 24360: train loss 1.41995. lr 3.000000e-04, running loss 1.42885, it/sec: 4.354950672895371
epoch 1 iter 24380: train loss 1.43781. lr 3.000000e-04, running loss 1.42878, it/sec: 4.380062163677008
epoch 1 iter 24400: train loss 1.42123. lr 3.000000e-04, running loss 1.42883, it/sec: 4.367251640989632
epoch 1 iter 24420: train loss 1.42192. lr 3.000000e-04, running loss 1.42899, it/sec: 4.371479305991068
epoch 1 iter 24440: train loss 1.43842. lr 3.000000e-04, running loss 1.42947, it/sec: 4.3619066829718625
epoch 1 iter 24460: train loss 1.43573. lr 3.000000e-04, running loss 1.42941, it/sec: 4.351043343551458
epoch 1 iter 24480: train loss 1.43729. lr 3.000000e-04, running loss 1.42929, it/sec: 4.378191701747991
epoch 1 iter 24500: train loss 1.42021. lr 3.000000e-04, running loss 1.42916, it/sec: 4.3854817211731305
epoch 1 iter 24520: train loss 1.43064. lr 3.000000e-04, running loss 1.42911, it/sec: 4.412698257608219
epoch 1 iter 24540: train loss 1.43802. lr 3.000000e-04, running loss 1.42915, it/sec: 4.422638047504054
epoch 1 iter 24560: train loss 1.42497. lr 3.000000e-04, running loss 1.42941, it/sec: 4.420121459348143
epoch 1 iter 24580: train loss 1.42351. lr 3.000000e-04, running loss 1.42935, it/sec: 4.435033775413078
epoch 1 iter 24600: train loss 1.43197. lr 3.000000e-04, running loss 1.42928, it/sec: 4.411706523329234
epoch 1 iter 24620: train loss 1.43146. lr 3.000000e-04, running loss 1.42919, it/sec: 4.427167394237182
epoch 1 iter 24640: train loss 1.42844. lr 3.000000e-04, running loss 1.42922, it/sec: 4.408945029303654
epoch 1 iter 24660: train loss 1.42918. lr 3.000000e-04, running loss 1.42940, it/sec: 4.4162888757041925
epoch 1 iter 24680: train loss 1.42602. lr 3.000000e-04, running loss 1.42941, it/sec: 4.419832694413182
epoch 1 iter 24700: train loss 1.43203. lr 3.000000e-04, running loss 1.42940, it/sec: 4.429739127747278
epoch 1 iter 24720: train loss 1.53451. lr 3.000000e-04, running loss 1.42945, it/sec: 4.413120448279043
epoch 1 iter 24740: train loss 1.44153. lr 3.000000e-04, running loss 1.42949, it/sec: 4.4346692512040145
epoch 1 iter 24760: train loss 1.43826. lr 3.000000e-04, running loss 1.42951, it/sec: 4.429050952856043
epoch 1 iter 24780: train loss 1.42953. lr 3.000000e-04, running loss 1.42942, it/sec: 4.420027348692666
epoch 1 iter 24800: train loss 1.42551. lr 3.000000e-04, running loss 1.42935, it/sec: 4.405584041617267
epoch 1 iter 24820: train loss 1.44462. lr 3.000000e-04, running loss 1.42927, it/sec: 4.384864850220852
epoch 1 iter 24840: train loss 1.42307. lr 3.000000e-04, running loss 1.42922, it/sec: 4.407868714051272
epoch 1 iter 24860: train loss 1.42929. lr 3.000000e-04, running loss 1.42928, it/sec: 4.374621996933325
epoch 1 iter 24880: train loss 1.48447. lr 3.000000e-04, running loss 1.42923, it/sec: 4.370427258020613
epoch 1 iter 24900: train loss 1.43638. lr 3.000000e-04, running loss 1.42926, it/sec: 4.357162880081912
epoch 1 iter 24920: train loss 1.41957. lr 3.000000e-04, running loss 1.42921, it/sec: 4.37423694897373
epoch 1 iter 24940: train loss 1.43199. lr 3.000000e-04, running loss 1.42915, it/sec: 4.34452819323039
epoch 1 iter 24960: train loss 1.42415. lr 3.000000e-04, running loss 1.42914, it/sec: 4.392268028106545
epoch 1 iter 24980: train loss 1.43774. lr 3.000000e-04, running loss 1.42910, it/sec: 4.40894856746591
epoch 1 iter 25000: train loss 1.42578. lr 3.000000e-04, running loss 1.42906, it/sec: 1.1806837554279785
epoch 1 iter 25020: train loss 1.42476. lr 3.000000e-04, running loss 1.42912, it/sec: 4.436340512923489
epoch 1 iter 25040: train loss 1.44140. lr 3.000000e-04, running loss 1.42914, it/sec: 4.433828518015404
epoch 1 iter 25060: train loss 1.45164. lr 3.000000e-04, running loss 1.42914, it/sec: 4.432616765284364
epoch 1 iter 25080: train loss 1.43919. lr 3.000000e-04, running loss 1.42910, it/sec: 4.44137934014086
epoch 1 iter 25100: train loss 1.43202. lr 3.000000e-04, running loss 1.42914, it/sec: 4.420622006683152
epoch 1 iter 25120: train loss 1.43067. lr 3.000000e-04, running loss 1.42918, it/sec: 4.408749814598854
epoch 1 iter 25140: train loss 1.42444. lr 3.000000e-04, running loss 1.42910, it/sec: 4.3878698062416595
epoch 1 iter 25160: train loss 1.42414. lr 3.000000e-04, running loss 1.42906, it/sec: 4.419169231591225
epoch 1 iter 25180: train loss 1.42072. lr 3.000000e-04, running loss 1.42892, it/sec: 4.355689715876881
epoch 1 iter 25200: train loss 1.43370. lr 3.000000e-04, running loss 1.42886, it/sec: 4.3389572017974585
epoch 1 iter 25220: train loss 1.44128. lr 3.000000e-04, running loss 1.42893, it/sec: 4.343528543436679
epoch 1 iter 25240: train loss 1.43664. lr 3.000000e-04, running loss 1.42881, it/sec: 4.327883257723995
epoch 1 iter 25260: train loss 1.41527. lr 3.000000e-04, running loss 1.42871, it/sec: 4.359025712529314
epoch 1 iter 25280: train loss 1.44533. lr 3.000000e-04, running loss 1.42866, it/sec: 4.435621126923398
epoch 1 iter 25300: train loss 1.40578. lr 3.000000e-04, running loss 1.42863, it/sec: 4.388340409836878
epoch 1 iter 25320: train loss 1.43573. lr 3.000000e-04, running loss 1.42869, it/sec: 4.37589430973701
epoch 1 iter 25340: train loss 1.43048. lr 3.000000e-04, running loss 1.42871, it/sec: 4.379402665504625
epoch 1 iter 25360: train loss 1.42682. lr 3.000000e-04, running loss 1.42865, it/sec: 4.352424631771451
epoch 1 iter 25380: train loss 1.43014. lr 3.000000e-04, running loss 1.42866, it/sec: 4.328508986805536
epoch 1 iter 25400: train loss 1.43625. lr 3.000000e-04, running loss 1.42866, it/sec: 4.362325013175662
epoch 1 iter 25420: train loss 1.41935. lr 3.000000e-04, running loss 1.42865, it/sec: 4.3807463105727695
epoch 1 iter 25440: train loss 1.43065. lr 3.000000e-04, running loss 1.42869, it/sec: 4.374600907951411
epoch 1 iter 25460: train loss 1.44545. lr 3.000000e-04, running loss 1.42868, it/sec: 4.421148626360614
epoch 1 iter 25480: train loss 1.44176. lr 3.000000e-04, running loss 1.42866, it/sec: 4.41525571923839
epoch 1 iter 25500: train loss 1.42001. lr 3.000000e-04, running loss 1.42862, it/sec: 4.426765380568989
epoch 1 iter 25520: train loss 1.43346. lr 3.000000e-04, running loss 1.42857, it/sec: 4.4191490977314976
epoch 1 iter 25540: train loss 1.43312. lr 3.000000e-04, running loss 1.42857, it/sec: 4.43580629272424
epoch 1 iter 25560: train loss 1.42212. lr 3.000000e-04, running loss 1.42855, it/sec: 4.415959913397616
epoch 1 iter 25580: train loss 1.43785. lr 3.000000e-04, running loss 1.42856, it/sec: 4.414843331695308
epoch 1 iter 25600: train loss 1.43346. lr 3.000000e-04, running loss 1.42854, it/sec: 4.418316406436264
epoch 1 iter 25620: train loss 1.44234. lr 3.000000e-04, running loss 1.42856, it/sec: 4.420453875255841
epoch 1 iter 25640: train loss 1.43404. lr 3.000000e-04, running loss 1.42857, it/sec: 4.424340829920294
epoch 1 iter 25660: train loss 1.43703. lr 3.000000e-04, running loss 1.42863, it/sec: 4.437439325061806
epoch 1 iter 25680: train loss 1.44032. lr 3.000000e-04, running loss 1.42858, it/sec: 4.428012287500843
epoch 1 iter 25700: train loss 1.43037. lr 3.000000e-04, running loss 1.42858, it/sec: 4.4319486142405955
epoch 1 iter 25720: train loss 1.42645. lr 3.000000e-04, running loss 1.42864, it/sec: 4.41691425035026
epoch 1 iter 25740: train loss 1.42904. lr 3.000000e-04, running loss 1.42866, it/sec: 4.401171539506184
epoch 1 iter 25760: train loss 1.46255. lr 3.000000e-04, running loss 1.42868, it/sec: 4.407661684729289
epoch 1 iter 25780: train loss 1.42098. lr 3.000000e-04, running loss 1.42864, it/sec: 4.398936101265376
epoch 1 iter 25800: train loss 1.42664. lr 3.000000e-04, running loss 1.42862, it/sec: 4.362708880771867
epoch 1 iter 25820: train loss 1.44981. lr 3.000000e-04, running loss 1.42871, it/sec: 4.4050349653043535
epoch 1 iter 25840: train loss 1.41174. lr 3.000000e-04, running loss 1.42868, it/sec: 4.401272227603372
epoch 1 iter 25860: train loss 1.42127. lr 3.000000e-04, running loss 1.42861, it/sec: 4.43757948873171
epoch 1 iter 25880: train loss 1.43487. lr 3.000000e-04, running loss 1.42863, it/sec: 4.425283009029815
epoch 1 iter 25900: train loss 1.42282. lr 3.000000e-04, running loss 1.42874, it/sec: 4.4268554072163795
epoch 1 iter 25920: train loss 1.41077. lr 3.000000e-04, running loss 1.42864, it/sec: 4.369711349557369
epoch 1 iter 25940: train loss 1.43318. lr 3.000000e-04, running loss 1.42860, it/sec: 4.399135364818284
epoch 1 iter 25960: train loss 1.42813. lr 3.000000e-04, running loss 1.42866, it/sec: 4.391349091339728
epoch 1 iter 25980: train loss 1.43513. lr 3.000000e-04, running loss 1.42857, it/sec: 4.3706452445909845
epoch 1 iter 26000: train loss 1.43011. lr 3.000000e-04, running loss 1.42859, it/sec: 4.359420610420669
epoch 1 iter 26020: train loss 1.41767. lr 3.000000e-04, running loss 1.42856, it/sec: 4.3606073494578546
epoch 1 iter 26040: train loss 1.43600. lr 3.000000e-04, running loss 1.42857, it/sec: 4.365145351500713
epoch 1 iter 26060: train loss 1.42324. lr 3.000000e-04, running loss 1.42852, it/sec: 4.376287596848842
epoch 1 iter 26080: train loss 1.42780. lr 3.000000e-04, running loss 1.42851, it/sec: 4.378047021831236
epoch 1 iter 26100: train loss 1.41330. lr 3.000000e-04, running loss 1.42857, it/sec: 4.40338507089179
epoch 1 iter 26120: train loss 1.43177. lr 3.000000e-04, running loss 1.42853, it/sec: 4.405038302922759
epoch 1 iter 26140: train loss 1.42535. lr 3.000000e-04, running loss 1.42855, it/sec: 4.379535562756745
epoch 1 iter 26160: train loss 1.42376. lr 3.000000e-04, running loss 1.42852, it/sec: 4.360563672854726
epoch 1 iter 26180: train loss 1.40105. lr 3.000000e-04, running loss 1.42855, it/sec: 4.351297040116256
epoch 1 iter 26200: train loss 1.44132. lr 3.000000e-04, running loss 1.42857, it/sec: 4.324382013812927
epoch 1 iter 26220: train loss 1.42819. lr 3.000000e-04, running loss 1.42860, it/sec: 4.392707930160654
epoch 1 iter 26240: train loss 1.42379. lr 3.000000e-04, running loss 1.42863, it/sec: 4.404288646121328
epoch 1 iter 26260: train loss 1.42570. lr 3.000000e-04, running loss 1.42867, it/sec: 4.411958215864319
epoch 1 iter 26280: train loss 1.43147. lr 3.000000e-04, running loss 1.42862, it/sec: 4.410699593000366
epoch 1 iter 26300: train loss 1.42317. lr 3.000000e-04, running loss 1.42862, it/sec: 4.425155290525922
epoch 1 iter 26320: train loss 1.44351. lr 3.000000e-04, running loss 1.42861, it/sec: 4.431926320664709
epoch 1 iter 26340: train loss 1.43505. lr 3.000000e-04, running loss 1.42866, it/sec: 4.421680845456193
epoch 1 iter 26360: train loss 1.42522. lr 3.000000e-04, running loss 1.42869, it/sec: 4.4051534894356115
epoch 1 iter 26380: train loss 1.40986. lr 3.000000e-04, running loss 1.42869, it/sec: 4.388817778670708
epoch 1 iter 26400: train loss 1.43052. lr 3.000000e-04, running loss 1.42869, it/sec: 4.409364364269256
epoch 1 iter 26420: train loss 1.42329. lr 3.000000e-04, running loss 1.42873, it/sec: 4.424985894329256
epoch 1 iter 26440: train loss 1.41970. lr 3.000000e-04, running loss 1.42874, it/sec: 4.394277479266625
epoch 1 iter 26460: train loss 1.41832. lr 3.000000e-04, running loss 1.42871, it/sec: 4.412075264777214
epoch 1 iter 26480: train loss 1.42430. lr 3.000000e-04, running loss 1.42876, it/sec: 4.4362711770638725
epoch 1 iter 26500: train loss 1.47680. lr 3.000000e-04, running loss 1.42883, it/sec: 4.420514490498557
epoch 1 iter 26520: train loss 1.42787. lr 3.000000e-04, running loss 1.42879, it/sec: 4.412817876840874
epoch 1 iter 26540: train loss 1.44096. lr 3.000000e-04, running loss 1.42881, it/sec: 4.408471979382979
epoch 1 iter 26560: train loss 1.43283. lr 3.000000e-04, running loss 1.42876, it/sec: 4.395982875532741
epoch 1 iter 26580: train loss 1.41853. lr 3.000000e-04, running loss 1.42872, it/sec: 4.375607370445928
epoch 1 iter 26600: train loss 1.42965. lr 3.000000e-04, running loss 1.42866, it/sec: 4.403984451960567
epoch 1 iter 26620: train loss 1.43426. lr 3.000000e-04, running loss 1.42864, it/sec: 4.377991513936314
epoch 1 iter 26640: train loss 1.42256. lr 3.000000e-04, running loss 1.42865, it/sec: 4.368308268335037
epoch 1 iter 26660: train loss 1.42676. lr 3.000000e-04, running loss 1.42870, it/sec: 4.370027461240369
epoch 1 iter 26680: train loss 1.43202. lr 3.000000e-04, running loss 1.42863, it/sec: 4.377543918688436
epoch 1 iter 26700: train loss 1.42826. lr 3.000000e-04, running loss 1.42855, it/sec: 4.361454703900676
epoch 1 iter 26720: train loss 1.43647. lr 3.000000e-04, running loss 1.42842, it/sec: 4.38629401892051
epoch 1 iter 26740: train loss 1.42143. lr 3.000000e-04, running loss 1.42843, it/sec: 4.355940085706548
epoch 1 iter 26760: train loss 1.42701. lr 3.000000e-04, running loss 1.42844, it/sec: 4.3735750729497065
epoch 1 iter 26780: train loss 1.43285. lr 3.000000e-04, running loss 1.42849, it/sec: 4.379653466697812
epoch 1 iter 26800: train loss 1.45857. lr 3.000000e-04, running loss 1.42853, it/sec: 4.376409922490288
epoch 1 iter 26820: train loss 1.41201. lr 3.000000e-04, running loss 1.42856, it/sec: 4.337754481101648
epoch 1 iter 26840: train loss 1.42721. lr 3.000000e-04, running loss 1.42861, it/sec: 4.3371960715869085
epoch 1 iter 26860: train loss 1.40505. lr 3.000000e-04, running loss 1.42853, it/sec: 4.361652525127759
epoch 1 iter 26880: train loss 1.43277. lr 3.000000e-04, running loss 1.42853, it/sec: 4.3923274112723005
epoch 1 iter 26900: train loss 1.42744. lr 3.000000e-04, running loss 1.42853, it/sec: 4.410715156590511
epoch 1 iter 26920: train loss 1.42932. lr 3.000000e-04, running loss 1.42854, it/sec: 4.414064303809679
epoch 1 iter 26940: train loss 1.40830. lr 3.000000e-04, running loss 1.42857, it/sec: 4.398440046079801
epoch 1 iter 26960: train loss 1.44321. lr 3.000000e-04, running loss 1.42864, it/sec: 4.426893974564312
epoch 1 iter 26980: train loss 1.42637. lr 3.000000e-04, running loss 1.42888, it/sec: 4.420814366587219
epoch 1 iter 27000: train loss 1.41384. lr 3.000000e-04, running loss 1.42889, it/sec: 4.415207139786631
epoch 1 iter 27020: train loss 1.42599. lr 3.000000e-04, running loss 1.42894, it/sec: 4.409025158133516
epoch 1 iter 27040: train loss 1.42286. lr 3.000000e-04, running loss 1.42895, it/sec: 4.3976802368309835
epoch 1 iter 27060: train loss 1.42328. lr 3.000000e-04, running loss 1.42897, it/sec: 4.392630151015996
epoch 1 iter 27080: train loss 1.43393. lr 3.000000e-04, running loss 1.42914, it/sec: 4.418426529344909
epoch 1 iter 27100: train loss 1.43853. lr 3.000000e-04, running loss 1.42913, it/sec: 4.402355403022625
epoch 1 iter 27120: train loss 1.43028. lr 3.000000e-04, running loss 1.42911, it/sec: 4.425662953837771
epoch 1 iter 27140: train loss 1.41703. lr 3.000000e-04, running loss 1.42904, it/sec: 4.417831447340042
epoch 1 iter 27160: train loss 1.42790. lr 3.000000e-04, running loss 1.42904, it/sec: 4.42276125766329
epoch 1 iter 27180: train loss 1.43817. lr 3.000000e-04, running loss 1.42911, it/sec: 4.39985242792833
epoch 1 iter 27200: train loss 1.42676. lr 3.000000e-04, running loss 1.42913, it/sec: 4.36566882235446
epoch 1 iter 27220: train loss 1.41476. lr 3.000000e-04, running loss 1.42901, it/sec: 4.369278179127999
epoch 1 iter 27240: train loss 1.43427. lr 3.000000e-04, running loss 1.42900, it/sec: 4.388925396626983
epoch 1 iter 27260: train loss 1.43304. lr 3.000000e-04, running loss 1.42893, it/sec: 4.388044940949397
epoch 1 iter 27280: train loss 1.42615. lr 3.000000e-04, running loss 1.42903, it/sec: 4.380884622559992
epoch 1 iter 27300: train loss 1.42832. lr 3.000000e-04, running loss 1.42906, it/sec: 4.359817043650508
epoch 1 iter 27320: train loss 1.42888. lr 3.000000e-04, running loss 1.42903, it/sec: 4.381541419278398
epoch 1 iter 27340: train loss 1.40963. lr 3.000000e-04, running loss 1.42896, it/sec: 4.351231758712107
epoch 1 iter 27360: train loss 1.43229. lr 3.000000e-04, running loss 1.42894, it/sec: 4.356017958175239
epoch 1 iter 27380: train loss 1.42753. lr 3.000000e-04, running loss 1.42901, it/sec: 4.3586349522405525
epoch 1 iter 27400: train loss 1.42129. lr 3.000000e-04, running loss 1.42908, it/sec: 4.376848129979206
epoch 1 iter 27420: train loss 1.43051. lr 3.000000e-04, running loss 1.42913, it/sec: 4.349427299432669
epoch 1 iter 27440: train loss 1.42796. lr 3.000000e-04, running loss 1.42912, it/sec: 4.354341185199948
epoch 1 iter 27460: train loss 1.42611. lr 3.000000e-04, running loss 1.42908, it/sec: 4.418036856332553
epoch 1 iter 27480: train loss 1.42934. lr 3.000000e-04, running loss 1.42904, it/sec: 4.35929583015312
epoch 1 iter 27500: train loss 1.42067. lr 3.000000e-04, running loss 1.42904, it/sec: 4.399474598860604
epoch 1 iter 27520: train loss 1.44092. lr 3.000000e-04, running loss 1.42908, it/sec: 4.424438705650218
epoch 1 iter 27540: train loss 1.44243. lr 3.000000e-04, running loss 1.42904, it/sec: 4.439912063620882
epoch 1 iter 27560: train loss 1.44206. lr 3.000000e-04, running loss 1.42903, it/sec: 4.408004469953778
epoch 1 iter 27580: train loss 1.42556. lr 3.000000e-04, running loss 1.42904, it/sec: 4.4157931102410375
epoch 1 iter 27600: train loss 1.43293. lr 3.000000e-04, running loss 1.42905, it/sec: 4.416919555790648
epoch 1 iter 27620: train loss 1.42323. lr 3.000000e-04, running loss 1.42897, it/sec: 4.4350580088590865
epoch 1 iter 27640: train loss 1.44051. lr 3.000000e-04, running loss 1.42896, it/sec: 4.416186269940671
epoch 1 iter 27660: train loss 1.40639. lr 3.000000e-04, running loss 1.42887, it/sec: 4.389699680755842
epoch 1 iter 27680: train loss 1.42035. lr 3.000000e-04, running loss 1.42886, it/sec: 4.410634286909432
epoch 1 iter 27700: train loss 1.44688. lr 3.000000e-04, running loss 1.42873, it/sec: 4.418757421990397
epoch 1 iter 27720: train loss 1.42569. lr 3.000000e-04, running loss 1.42868, it/sec: 4.401535808686182
epoch 1 iter 27740: train loss 1.43289. lr 3.000000e-04, running loss 1.42883, it/sec: 4.372420762936686
epoch 1 iter 27760: train loss 1.41367. lr 3.000000e-04, running loss 1.42879, it/sec: 4.376312551495921
epoch 1 iter 27780: train loss 1.42081. lr 3.000000e-04, running loss 1.42877, it/sec: 4.376728230633765
epoch 1 iter 27800: train loss 1.41119. lr 3.000000e-04, running loss 1.42869, it/sec: 4.364803235524516
epoch 1 iter 27820: train loss 1.42063. lr 3.000000e-04, running loss 1.42869, it/sec: 4.394417169697721
epoch 1 iter 27840: train loss 1.42706. lr 3.000000e-04, running loss 1.42865, it/sec: 4.423256925238042
epoch 1 iter 27860: train loss 1.44483. lr 3.000000e-04, running loss 1.42863, it/sec: 4.384447969926447
epoch 1 iter 27880: train loss 1.44768. lr 3.000000e-04, running loss 1.42857, it/sec: 4.402695619045632
epoch 1 iter 27900: train loss 1.43854. lr 3.000000e-04, running loss 1.42853, it/sec: 4.400637254831946
epoch 1 iter 27920: train loss 1.41940. lr 3.000000e-04, running loss 1.42857, it/sec: 4.364752082770118
epoch 1 iter 27940: train loss 1.41573. lr 3.000000e-04, running loss 1.42865, it/sec: 4.360757628803211
epoch 1 iter 27960: train loss 1.42891. lr 3.000000e-04, running loss 1.42866, it/sec: 4.371481025667192
epoch 1 iter 27980: train loss 1.43109. lr 3.000000e-04, running loss 1.42858, it/sec: 4.403332758156641
epoch 1 iter 28000: train loss 1.42436. lr 3.000000e-04, running loss 1.42862, it/sec: 4.407150626582369
epoch 1 iter 28020: train loss 1.41244. lr 3.000000e-04, running loss 1.42855, it/sec: 4.422444767258084
epoch 1 iter 28040: train loss 1.42262. lr 3.000000e-04, running loss 1.42850, it/sec: 4.441757832593934
epoch 1 iter 28060: train loss 1.43776. lr 3.000000e-04, running loss 1.42847, it/sec: 4.42358181022482
epoch 1 iter 28080: train loss 1.43085. lr 3.000000e-04, running loss 1.42840, it/sec: 4.432568490379345
epoch 1 iter 28100: train loss 1.43131. lr 3.000000e-04, running loss 1.42837, it/sec: 4.431666707742442
epoch 1 iter 28120: train loss 1.42546. lr 3.000000e-04, running loss 1.42841, it/sec: 4.426573717131589
epoch 1 iter 28140: train loss 1.43663. lr 3.000000e-04, running loss 1.42839, it/sec: 4.369942536662218
epoch 1 iter 28160: train loss 1.43700. lr 3.000000e-04, running loss 1.42838, it/sec: 4.376912152802854
epoch 1 iter 28180: train loss 1.43766. lr 3.000000e-04, running loss 1.42834, it/sec: 4.380720459731462
epoch 1 iter 28200: train loss 1.41394. lr 3.000000e-04, running loss 1.42825, it/sec: 4.372477870150891
epoch 1 iter 28220: train loss 1.43100. lr 3.000000e-04, running loss 1.42826, it/sec: 4.409818685353678
epoch 1 iter 28240: train loss 1.42930. lr 3.000000e-04, running loss 1.42823, it/sec: 4.431702845076092
epoch 1 iter 28260: train loss 1.42683. lr 3.000000e-04, running loss 1.42826, it/sec: 4.400515525599969
epoch 1 iter 28280: train loss 1.41538. lr 3.000000e-04, running loss 1.42826, it/sec: 4.392757869033439
epoch 1 iter 28300: train loss 1.39406. lr 3.000000e-04, running loss 1.42824, it/sec: 4.3427822854607205
epoch 1 iter 28320: train loss 1.42469. lr 3.000000e-04, running loss 1.42820, it/sec: 4.3582284584239135
epoch 1 iter 28340: train loss 1.43027. lr 3.000000e-04, running loss 1.42824, it/sec: 4.380496304555856
epoch 1 iter 28360: train loss 1.40501. lr 3.000000e-04, running loss 1.42823, it/sec: 4.357587689610858
epoch 1 iter 28380: train loss 1.43416. lr 3.000000e-04, running loss 1.42822, it/sec: 4.427789933382213
epoch 1 iter 28400: train loss 1.43323. lr 3.000000e-04, running loss 1.42813, it/sec: 4.4093227191568705
epoch 1 iter 28420: train loss 1.43466. lr 3.000000e-04, running loss 1.42804, it/sec: 4.437192316255981
epoch 1 iter 28440: train loss 1.41925. lr 3.000000e-04, running loss 1.42807, it/sec: 4.434981062915192
epoch 1 iter 28460: train loss 1.44041. lr 3.000000e-04, running loss 1.42809, it/sec: 4.430582608050916
epoch 1 iter 28480: train loss 1.42867. lr 3.000000e-04, running loss 1.42810, it/sec: 4.388759262837219
epoch 1 iter 28500: train loss 1.43058. lr 3.000000e-04, running loss 1.42804, it/sec: 4.418253957296832
epoch 1 iter 28520: train loss 1.43865. lr 3.000000e-04, running loss 1.42800, it/sec: 4.427890666494253
epoch 1 iter 28540: train loss 1.41210. lr 3.000000e-04, running loss 1.42795, it/sec: 4.412015133426783
epoch 1 iter 28560: train loss 1.40423. lr 3.000000e-04, running loss 1.42793, it/sec: 4.415990256838599
epoch 1 iter 28580: train loss 1.41534. lr 3.000000e-04, running loss 1.42790, it/sec: 4.43176343653558
epoch 1 iter 28600: train loss 1.43635. lr 3.000000e-04, running loss 1.42791, it/sec: 4.434436218412282
epoch 1 iter 28620: train loss 1.42558. lr 3.000000e-04, running loss 1.42797, it/sec: 4.419139977762703
epoch 1 iter 28640: train loss 1.41886. lr 3.000000e-04, running loss 1.42794, it/sec: 4.415378303931467
epoch 1 iter 28660: train loss 1.42262. lr 3.000000e-04, running loss 1.42795, it/sec: 4.379255277773862
epoch 1 iter 28680: train loss 1.42150. lr 3.000000e-04, running loss 1.42795, it/sec: 4.380389771926381
epoch 1 iter 28700: train loss 1.42158. lr 3.000000e-04, running loss 1.42797, it/sec: 4.360812587638342
epoch 1 iter 28720: train loss 1.44224. lr 3.000000e-04, running loss 1.42796, it/sec: 4.382077893933045
epoch 1 iter 28740: train loss 1.41543. lr 3.000000e-04, running loss 1.42795, it/sec: 4.37525061479278
epoch 1 iter 28760: train loss 1.41687. lr 3.000000e-04, running loss 1.42793, it/sec: 4.340501113502014
epoch 1 iter 28780: train loss 1.42604. lr 3.000000e-04, running loss 1.42795, it/sec: 4.370355192122202
epoch 1 iter 28800: train loss 1.43917. lr 3.000000e-04, running loss 1.42795, it/sec: 4.373122949064175
epoch 1 iter 28820: train loss 1.42314. lr 3.000000e-04, running loss 1.42786, it/sec: 4.401354711838889
epoch 1 iter 28840: train loss 1.41938. lr 3.000000e-04, running loss 1.42786, it/sec: 4.355566458740879
epoch 1 iter 28860: train loss 1.43451. lr 3.000000e-04, running loss 1.42790, it/sec: 4.337994776264581
epoch 1 iter 28880: train loss 1.43659. lr 3.000000e-04, running loss 1.42795, it/sec: 4.376298570838615
epoch 1 iter 28900: train loss 1.41579. lr 3.000000e-04, running loss 1.42793, it/sec: 4.375478042348551
epoch 1 iter 28920: train loss 1.41674. lr 3.000000e-04, running loss 1.42796, it/sec: 4.346660878732436
epoch 1 iter 28940: train loss 1.42958. lr 3.000000e-04, running loss 1.42801, it/sec: 4.3460366535288655
epoch 1 iter 28960: train loss 1.43435. lr 3.000000e-04, running loss 1.42801, it/sec: 4.363672707501763
epoch 1 iter 28980: train loss 1.42081. lr 3.000000e-04, running loss 1.42800, it/sec: 4.383848686118469
epoch 1 iter 29000: train loss 1.41176. lr 3.000000e-04, running loss 1.42801, it/sec: 4.408367772773472
epoch 1 iter 29020: train loss 1.43931. lr 3.000000e-04, running loss 1.42805, it/sec: 4.392931099297502
epoch 1 iter 29040: train loss 1.42902. lr 3.000000e-04, running loss 1.42802, it/sec: 4.39253940635913
epoch 1 iter 29060: train loss 1.41287. lr 3.000000e-04, running loss 1.42798, it/sec: 4.430547137982005
epoch 1 iter 29080: train loss 1.42800. lr 3.000000e-04, running loss 1.42793, it/sec: 4.412967336321005
epoch 1 iter 29100: train loss 1.41846. lr 3.000000e-04, running loss 1.42798, it/sec: 4.388061789773877
epoch 1 iter 29120: train loss 1.40870. lr 3.000000e-04, running loss 1.42794, it/sec: 4.382356964142073
epoch 1 iter 29140: train loss 1.43679. lr 3.000000e-04, running loss 1.42789, it/sec: 4.367113423611025
epoch 1 iter 29160: train loss 1.43601. lr 3.000000e-04, running loss 1.42784, it/sec: 4.407209013177799
epoch 1 iter 29180: train loss 1.43578. lr 3.000000e-04, running loss 1.42778, it/sec: 4.420070447181827
epoch 1 iter 29200: train loss 1.44408. lr 3.000000e-04, running loss 1.42783, it/sec: 4.427923135922796
epoch 1 iter 29220: train loss 1.43286. lr 3.000000e-04, running loss 1.42782, it/sec: 4.436968645053263
epoch 1 iter 29240: train loss 1.42130. lr 3.000000e-04, running loss 1.42779, it/sec: 4.435562004568721
epoch 1 iter 29260: train loss 1.41484. lr 3.000000e-04, running loss 1.42778, it/sec: 4.423401596264593
epoch 1 iter 29280: train loss 1.41317. lr 3.000000e-04, running loss 1.42773, it/sec: 4.42135399120531
epoch 1 iter 29300: train loss 1.42064. lr 3.000000e-04, running loss 1.42778, it/sec: 4.42072935291621
epoch 1 iter 29320: train loss 1.43705. lr 3.000000e-04, running loss 1.42782, it/sec: 4.427039920586813
epoch 1 iter 29340: train loss 1.43199. lr 3.000000e-04, running loss 1.42786, it/sec: 4.4088537665939835
epoch 1 iter 29360: train loss 1.41857. lr 3.000000e-04, running loss 1.42791, it/sec: 4.397691471469258
epoch 1 iter 29380: train loss 1.41598. lr 3.000000e-04, running loss 1.42790, it/sec: 4.422294997529507
epoch 1 iter 29400: train loss 1.41449. lr 3.000000e-04, running loss 1.42780, it/sec: 4.426958254970905
epoch 1 iter 29420: train loss 1.43364. lr 3.000000e-04, running loss 1.42783, it/sec: 4.448710282435947
epoch 1 iter 29440: train loss 1.44156. lr 3.000000e-04, running loss 1.42790, it/sec: 4.444189426623947
epoch 1 iter 29460: train loss 1.43337. lr 3.000000e-04, running loss 1.42792, it/sec: 4.443804437924194
epoch 1 iter 29480: train loss 1.42481. lr 3.000000e-04, running loss 1.42785, it/sec: 4.430188177605633
epoch 1 iter 29500: train loss 1.41642. lr 3.000000e-04, running loss 1.42782, it/sec: 4.424985502260411
epoch 1 iter 29520: train loss 1.42677. lr 3.000000e-04, running loss 1.42786, it/sec: 4.419805658178418
epoch 1 iter 29540: train loss 1.42916. lr 3.000000e-04, running loss 1.42790, it/sec: 4.418964362900771
epoch 1 iter 29560: train loss 1.43111. lr 3.000000e-04, running loss 1.42784, it/sec: 4.38574273916087
epoch 1 iter 29580: train loss 1.43844. lr 3.000000e-04, running loss 1.42788, it/sec: 4.399641757850556
epoch 1 iter 29600: train loss 1.43077. lr 3.000000e-04, running loss 1.42788, it/sec: 4.363357381490997
epoch 1 iter 29620: train loss 1.42010. lr 3.000000e-04, running loss 1.42788, it/sec: 4.371523317222485
epoch 1 iter 29640: train loss 1.43753. lr 3.000000e-04, running loss 1.42799, it/sec: 4.3694303358238615
epoch 1 iter 29660: train loss 1.43197. lr 3.000000e-04, running loss 1.42803, it/sec: 4.369204623244834
epoch 1 iter 29680: train loss 1.41191. lr 3.000000e-04, running loss 1.42806, it/sec: 4.356485321061385
epoch 1 iter 29700: train loss 1.44064. lr 3.000000e-04, running loss 1.42803, it/sec: 4.408457092261248
epoch 1 iter 29720: train loss 1.43753. lr 3.000000e-04, running loss 1.42804, it/sec: 4.408530284473627
epoch 1 iter 29740: train loss 1.44722. lr 3.000000e-04, running loss 1.42800, it/sec: 4.431139073316712
epoch 1 iter 29760: train loss 1.40801. lr 3.000000e-04, running loss 1.42800, it/sec: 4.392724987782532
epoch 1 iter 29780: train loss 1.42937. lr 3.000000e-04, running loss 1.42806, it/sec: 4.358841752138737
epoch 1 iter 29800: train loss 1.41646. lr 3.000000e-04, running loss 1.42802, it/sec: 4.359768364301323
epoch 1 iter 29820: train loss 1.43570. lr 3.000000e-04, running loss 1.42811, it/sec: 4.387041007919141
epoch 1 iter 29840: train loss 1.43169. lr 3.000000e-04, running loss 1.42815, it/sec: 4.351045860436139
epoch 1 iter 29860: train loss 1.41577. lr 3.000000e-04, running loss 1.42818, it/sec: 4.345029436870393
epoch 1 iter 29880: train loss 1.43516. lr 3.000000e-04, running loss 1.42818, it/sec: 4.379705487393654
epoch 1 iter 29900: train loss 1.44460. lr 3.000000e-04, running loss 1.42817, it/sec: 4.370671816165266
epoch 1 iter 29920: train loss 1.42568. lr 3.000000e-04, running loss 1.42812, it/sec: 4.369912747147172
epoch 1 iter 29940: train loss 1.66511. lr 3.000000e-04, running loss 1.42834, it/sec: 4.345479264296118
epoch 1 iter 29960: train loss 1.42624. lr 3.000000e-04, running loss 1.42831, it/sec: 4.359303489128153
epoch 1 iter 29980: train loss 1.44134. lr 3.000000e-04, running loss 1.42836, it/sec: 4.364927778997611
epoch 1 iter 30000: train loss 1.44268. lr 3.000000e-04, running loss 1.42843, it/sec: 4.396246421933877
epoch 1 iter 30020: train loss 1.42471. lr 3.000000e-04, running loss 1.42841, it/sec: 4.385014999765035
epoch 1 iter 30040: train loss 1.42124. lr 3.000000e-04, running loss 1.42843, it/sec: 4.355423782605614
epoch 1 iter 30060: train loss 1.42018. lr 3.000000e-04, running loss 1.42833, it/sec: 4.370610688795857
epoch 1 iter 30080: train loss 1.42222. lr 3.000000e-04, running loss 1.42834, it/sec: 4.369863116271716
epoch 1 iter 30100: train loss 1.43446. lr 3.000000e-04, running loss 1.42848, it/sec: 4.374179147620262
epoch 1 iter 30120: train loss 1.41145. lr 3.000000e-04, running loss 1.42846, it/sec: 4.381257904308165
epoch 1 iter 30140: train loss 1.43394. lr 3.000000e-04, running loss 1.42846, it/sec: 4.421082482578099
epoch 1 iter 30160: train loss 1.42673. lr 3.000000e-04, running loss 1.42840, it/sec: 4.415555349816296
epoch 1 iter 30180: train loss 1.42497. lr 3.000000e-04, running loss 1.42841, it/sec: 4.408120376045049
epoch 1 iter 30200: train loss 1.42848. lr 3.000000e-04, running loss 1.42843, it/sec: 4.4263056794077995
epoch 1 iter 30220: train loss 1.41960. lr 3.000000e-04, running loss 1.42837, it/sec: 4.438025245724215
epoch 1 iter 30240: train loss 1.42936. lr 3.000000e-04, running loss 1.42840, it/sec: 4.409741464945986
epoch 1 iter 30260: train loss 1.42900. lr 3.000000e-04, running loss 1.42838, it/sec: 4.423777773379716
epoch 1 iter 30280: train loss 1.42574. lr 3.000000e-04, running loss 1.42837, it/sec: 4.407681054771629
epoch 1 iter 30300: train loss 1.42879. lr 3.000000e-04, running loss 1.42841, it/sec: 4.383924598503106
epoch 1 iter 30320: train loss 1.46159. lr 3.000000e-04, running loss 1.42857, it/sec: 4.40495026716487
epoch 1 iter 30340: train loss 1.47139. lr 3.000000e-04, running loss 1.42845, it/sec: 4.42446137404941
epoch 1 iter 30360: train loss 1.44595. lr 3.000000e-04, running loss 1.42848, it/sec: 4.408800118886792
epoch 1 iter 30380: train loss 1.43052. lr 3.000000e-04, running loss 1.42841, it/sec: 4.429444612085962
epoch 1 iter 30400: train loss 1.44223. lr 3.000000e-04, running loss 1.42841, it/sec: 4.425360205883358
epoch 1 iter 30420: train loss 1.42479. lr 3.000000e-04, running loss 1.42844, it/sec: 4.423758555490272
epoch 1 iter 30440: train loss 1.40721. lr 3.000000e-04, running loss 1.42844, it/sec: 4.428912759183195
epoch 1 iter 30460: train loss 1.43536. lr 3.000000e-04, running loss 1.42843, it/sec: 4.4176860692035085
epoch 1 iter 30480: train loss 1.41431. lr 3.000000e-04, running loss 1.42834, it/sec: 4.432879259826823
epoch 1 iter 30500: train loss 1.44340. lr 3.000000e-04, running loss 1.42836, it/sec: 4.393314156248271
epoch 1 iter 30520: train loss 1.42932. lr 3.000000e-04, running loss 1.42838, it/sec: 4.4225722109426195
epoch 1 iter 30540: train loss 1.40451. lr 3.000000e-04, running loss 1.42840, it/sec: 4.422026811855655
epoch 1 iter 30560: train loss 1.44920. lr 3.000000e-04, running loss 1.42834, it/sec: 4.378542955784964
epoch 1 iter 30580: train loss 1.42699. lr 3.000000e-04, running loss 1.42837, it/sec: 4.425918787496022
epoch 1 iter 30600: train loss 1.42766. lr 3.000000e-04, running loss 1.42827, it/sec: 4.418627151210381
epoch 1 iter 30620: train loss 1.42496. lr 3.000000e-04, running loss 1.42836, it/sec: 4.430451150091804
epoch 1 iter 30640: train loss 1.42237. lr 3.000000e-04, running loss 1.42843, it/sec: 4.4357844528027295
epoch 1 iter 30660: train loss 1.43310. lr 3.000000e-04, running loss 1.42841, it/sec: 4.410452323985121
epoch 1 iter 30680: train loss 1.41396. lr 3.000000e-04, running loss 1.42830, it/sec: 4.346690958043091
epoch 1 iter 30700: train loss 1.41560. lr 3.000000e-04, running loss 1.42822, it/sec: 4.3746905096794855
epoch 1 iter 30720: train loss 1.40523. lr 3.000000e-04, running loss 1.42809, it/sec: 4.3709097360508435
epoch 1 iter 30740: train loss 1.42744. lr 3.000000e-04, running loss 1.42810, it/sec: 4.387438745066019
epoch 1 iter 30760: train loss 1.43147. lr 3.000000e-04, running loss 1.42812, it/sec: 4.369344482108747
epoch 1 iter 30780: train loss 1.42557. lr 3.000000e-04, running loss 1.42821, it/sec: 4.384712614914989
epoch 1 iter 30800: train loss 1.41928. lr 3.000000e-04, running loss 1.42816, it/sec: 4.391264147560432
epoch 1 iter 30820: train loss 1.42384. lr 3.000000e-04, running loss 1.42815, it/sec: 4.398204188905682
epoch 1 iter 30840: train loss 1.43561. lr 3.000000e-04, running loss 1.42818, it/sec: 4.3928452249181245
epoch 1 iter 30860: train loss 1.42655. lr 3.000000e-04, running loss 1.42820, it/sec: 4.381260898729276
epoch 1 iter 30880: train loss 1.42879. lr 3.000000e-04, running loss 1.42848, it/sec: 4.374368824505582
epoch 1 iter 30900: train loss 1.43616. lr 3.000000e-04, running loss 1.42842, it/sec: 4.354108933405842
epoch 1 iter 30920: train loss 1.41393. lr 3.000000e-04, running loss 1.42840, it/sec: 4.357005425091546
epoch 1 iter 30940: train loss 1.42379. lr 3.000000e-04, running loss 1.42832, it/sec: 4.360924312723776
epoch 1 iter 30960: train loss 1.44464. lr 3.000000e-04, running loss 1.42844, it/sec: 4.369024403742156
epoch 1 iter 30980: train loss 1.41778. lr 3.000000e-04, running loss 1.42852, it/sec: 4.364310410069014
epoch 1 iter 31000: train loss 1.42413. lr 3.000000e-04, running loss 1.42839, it/sec: 4.343776628590136
epoch 1 iter 31020: train loss 1.40430. lr 3.000000e-04, running loss 1.42834, it/sec: 4.364637856148923
epoch 1 iter 31040: train loss 1.41952. lr 3.000000e-04, running loss 1.42834, it/sec: 4.367141307729532
epoch 1 iter 31060: train loss 1.40126. lr 3.000000e-04, running loss 1.42826, it/sec: 4.402505841250405
epoch 1 iter 31080: train loss 1.41794. lr 3.000000e-04, running loss 1.42822, it/sec: 4.3971216448796815
epoch 1 iter 31100: train loss 1.42903. lr 3.000000e-04, running loss 1.42818, it/sec: 4.397791963246204
epoch 1 iter 31120: train loss 1.41556. lr 3.000000e-04, running loss 1.42813, it/sec: 4.413379723784374
epoch 1 iter 31140: train loss 1.43197. lr 3.000000e-04, running loss 1.42814, it/sec: 4.424875501200488
epoch 1 iter 31160: train loss 1.43132. lr 3.000000e-04, running loss 1.42811, it/sec: 4.418376806258922
epoch 1 iter 31180: train loss 1.43029. lr 3.000000e-04, running loss 1.42806, it/sec: 4.334920519037107
epoch 1 iter 31200: train loss 1.43646. lr 3.000000e-04, running loss 1.42809, it/sec: 4.364266735752062
epoch 1 iter 31220: train loss 1.43237. lr 3.000000e-04, running loss 1.42808, it/sec: 4.366294162703294
epoch 1 iter 31240: train loss 1.46303. lr 3.000000e-04, running loss 1.42803, it/sec: 4.391757408811332
epoch 1 iter 31260: train loss 1.43104. lr 3.000000e-04, running loss 1.42809, it/sec: 4.418997656336653
epoch 1 iter 31280: train loss 1.43098. lr 3.000000e-04, running loss 1.42812, it/sec: 4.387746028919584
epoch 1 iter 31300: train loss 1.43119. lr 3.000000e-04, running loss 1.42808, it/sec: 4.437723463550694
epoch 1 iter 31320: train loss 1.43471. lr 3.000000e-04, running loss 1.42807, it/sec: 4.437930489219406
epoch 1 iter 31340: train loss 1.42091. lr 3.000000e-04, running loss 1.42802, it/sec: 4.409616683769154
epoch 1 iter 31360: train loss 1.42688. lr 3.000000e-04, running loss 1.42805, it/sec: 4.408934026752865
epoch 1 iter 31380: train loss 1.41965. lr 3.000000e-04, running loss 1.42805, it/sec: 4.381931153153169
epoch 1 iter 31400: train loss 1.43233. lr 3.000000e-04, running loss 1.42817, it/sec: 4.371312598379136
epoch 1 iter 31420: train loss 1.42150. lr 3.000000e-04, running loss 1.42821, it/sec: 4.385768706410496
epoch 1 iter 31440: train loss 1.43320. lr 3.000000e-04, running loss 1.42820, it/sec: 4.371707852350104
epoch 1 iter 31460: train loss 1.43222. lr 3.000000e-04, running loss 1.42825, it/sec: 4.353139017488343
epoch 1 iter 31480: train loss 1.42513. lr 3.000000e-04, running loss 1.42827, it/sec: 4.4132597033692145
epoch 1 iter 31500: train loss 1.43239. lr 3.000000e-04, running loss 1.42814, it/sec: 4.409947736549046
epoch 1 iter 31520: train loss 1.43918. lr 3.000000e-04, running loss 1.42810, it/sec: 4.370775661558785
epoch 1 iter 31540: train loss 1.43103. lr 3.000000e-04, running loss 1.42814, it/sec: 4.357190863817064
epoch 1 iter 31560: train loss 1.45459. lr 3.000000e-04, running loss 1.42816, it/sec: 4.374225832738999
epoch 1 iter 31580: train loss 1.43418. lr 3.000000e-04, running loss 1.42813, it/sec: 4.380583269500996
epoch 1 iter 31600: train loss 1.45078. lr 3.000000e-04, running loss 1.42806, it/sec: 4.375873820388521
epoch 1 iter 31620: train loss 1.43115. lr 3.000000e-04, running loss 1.42805, it/sec: 4.363025273174045
epoch 1 iter 31640: train loss 1.42568. lr 3.000000e-04, running loss 1.42801, it/sec: 4.344141518356243
epoch 1 iter 31660: train loss 1.40953. lr 3.000000e-04, running loss 1.42801, it/sec: 4.380172518696669
epoch 1 iter 31680: train loss 1.42239. lr 3.000000e-04, running loss 1.42798, it/sec: 4.393794789001644
epoch 1 iter 31700: train loss 1.43287. lr 3.000000e-04, running loss 1.42799, it/sec: 4.364706284950321
epoch 1 iter 31720: train loss 1.43156. lr 3.000000e-04, running loss 1.42797, it/sec: 4.375936877325771
epoch 1 iter 31740: train loss 1.42364. lr 3.000000e-04, running loss 1.42840, it/sec: 4.381880998557012
epoch 1 iter 31760: train loss 1.43903. lr 3.000000e-04, running loss 1.42838, it/sec: 4.372370484183382
epoch 1 iter 31780: train loss 1.42639. lr 3.000000e-04, running loss 1.42843, it/sec: 4.374326996901157
epoch 1 iter 31800: train loss 1.41615. lr 3.000000e-04, running loss 1.42847, it/sec: 4.37947295803627
epoch 1 iter 31820: train loss 1.45493. lr 3.000000e-04, running loss 1.42853, it/sec: 4.384690121456032
epoch 1 iter 31840: train loss 1.43045. lr 3.000000e-04, running loss 1.42850, it/sec: 4.364170836702181
epoch 1 iter 31860: train loss 1.42001. lr 3.000000e-04, running loss 1.42849, it/sec: 4.372469610617423
epoch 1 iter 31880: train loss 1.42756. lr 3.000000e-04, running loss 1.42848, it/sec: 4.342690704895514
epoch 1 iter 31900: train loss 1.44954. lr 3.000000e-04, running loss 1.42849, it/sec: 4.34310867740301
epoch 1 iter 31920: train loss 1.43425. lr 3.000000e-04, running loss 1.42848, it/sec: 4.363834642939861
epoch 1 iter 31940: train loss 1.44147. lr 3.000000e-04, running loss 1.42848, it/sec: 4.3536089103813715
epoch 1 iter 31960: train loss 1.44212. lr 3.000000e-04, running loss 1.42843, it/sec: 4.437672083265449
epoch 1 iter 31980: train loss 1.41743. lr 3.000000e-04, running loss 1.42843, it/sec: 4.402487583920541
epoch 1 iter 32000: train loss 1.43358. lr 3.000000e-04, running loss 1.42841, it/sec: 4.4034674797118685
epoch 1 iter 32020: train loss 1.42992. lr 3.000000e-04, running loss 1.42950, it/sec: 4.3866380875206366
epoch 1 iter 32040: train loss 1.40109. lr 3.000000e-04, running loss 1.42937, it/sec: 4.396620662120815
epoch 1 iter 32060: train loss 1.43082. lr 3.000000e-04, running loss 1.42948, it/sec: 4.375226648334636
epoch 1 iter 32080: train loss 1.41411. lr 3.000000e-04, running loss 1.42942, it/sec: 4.40347411070013
epoch 1 iter 32100: train loss 1.42686. lr 3.000000e-04, running loss 1.43080, it/sec: 4.376335917820605
epoch 1 iter 32120: train loss 1.42375. lr 3.000000e-04, running loss 1.43068, it/sec: 4.374692346622371
epoch 1 iter 32140: train loss 1.42303. lr 3.000000e-04, running loss 1.43059, it/sec: 4.360309197838844
epoch 1 iter 32160: train loss 1.43004. lr 3.000000e-04, running loss 1.43048, it/sec: 4.343422178291658
epoch 1 iter 32180: train loss 1.42429. lr 3.000000e-04, running loss 1.43041, it/sec: 4.409770166611665
epoch 1 iter 32200: train loss 1.43162. lr 3.000000e-04, running loss 1.43033, it/sec: 4.400234196590239
epoch 1 iter 32220: train loss 1.43013. lr 3.000000e-04, running loss 1.43022, it/sec: 4.413769706233219
epoch 1 iter 32240: train loss 1.43262. lr 3.000000e-04, running loss 1.43017, it/sec: 4.42057471611605
epoch 1 iter 32260: train loss 1.43708. lr 3.000000e-04, running loss 1.43016, it/sec: 4.419195030072251
epoch 1 iter 32280: train loss 1.43077. lr 3.000000e-04, running loss 1.43007, it/sec: 4.436252737125839
epoch 1 iter 32300: train loss 1.42693. lr 3.000000e-04, running loss 1.42995, it/sec: 4.398108204528885
epoch 1 iter 32320: train loss 1.44420. lr 3.000000e-04, running loss 1.42987, it/sec: 4.427971445507145
epoch 1 iter 32340: train loss 1.41995. lr 3.000000e-04, running loss 1.42976, it/sec: 4.417555276590672
epoch 1 iter 32360: train loss 1.41945. lr 3.000000e-04, running loss 1.42963, it/sec: 4.387052573639199
epoch 1 iter 32380: train loss 1.43506. lr 3.000000e-04, running loss 1.42962, it/sec: 4.389968852660406
epoch 1 iter 32400: train loss 1.42001. lr 3.000000e-04, running loss 1.42972, it/sec: 4.410709826410885
epoch 1 iter 32420: train loss 1.42476. lr 3.000000e-04, running loss 1.42971, it/sec: 4.395983165742402
epoch 1 iter 32440: train loss 1.43312. lr 3.000000e-04, running loss 1.42962, it/sec: 4.4259791422181625
epoch 1 iter 32460: train loss 1.42411. lr 3.000000e-04, running loss 1.42962, it/sec: 4.422648042646171
epoch 1 iter 32480: train loss 1.42731. lr 3.000000e-04, running loss 1.42955, it/sec: 4.42907370850571
epoch 1 iter 32500: train loss 1.44786. lr 3.000000e-04, running loss 1.42953, it/sec: 4.43593899414038
epoch 1 iter 32520: train loss 1.42630. lr 3.000000e-04, running loss 1.42953, it/sec: 4.413096864345986
epoch 1 iter 32540: train loss 1.43521. lr 3.000000e-04, running loss 1.42948, it/sec: 4.426206878331128
epoch 1 iter 32560: train loss 1.42443. lr 3.000000e-04, running loss 1.42944, it/sec: 4.4118911390073405
epoch 1 iter 32580: train loss 1.44538. lr 3.000000e-04, running loss 1.42940, it/sec: 4.3360134103144015
epoch 1 iter 32600: train loss 1.42376. lr 3.000000e-04, running loss 1.42927, it/sec: 4.4208296489867935
epoch 1 iter 32620: train loss 1.41017. lr 3.000000e-04, running loss 1.42929, it/sec: 4.438949931588607
epoch 1 iter 32640: train loss 1.42587. lr 3.000000e-04, running loss 1.42923, it/sec: 4.412684568275227
epoch 1 iter 32660: train loss 1.45757. lr 3.000000e-04, running loss 1.42926, it/sec: 4.418587148115453
epoch 1 iter 32680: train loss 1.44954. lr 3.000000e-04, running loss 1.42925, it/sec: 4.444781575903745
epoch 1 iter 32700: train loss 1.41868. lr 3.000000e-04, running loss 1.42925, it/sec: 4.435103603856479
epoch 1 iter 32720: train loss 1.43350. lr 3.000000e-04, running loss 1.42923, it/sec: 4.379040266750285
epoch 1 iter 32740: train loss 1.42398. lr 3.000000e-04, running loss 1.42913, it/sec: 4.424456402569503
epoch 1 iter 32760: train loss 1.42388. lr 3.000000e-04, running loss 1.42914, it/sec: 4.428519762973362
epoch 1 iter 32780: train loss 1.41342. lr 3.000000e-04, running loss 1.42908, it/sec: 4.398890453932828
epoch 1 iter 32800: train loss 1.41532. lr 3.000000e-04, running loss 1.42896, it/sec: 4.381329657574121
epoch 1 iter 32820: train loss 1.41763. lr 3.000000e-04, running loss 1.42879, it/sec: 4.358035866060515
epoch 1 iter 32840: train loss 1.41279. lr 3.000000e-04, running loss 1.42875, it/sec: 4.364385647291369
epoch 1 iter 32860: train loss 1.43072. lr 3.000000e-04, running loss 1.42877, it/sec: 4.385168447021218
epoch 1 iter 32880: train loss 1.41791. lr 3.000000e-04, running loss 1.42872, it/sec: 4.372022150126666
epoch 1 iter 32900: train loss 1.42930. lr 3.000000e-04, running loss 1.42877, it/sec: 4.357146268811465
epoch 1 iter 32920: train loss 1.43123. lr 3.000000e-04, running loss 1.42877, it/sec: 4.345279753118648
epoch 1 iter 32940: train loss 1.41814. lr 3.000000e-04, running loss 1.42876, it/sec: 4.370273081920198
epoch 1 iter 32960: train loss 1.44105. lr 3.000000e-04, running loss 1.42877, it/sec: 4.378911119593377
epoch 1 iter 32980: train loss 1.43979. lr 3.000000e-04, running loss 1.42881, it/sec: 4.372340335513414
epoch 1 iter 33000: train loss 1.40380. lr 3.000000e-04, running loss 1.42883, it/sec: 4.3721448315957385
epoch 1 iter 33020: train loss 1.42979. lr 3.000000e-04, running loss 1.42869, it/sec: 4.380659761385885
epoch 1 iter 33040: train loss 1.41560. lr 3.000000e-04, running loss 1.42861, it/sec: 4.35432478415736
epoch 1 iter 33060: train loss 1.42439. lr 3.000000e-04, running loss 1.42866, it/sec: 4.337263228714193
epoch 1 iter 33080: train loss 1.41258. lr 3.000000e-04, running loss 1.42866, it/sec: 4.397815481824317
epoch 1 iter 33100: train loss 1.43146. lr 3.000000e-04, running loss 1.42863, it/sec: 4.373420025612108
epoch 1 iter 33120: train loss 1.43528. lr 3.000000e-04, running loss 1.42862, it/sec: 4.422324019936838
epoch 1 iter 33140: train loss 1.42301. lr 3.000000e-04, running loss 1.42862, it/sec: 4.377885811509055
epoch 1 iter 33160: train loss 1.41927. lr 3.000000e-04, running loss 1.42860, it/sec: 4.407400459660529
epoch 1 iter 33180: train loss 1.42093. lr 3.000000e-04, running loss 1.42859, it/sec: 4.439757303847959
epoch 1 iter 33200: train loss 1.43108. lr 3.000000e-04, running loss 1.42860, it/sec: 4.424886035271607
epoch 1 iter 33220: train loss 1.41557. lr 3.000000e-04, running loss 1.42853, it/sec: 4.402809190278714
epoch 1 iter 33240: train loss 1.43482. lr 3.000000e-04, running loss 1.42855, it/sec: 4.4161485904841395
epoch 1 iter 33260: train loss 1.42801. lr 3.000000e-04, running loss 1.42861, it/sec: 4.389588364006302
epoch 1 iter 33280: train loss 1.44359. lr 3.000000e-04, running loss 1.42862, it/sec: 4.358018392476843
epoch 1 iter 33300: train loss 1.42626. lr 3.000000e-04, running loss 1.42864, it/sec: 4.395684870680877
epoch 1 iter 33320: train loss 1.42232. lr 3.000000e-04, running loss 1.42869, it/sec: 4.383118615044413
epoch 1 iter 33340: train loss 1.42411. lr 3.000000e-04, running loss 1.42861, it/sec: 4.412716367372979
epoch 1 iter 33360: train loss 1.41954. lr 3.000000e-04, running loss 1.42855, it/sec: 4.425931129225519
epoch 1 iter 33380: train loss 1.44163. lr 3.000000e-04, running loss 1.42849, it/sec: 4.4193540233410324
epoch 1 iter 33400: train loss 1.42057. lr 3.000000e-04, running loss 1.42856, it/sec: 4.40320006041626
epoch 1 iter 33420: train loss 1.41955. lr 3.000000e-04, running loss 1.42849, it/sec: 4.439182296559995
epoch 1 iter 33440: train loss 1.43373. lr 3.000000e-04, running loss 1.42847, it/sec: 4.436621734584255
epoch 1 iter 33460: train loss 1.42508. lr 3.000000e-04, running loss 1.42842, it/sec: 4.42424890822702
epoch 1 iter 33480: train loss 1.41675. lr 3.000000e-04, running loss 1.42854, it/sec: 4.435308398473882
epoch 1 iter 33500: train loss 1.41359. lr 3.000000e-04, running loss 1.42849, it/sec: 4.434654462501714
epoch 1 iter 33520: train loss 1.44041. lr 3.000000e-04, running loss 1.42843, it/sec: 4.420887812713655
epoch 1 iter 33540: train loss 1.43526. lr 3.000000e-04, running loss 1.42849, it/sec: 4.429226095971365
epoch 1 iter 33560: train loss 1.42244. lr 3.000000e-04, running loss 1.42860, it/sec: 4.431510382352616
epoch 1 iter 33580: train loss 1.43057. lr 3.000000e-04, running loss 1.42863, it/sec: 4.422058667204602
epoch 1 iter 33600: train loss 1.42806. lr 3.000000e-04, running loss 1.42857, it/sec: 4.430490192587203
epoch 1 iter 33620: train loss 1.43826. lr 3.000000e-04, running loss 1.42850, it/sec: 4.4302538686750905
epoch 1 iter 33640: train loss 1.43164. lr 3.000000e-04, running loss 1.42844, it/sec: 4.428818529544039
epoch 1 iter 33660: train loss 1.42136. lr 3.000000e-04, running loss 1.42854, it/sec: 4.440849310191666
epoch 1 iter 33680: train loss 1.42094. lr 3.000000e-04, running loss 1.42842, it/sec: 4.419152221468713
epoch 1 iter 33700: train loss 1.43619. lr 3.000000e-04, running loss 1.42841, it/sec: 4.418710230406508
epoch 1 iter 33720: train loss 1.43662. lr 3.000000e-04, running loss 1.42847, it/sec: 4.410168632817345
epoch 1 iter 33740: train loss 1.42762. lr 3.000000e-04, running loss 1.42848, it/sec: 4.40983723671979
epoch 1 iter 33760: train loss 1.43158. lr 3.000000e-04, running loss 1.42847, it/sec: 4.364119964832404
epoch 1 iter 33780: train loss 1.43194. lr 3.000000e-04, running loss 1.42842, it/sec: 4.353482889423172
epoch 1 iter 33800: train loss 1.42297. lr 3.000000e-04, running loss 1.42836, it/sec: 4.358083329336871
epoch 1 iter 33820: train loss 1.43070. lr 3.000000e-04, running loss 1.42842, it/sec: 4.378449916578973
epoch 1 iter 33840: train loss 1.43094. lr 3.000000e-04, running loss 1.42848, it/sec: 4.42377497572032
epoch 1 iter 33860: train loss 1.41733. lr 3.000000e-04, running loss 1.42842, it/sec: 4.4065674515252
epoch 1 iter 33880: train loss 1.46085. lr 3.000000e-04, running loss 1.42841, it/sec: 4.413638775763906
epoch 1 iter 33900: train loss 1.43858. lr 3.000000e-04, running loss 1.42844, it/sec: 4.416446002326186
epoch 1 iter 33920: train loss 1.44249. lr 3.000000e-04, running loss 1.42850, it/sec: 4.386713347191267
epoch 1 iter 33940: train loss 1.43094. lr 3.000000e-04, running loss 1.42851, it/sec: 4.370183662164723
epoch 1 iter 33960: train loss 1.40968. lr 3.000000e-04, running loss 1.42854, it/sec: 4.3853614255902
epoch 1 iter 33980: train loss 1.42263. lr 3.000000e-04, running loss 1.42846, it/sec: 4.376836597929873
epoch 1 iter 34000: train loss 1.43754. lr 3.000000e-04, running loss 1.42852, it/sec: 4.3611484852846045
epoch 1 iter 34020: train loss 1.43220. lr 3.000000e-04, running loss 1.42850, it/sec: 4.390946576711026
epoch 1 iter 34040: train loss 1.43134. lr 3.000000e-04, running loss 1.42845, it/sec: 4.371068368888686
epoch 1 iter 34060: train loss 1.42714. lr 3.000000e-04, running loss 1.42848, it/sec: 4.379575245939521
epoch 1 iter 34080: train loss 1.42320. lr 3.000000e-04, running loss 1.42846, it/sec: 4.368606465145613
epoch 1 iter 34100: train loss 1.42598. lr 3.000000e-04, running loss 1.42843, it/sec: 4.374715466167634
epoch 1 iter 34120: train loss 1.41065. lr 3.000000e-04, running loss 1.42838, it/sec: 4.363191788087477
epoch 1 iter 34140: train loss 1.42132. lr 3.000000e-04, running loss 1.42838, it/sec: 4.38379258915955
epoch 1 iter 34160: train loss 1.42408. lr 3.000000e-04, running loss 1.42857, it/sec: 4.3680689334529585
epoch 1 iter 34180: train loss 1.43123. lr 3.000000e-04, running loss 1.42874, it/sec: 4.347884745797476
epoch 1 iter 34200: train loss 1.41655. lr 3.000000e-04, running loss 1.42868, it/sec: 4.356744418981661
epoch 1 iter 34220: train loss 1.44449. lr 3.000000e-04, running loss 1.42870, it/sec: 4.3744141380894845
epoch 1 iter 34240: train loss 1.43527. lr 3.000000e-04, running loss 1.42870, it/sec: 4.375098549399564
epoch 1 iter 34260: train loss 1.43495. lr 3.000000e-04, running loss 1.42864, it/sec: 4.374001634544777
epoch 1 iter 34280: train loss 1.44161. lr 3.000000e-04, running loss 1.42862, it/sec: 4.3475049763899785
epoch 1 iter 34300: train loss 1.42191. lr 3.000000e-04, running loss 1.42855, it/sec: 4.3797965455668235
epoch 1 iter 34320: train loss 1.41151. lr 3.000000e-04, running loss 1.42854, it/sec: 4.380095066689636
epoch 1 iter 34340: train loss 1.41290. lr 3.000000e-04, running loss 1.42857, it/sec: 4.3602567803500305
epoch 1 iter 34360: train loss 1.42549. lr 3.000000e-04, running loss 1.42851, it/sec: 4.377127070873471
epoch 1 iter 34380: train loss 1.41657. lr 3.000000e-04, running loss 1.42847, it/sec: 4.377855663707475
epoch 1 iter 34400: train loss 1.42496. lr 3.000000e-04, running loss 1.42848, it/sec: 4.370349004038148
epoch 1 iter 34420: train loss 1.42023. lr 3.000000e-04, running loss 1.42840, it/sec: 4.375436652642471
epoch 1 iter 34440: train loss 1.43458. lr 3.000000e-04, running loss 1.42844, it/sec: 4.368510204215727
epoch 1 iter 34460: train loss 1.43360. lr 3.000000e-04, running loss 1.42842, it/sec: 4.381084154446114
epoch 1 iter 34480: train loss 1.42953. lr 3.000000e-04, running loss 1.42837, it/sec: 4.3530602828064024
epoch 1 iter 34500: train loss 1.42085. lr 3.000000e-04, running loss 1.42830, it/sec: 4.348524687069909
epoch 1 iter 34520: train loss 1.40574. lr 3.000000e-04, running loss 1.42837, it/sec: 4.347165394329513
epoch 1 iter 34540: train loss 1.42896. lr 3.000000e-04, running loss 1.42831, it/sec: 4.341092409989578
epoch 1 iter 34560: train loss 1.41536. lr 3.000000e-04, running loss 1.42829, it/sec: 4.379738939971039
epoch 1 iter 34580: train loss 1.43150. lr 3.000000e-04, running loss 1.42831, it/sec: 4.390169827709606
epoch 1 iter 34600: train loss 1.41207. lr 3.000000e-04, running loss 1.42824, it/sec: 4.393936206362181
epoch 1 iter 34620: train loss 1.45210. lr 3.000000e-04, running loss 1.42827, it/sec: 4.414036616810633
epoch 1 iter 34640: train loss 1.41902. lr 3.000000e-04, running loss 1.42818, it/sec: 4.428538726587234
epoch 1 iter 34660: train loss 1.41875. lr 3.000000e-04, running loss 1.42814, it/sec: 4.424131604434604
epoch 1 iter 34680: train loss 1.43184. lr 3.000000e-04, running loss 1.42813, it/sec: 4.428945712873234
epoch 1 iter 34700: train loss 1.42720. lr 3.000000e-04, running loss 1.42812, it/sec: 4.414639974477739
epoch 1 iter 34720: train loss 1.41023. lr 3.000000e-04, running loss 1.42808, it/sec: 4.428996616110223
epoch 1 iter 34740: train loss 1.42765. lr 3.000000e-04, running loss 1.42809, it/sec: 4.423263655820068
epoch 1 iter 34760: train loss 1.43533. lr 3.000000e-04, running loss 1.42811, it/sec: 4.428249351656313
epoch 1 iter 34780: train loss 1.42836. lr 3.000000e-04, running loss 1.42811, it/sec: 4.437345343131463
epoch 1 iter 34800: train loss 1.44654. lr 3.000000e-04, running loss 1.42804, it/sec: 4.434257005143049
epoch 1 iter 34820: train loss 1.42518. lr 3.000000e-04, running loss 1.42804, it/sec: 4.4217508401581025
epoch 1 iter 34840: train loss 1.42318. lr 3.000000e-04, running loss 1.42798, it/sec: 4.400305836800717
epoch 1 iter 34860: train loss 1.41495. lr 3.000000e-04, running loss 1.42797, it/sec: 4.384022060166857
epoch 1 iter 34880: train loss 1.42639. lr 3.000000e-04, running loss 1.42793, it/sec: 4.3181430211262155
epoch 1 iter 34900: train loss 1.43302. lr 3.000000e-04, running loss 1.42796, it/sec: 4.336895658363226
epoch 1 iter 34920: train loss 1.41747. lr 3.000000e-04, running loss 1.42799, it/sec: 4.348140892509379
epoch 1 iter 34940: train loss 1.42259. lr 3.000000e-04, running loss 1.42794, it/sec: 4.375725771795179
epoch 1 iter 34960: train loss 1.42007. lr 3.000000e-04, running loss 1.42792, it/sec: 4.369346180103103
epoch 1 iter 34980: train loss 1.42165. lr 3.000000e-04, running loss 1.42787, it/sec: 4.3635144004251964
epoch 1 iter 35000: train loss 1.42137. lr 3.000000e-04, running loss 1.42794, it/sec: 4.376226541936858
epoch 1 iter 35020: train loss 1.44515. lr 3.000000e-04, running loss 1.42795, it/sec: 4.354069804175221
epoch 1 iter 35040: train loss 1.43530. lr 3.000000e-04, running loss 1.42795, it/sec: 4.359124084374555
epoch 1 iter 35060: train loss 1.43825. lr 3.000000e-04, running loss 1.42796, it/sec: 4.327780336085868
epoch 1 iter 35080: train loss 1.42566. lr 3.000000e-04, running loss 1.42794, it/sec: 4.35238378952995
epoch 1 iter 35100: train loss 1.43854. lr 3.000000e-04, running loss 1.42795, it/sec: 4.37377851048387
epoch 1 iter 35120: train loss 1.43396. lr 3.000000e-04, running loss 1.42799, it/sec: 4.410446002583019
epoch 1 iter 35140: train loss 1.42348. lr 3.000000e-04, running loss 1.42809, it/sec: 4.424246931446115
epoch 1 iter 35160: train loss 1.42152. lr 3.000000e-04, running loss 1.42801, it/sec: 4.411635328215567
epoch 1 iter 35180: train loss 1.41670. lr 3.000000e-04, running loss 1.42804, it/sec: 4.405158437963006
epoch 1 iter 35200: train loss 1.41995. lr 3.000000e-04, running loss 1.42795, it/sec: 4.408774071662447
epoch 1 iter 35220: train loss 1.41731. lr 3.000000e-04, running loss 1.42800, it/sec: 4.400735149890237
epoch 1 iter 35240: train loss 1.42237. lr 3.000000e-04, running loss 1.42804, it/sec: 4.354272758762114
epoch 1 iter 35260: train loss 1.41696. lr 3.000000e-04, running loss 1.42803, it/sec: 4.3553892771659966
epoch 1 iter 35280: train loss 1.42327. lr 3.000000e-04, running loss 1.42804, it/sec: 4.361830179017038
epoch 1 iter 35300: train loss 1.41072. lr 3.000000e-04, running loss 1.42805, it/sec: 4.3840005336538015
epoch 1 iter 35320: train loss 1.42582. lr 3.000000e-04, running loss 1.42803, it/sec: 4.415124953622492
epoch 1 iter 35340: train loss 1.42066. lr 3.000000e-04, running loss 1.42806, it/sec: 4.404465173013525
epoch 1 iter 35360: train loss 1.42600. lr 3.000000e-04, running loss 1.42800, it/sec: 4.413350039769827
epoch 1 iter 35380: train loss 1.42519. lr 3.000000e-04, running loss 1.42798, it/sec: 4.382874850259745
epoch 1 iter 35400: train loss 1.48090. lr 3.000000e-04, running loss 1.42797, it/sec: 4.3556470300004575
epoch 1 iter 35420: train loss 1.42867. lr 3.000000e-04, running loss 1.42795, it/sec: 4.380650242200589
epoch 1 iter 35440: train loss 1.43531. lr 3.000000e-04, running loss 1.42807, it/sec: 4.373888586956455
epoch 1 iter 35460: train loss 1.43034. lr 3.000000e-04, running loss 1.42808, it/sec: 4.377550645839297
epoch 1 iter 35480: train loss 1.42584. lr 3.000000e-04, running loss 1.42816, it/sec: 4.392322838528433
epoch 1 iter 35500: train loss 1.44462. lr 3.000000e-04, running loss 1.42815, it/sec: 4.399623718061498
epoch 1 iter 35520: train loss 1.40968. lr 3.000000e-04, running loss 1.42816, it/sec: 4.40066357235246
epoch 1 iter 35540: train loss 1.44614. lr 3.000000e-04, running loss 1.42819, it/sec: 4.415856640761791
epoch 1 iter 35560: train loss 1.42766. lr 3.000000e-04, running loss 1.42818, it/sec: 4.41610870831083
epoch 1 iter 35580: train loss 1.44337. lr 3.000000e-04, running loss 1.42823, it/sec: 4.412539645407068
epoch 1 iter 35600: train loss 1.44121. lr 3.000000e-04, running loss 1.42836, it/sec: 4.409876384248021
epoch 1 iter 35620: train loss 1.43094. lr 3.000000e-04, running loss 1.42838, it/sec: 4.421225680173595
epoch 1 iter 35640: train loss 1.46030. lr 3.000000e-04, running loss 1.42834, it/sec: 4.409008498709867
epoch 1 iter 35660: train loss 1.43269. lr 3.000000e-04, running loss 1.42899, it/sec: 4.4118432567015855
epoch 1 iter 35680: train loss 1.43913. lr 3.000000e-04, running loss 1.42911, it/sec: 4.40463013107081
epoch 1 iter 35700: train loss 1.43399. lr 3.000000e-04, running loss 1.42908, it/sec: 4.4294136323702435
epoch 1 iter 35720: train loss 1.41636. lr 3.000000e-04, running loss 1.42899, it/sec: 4.40344393901833
epoch 1 iter 35740: train loss 1.43840. lr 3.000000e-04, running loss 1.42896, it/sec: 4.4132560222401604
epoch 1 iter 35760: train loss 1.43231. lr 3.000000e-04, running loss 1.42898, it/sec: 4.4122893075683365
epoch 1 iter 35780: train loss 1.43292. lr 3.000000e-04, running loss 1.42890, it/sec: 4.388358703586483
epoch 1 iter 35800: train loss 1.41159. lr 3.000000e-04, running loss 1.42879, it/sec: 4.378336429165697
epoch 1 iter 35820: train loss 1.40981. lr 3.000000e-04, running loss 1.42890, it/sec: 4.368670991392294
epoch 1 iter 35840: train loss 1.42841. lr 3.000000e-04, running loss 1.42892, it/sec: 4.370856413964146
epoch 1 iter 35860: train loss 1.44060. lr 3.000000e-04, running loss 1.42898, it/sec: 4.361494821892244
epoch 1 iter 35880: train loss 1.42203. lr 3.000000e-04, running loss 1.42896, it/sec: 4.339749662069671
epoch 1 iter 35900: train loss 1.41362. lr 3.000000e-04, running loss 1.42897, it/sec: 4.355443814782388
epoch 1 iter 35920: train loss 1.42064. lr 3.000000e-04, running loss 1.42898, it/sec: 4.349137729831209
epoch 1 iter 35940: train loss 1.42703. lr 3.000000e-04, running loss 1.42892, it/sec: 4.405163290490052
epoch 1 iter 35960: train loss 1.43164. lr 3.000000e-04, running loss 1.42885, it/sec: 4.383385674142158
epoch 1 iter 35980: train loss 1.42012. lr 3.000000e-04, running loss 1.42890, it/sec: 4.416265373689825
epoch 1 iter 36000: train loss 1.41953. lr 3.000000e-04, running loss 1.42885, it/sec: 4.4204609487683495
epoch 1 iter 36020: train loss 1.43303. lr 3.000000e-04, running loss 1.42879, it/sec: 4.448395846179269
epoch 1 iter 36040: train loss 1.43086. lr 3.000000e-04, running loss 1.42887, it/sec: 4.408993063707235
epoch 1 iter 36060: train loss 1.42056. lr 3.000000e-04, running loss 1.42891, it/sec: 4.417246164709798
epoch 1 iter 36080: train loss 1.42401. lr 3.000000e-04, running loss 1.42893, it/sec: 4.390147105458938
epoch 1 iter 36100: train loss 1.43151. lr 3.000000e-04, running loss 1.42899, it/sec: 4.351395859180829
epoch 1 iter 36120: train loss 1.43132. lr 3.000000e-04, running loss 1.42896, it/sec: 4.370466031391188
epoch 1 iter 36140: train loss 1.41403. lr 3.000000e-04, running loss 1.42895, it/sec: 4.337763569438637
epoch 1 iter 36160: train loss 1.41327. lr 3.000000e-04, running loss 1.42892, it/sec: 4.3603619751548734
epoch 1 iter 36180: train loss 1.43060. lr 3.000000e-04, running loss 1.42899, it/sec: 4.358405718494282
epoch 1 iter 36200: train loss 1.42287. lr 3.000000e-04, running loss 1.42903, it/sec: 4.37270453207258
epoch 1 iter 36220: train loss 1.45707. lr 3.000000e-04, running loss 1.42907, it/sec: 4.3652481336233135
epoch 1 iter 36240: train loss 1.44293. lr 3.000000e-04, running loss 1.42909, it/sec: 4.374204939137405
epoch 1 iter 36260: train loss 1.43400. lr 3.000000e-04, running loss 1.42910, it/sec: 4.414138441080448
epoch 1 iter 36280: train loss 1.40741. lr 3.000000e-04, running loss 1.42904, it/sec: 4.426495477705125
epoch 1 iter 36300: train loss 1.43162. lr 3.000000e-04, running loss 1.42903, it/sec: 4.40971011731785
epoch 1 iter 36320: train loss 1.43215. lr 3.000000e-04, running loss 1.42898, it/sec: 4.435882284591222
epoch 1 iter 36340: train loss 1.42668. lr 3.000000e-04, running loss 1.42900, it/sec: 4.416874314601874
epoch 1 iter 36360: train loss 1.43367. lr 3.000000e-04, running loss 1.42906, it/sec: 4.374579244391721
epoch 1 iter 36380: train loss 1.41950. lr 3.000000e-04, running loss 1.42903, it/sec: 4.366754105805153
epoch 1 iter 36400: train loss 1.41371. lr 3.000000e-04, running loss 1.42897, it/sec: 4.364110689225096
epoch 1 iter 36420: train loss 1.43555. lr 3.000000e-04, running loss 1.42891, it/sec: 4.373341664420872
epoch 1 iter 36440: train loss 1.43416. lr 3.000000e-04, running loss 1.42886, it/sec: 4.3624971264878525
epoch 1 iter 36460: train loss 1.42661. lr 3.000000e-04, running loss 1.42892, it/sec: 4.368991686842345
epoch 1 iter 36480: train loss 1.44358. lr 3.000000e-04, running loss 1.42893, it/sec: 4.373099521304408
epoch 1 iter 36500: train loss 1.42934. lr 3.000000e-04, running loss 1.42883, it/sec: 4.3665100413682705
epoch 1 iter 36520: train loss 1.41843. lr 3.000000e-04, running loss 1.42883, it/sec: 4.352694784349284
epoch 1 iter 36540: train loss 1.42179. lr 3.000000e-04, running loss 1.42878, it/sec: 4.359857872829652
epoch 1 iter 36560: train loss 1.42208. lr 3.000000e-04, running loss 1.42880, it/sec: 4.3611185299204305
epoch 1 iter 36580: train loss 1.42533. lr 3.000000e-04, running loss 1.42874, it/sec: 4.397787031255196
epoch 1 iter 36600: train loss 1.42613. lr 3.000000e-04, running loss 1.42870, it/sec: 4.386617479516313
epoch 1 iter 36620: train loss 1.42688. lr 3.000000e-04, running loss 1.42864, it/sec: 4.440350974154488
epoch 1 iter 36640: train loss 1.42034. lr 3.000000e-04, running loss 1.42854, it/sec: 4.419127576120552
epoch 1 iter 36660: train loss 1.41695. lr 3.000000e-04, running loss 1.42841, it/sec: 4.431657163359517
epoch 1 iter 36680: train loss 1.43697. lr 3.000000e-04, running loss 1.42835, it/sec: 4.436362791358969
epoch 1 iter 36700: train loss 1.42808. lr 3.000000e-04, running loss 1.42834, it/sec: 4.420769143407406
epoch 1 iter 36720: train loss 1.41037. lr 3.000000e-04, running loss 1.42828, it/sec: 4.357297297678014
epoch 1 iter 36740: train loss 1.43108. lr 3.000000e-04, running loss 1.42828, it/sec: 4.381315068290471
epoch 1 iter 36760: train loss 1.41657. lr 3.000000e-04, running loss 1.42869, it/sec: 4.352944240873665
epoch 1 iter 36780: train loss 1.43213. lr 3.000000e-04, running loss 1.42865, it/sec: 4.381386593247275
epoch 1 iter 36800: train loss 1.42294. lr 3.000000e-04, running loss 1.42864, it/sec: 4.391641300721041
epoch 1 iter 36820: train loss 1.43710. lr 3.000000e-04, running loss 1.42863, it/sec: 4.398981962430903
epoch 1 iter 36840: train loss 1.44084. lr 3.000000e-04, running loss 1.42871, it/sec: 4.386680999923185
epoch 1 iter 36860: train loss 1.42026. lr 3.000000e-04, running loss 1.42871, it/sec: 4.399442624172722
epoch 1 iter 36880: train loss 1.43777. lr 3.000000e-04, running loss 1.42864, it/sec: 4.362161533891103
epoch 1 iter 36900: train loss 1.42926. lr 3.000000e-04, running loss 1.42858, it/sec: 4.385514878061728
epoch 1 iter 36920: train loss 1.45269. lr 3.000000e-04, running loss 1.42851, it/sec: 4.350542546946841
epoch 1 iter 36940: train loss 1.44211. lr 3.000000e-04, running loss 1.42852, it/sec: 4.4138410292214925
epoch 1 iter 36960: train loss 1.41704. lr 3.000000e-04, running loss 1.42879, it/sec: 4.408719045621811
epoch 1 iter 36980: train loss 1.42458. lr 3.000000e-04, running loss 1.42874, it/sec: 4.406782300863203
epoch 1 iter 37000: train loss 1.44242. lr 3.000000e-04, running loss 1.42871, it/sec: 4.408450990380374
epoch 1 iter 37020: train loss 1.44659. lr 3.000000e-04, running loss 1.42881, it/sec: 4.385248885901808
epoch 1 iter 37040: train loss 1.41568. lr 3.000000e-04, running loss 1.42890, it/sec: 4.426118052586769
epoch 1 iter 37060: train loss 1.42191. lr 3.000000e-04, running loss 1.42894, it/sec: 4.4409478591504525
epoch 1 iter 37080: train loss 1.41905. lr 3.000000e-04, running loss 1.42885, it/sec: 4.439227897248683
epoch 1 iter 37100: train loss 1.41886. lr 3.000000e-04, running loss 1.42883, it/sec: 4.4345772146276365
epoch 1 iter 37120: train loss 1.44462. lr 3.000000e-04, running loss 1.42883, it/sec: 4.415191407181572
epoch 1 iter 37140: train loss 1.40841. lr 3.000000e-04, running loss 1.42880, it/sec: 4.420565120491508
epoch 1 iter 37160: train loss 1.43735. lr 3.000000e-04, running loss 1.42879, it/sec: 4.412300579577584
epoch 1 iter 37180: train loss 1.40761. lr 3.000000e-04, running loss 1.42871, it/sec: 4.40409671344618
epoch 1 iter 37200: train loss 1.41354. lr 3.000000e-04, running loss 1.42863, it/sec: 4.415055031242848
epoch 1 iter 37220: train loss 1.43131. lr 3.000000e-04, running loss 1.42863, it/sec: 4.384853506434198
epoch 1 iter 37240: train loss 1.42395. lr 3.000000e-04, running loss 1.42868, it/sec: 4.359103713060242
epoch 1 iter 37260: train loss 1.44093. lr 3.000000e-04, running loss 1.42864, it/sec: 4.3428254550848395
epoch 1 iter 37280: train loss 1.43478. lr 3.000000e-04, running loss 1.42861, it/sec: 4.3746979543762965
epoch 1 iter 37300: train loss 1.42186. lr 3.000000e-04, running loss 1.42865, it/sec: 4.372526394149114
epoch 1 iter 37320: train loss 1.43882. lr 3.000000e-04, running loss 1.42869, it/sec: 4.4042247129958705
epoch 1 iter 37340: train loss 1.41371. lr 3.000000e-04, running loss 1.42869, it/sec: 4.392341551801793
epoch 1 iter 37360: train loss 1.41686. lr 3.000000e-04, running loss 1.42868, it/sec: 4.381707314379294
epoch 1 iter 37380: train loss 1.42935. lr 3.000000e-04, running loss 1.42911, it/sec: 4.3802831473148345
epoch 1 iter 37400: train loss 1.42377. lr 3.000000e-04, running loss 1.42907, it/sec: 4.376335649151046
epoch 1 iter 37420: train loss 1.44176. lr 3.000000e-04, running loss 1.42905, it/sec: 4.392497731426394
epoch 1 iter 37440: train loss 2.65235. lr 3.000000e-04, running loss 1.43028, it/sec: 4.410131989956033
epoch 1 iter 37460: train loss 1.42864. lr 3.000000e-04, running loss 1.43023, it/sec: 4.405442125432538
epoch 1 iter 37480: train loss 1.41180. lr 3.000000e-04, running loss 1.43014, it/sec: 4.4305818824873295
epoch 1 iter 37500: train loss 1.51272. lr 3.000000e-04, running loss 1.43020, it/sec: 1.1625730342383103
epoch 1 iter 37520: train loss 1.43986. lr 3.000000e-04, running loss 1.43017, it/sec: 4.403646751416218
epoch 1 iter 37540: train loss 1.41292. lr 3.000000e-04, running loss 1.43081, it/sec: 4.403462630920157
epoch 1 iter 37560: train loss 1.42234. lr 3.000000e-04, running loss 1.43072, it/sec: 4.428214762430001
epoch 1 iter 37580: train loss 1.41951. lr 3.000000e-04, running loss 1.43065, it/sec: 4.430895455502686
epoch 1 iter 37600: train loss 1.42007. lr 3.000000e-04, running loss 1.43321, it/sec: 4.424229158726987
epoch 1 iter 37620: train loss 1.42239. lr 3.000000e-04, running loss 1.43314, it/sec: 4.413568511534772
epoch 1 iter 37640: train loss 1.43271. lr 3.000000e-04, running loss 1.43307, it/sec: 4.420375147986918
epoch 1 iter 37660: train loss 1.43375. lr 3.000000e-04, running loss 1.43306, it/sec: 4.406841337857508
epoch 1 iter 37680: train loss 1.43535. lr 3.000000e-04, running loss 1.43293, it/sec: 4.4226484343009105
epoch 1 iter 37700: train loss 1.40989. lr 3.000000e-04, running loss 1.43280, it/sec: 4.417360995587506
epoch 1 iter 37720: train loss 1.42233. lr 3.000000e-04, running loss 1.43263, it/sec: 4.414098342294199
epoch 1 iter 37740: train loss 1.43245. lr 3.000000e-04, running loss 1.43249, it/sec: 4.445153792435005
epoch 1 iter 37760: train loss 1.42675. lr 3.000000e-04, running loss 1.43231, it/sec: 4.41027286684343
epoch 1 iter 37780: train loss 1.43840. lr 3.000000e-04, running loss 1.43222, it/sec: 4.380205269251139
epoch 1 iter 37800: train loss 1.45101. lr 3.000000e-04, running loss 1.43223, it/sec: 4.373085121471716
epoch 1 iter 37820: train loss 1.41286. lr 3.000000e-04, running loss 1.43216, it/sec: 4.370799140041622
epoch 1 iter 37840: train loss 1.43923. lr 3.000000e-04, running loss 1.43207, it/sec: 4.342917231261687
epoch 1 iter 37860: train loss 1.42153. lr 3.000000e-04, running loss 1.43194, it/sec: 4.372269774361918
epoch 1 iter 37880: train loss 1.41971. lr 3.000000e-04, running loss 1.43187, it/sec: 4.379254760927325
epoch 1 iter 37900: train loss 1.43041. lr 3.000000e-04, running loss 1.43184, it/sec: 4.361369181033559
epoch 1 iter 37920: train loss 1.43084. lr 3.000000e-04, running loss 1.43170, it/sec: 4.403905244543592
epoch 1 iter 37940: train loss 1.41939. lr 3.000000e-04, running loss 1.43171, it/sec: 4.3602191185769685
epoch 1 iter 37960: train loss 1.41969. lr 3.000000e-04, running loss 1.43169, it/sec: 4.362816915014986
epoch 1 iter 37980: train loss 1.42141. lr 3.000000e-04, running loss 1.43162, it/sec: 4.364759302945101
epoch 1 iter 38000: train loss 1.42616. lr 3.000000e-04, running loss 1.43153, it/sec: 4.366546707616098
epoch 1 iter 38020: train loss 1.44189. lr 3.000000e-04, running loss 1.43143, it/sec: 4.356234035805255
epoch 1 iter 38040: train loss 1.43213. lr 3.000000e-04, running loss 1.43151, it/sec: 4.364292753091421
epoch 1 iter 38060: train loss 1.40661. lr 3.000000e-04, running loss 1.43140, it/sec: 4.37305088996812
epoch 1 iter 38080: train loss 1.43553. lr 3.000000e-04, running loss 1.43135, it/sec: 4.384951199669863
epoch 1 iter 38100: train loss 1.43318. lr 3.000000e-04, running loss 1.43122, it/sec: 4.415067975084028
epoch 1 iter 38120: train loss 1.43257. lr 3.000000e-04, running loss 1.43119, it/sec: 4.406493004365798
epoch 1 iter 38140: train loss 1.42973. lr 3.000000e-04, running loss 1.43116, it/sec: 4.40435727603437
epoch 1 iter 38160: train loss 1.43551. lr 3.000000e-04, running loss 1.43101, it/sec: 4.413671892616867
epoch 1 iter 38180: train loss 1.43696. lr 3.000000e-04, running loss 1.43105, it/sec: 4.41720840810679
epoch 1 iter 38200: train loss 1.42083. lr 3.000000e-04, running loss 1.43097, it/sec: 4.412743316945743
epoch 1 iter 38220: train loss 1.42923. lr 3.000000e-04, running loss 1.43086, it/sec: 4.443781096168903
epoch 1 iter 38240: train loss 1.42022. lr 3.000000e-04, running loss 1.43086, it/sec: 4.444820614932383
epoch 1 iter 38260: train loss 1.41068. lr 3.000000e-04, running loss 1.43077, it/sec: 4.422442752244161
epoch 1 iter 38280: train loss 1.40878. lr 3.000000e-04, running loss 1.43071, it/sec: 4.403697151531371
epoch 1 iter 38300: train loss 1.41364. lr 3.000000e-04, running loss 1.43065, it/sec: 4.41548108550407
epoch 1 iter 38320: train loss 1.45077. lr 3.000000e-04, running loss 1.43122, it/sec: 4.416328194627207
epoch 1 iter 38340: train loss 1.42446. lr 3.000000e-04, running loss 1.43110, it/sec: 4.435738076467236
epoch 1 iter 38360: train loss 1.42228. lr 3.000000e-04, running loss 1.43110, it/sec: 4.4008302794632455
epoch 1 iter 38380: train loss 1.42733. lr 3.000000e-04, running loss 1.43109, it/sec: 4.363973528581225
epoch 1 iter 38400: train loss 1.43292. lr 3.000000e-04, running loss 1.43108, it/sec: 4.4171005309497335
epoch 1 iter 38420: train loss 1.43292. lr 3.000000e-04, running loss 1.43106, it/sec: 4.378280913931968
epoch 1 iter 38440: train loss 1.41905. lr 3.000000e-04, running loss 1.43099, it/sec: 4.353987206829329
epoch 1 iter 38460: train loss 1.41700. lr 3.000000e-04, running loss 1.43093, it/sec: 4.36915575403872
epoch 1 iter 38480: train loss 1.43273. lr 3.000000e-04, running loss 1.43091, it/sec: 4.342550794155685
epoch 1 iter 38500: train loss 1.42812. lr 3.000000e-04, running loss 1.43096, it/sec: 4.372370217113282
epoch 1 iter 38520: train loss 1.41738. lr 3.000000e-04, running loss 1.43087, it/sec: 4.346435511049466
epoch 1 iter 38540: train loss 1.43692. lr 3.000000e-04, running loss 1.43082, it/sec: 4.340576719740811
epoch 1 iter 38560: train loss 1.45244. lr 3.000000e-04, running loss 1.43088, it/sec: 4.370727253134942
epoch 1 iter 38580: train loss 1.42292. lr 3.000000e-04, running loss 1.43078, it/sec: 4.366780267134388
epoch 1 iter 38600: train loss 1.43619. lr 3.000000e-04, running loss 1.43066, it/sec: 4.376610368182652
epoch 1 iter 38620: train loss 1.41319. lr 3.000000e-04, running loss 1.43053, it/sec: 4.371046721395903
epoch 1 iter 38640: train loss 1.43254. lr 3.000000e-04, running loss 1.43050, it/sec: 4.376750049285625
epoch 1 iter 38660: train loss 1.41753. lr 3.000000e-04, running loss 1.43043, it/sec: 4.390285318356796
epoch 1 iter 38680: train loss 1.44645. lr 3.000000e-04, running loss 1.43035, it/sec: 4.413502341521345
epoch 1 iter 38700: train loss 1.44532. lr 3.000000e-04, running loss 1.43021, it/sec: 4.412020816879111
epoch 1 iter 38720: train loss 1.42249. lr 3.000000e-04, running loss 1.43015, it/sec: 4.437983706353483
epoch 1 iter 38740: train loss 1.43352. lr 3.000000e-04, running loss 1.43010, it/sec: 4.4396093547853335
epoch 1 iter 38760: train loss 1.42499. lr 3.000000e-04, running loss 1.43005, it/sec: 4.432257843402552
epoch 1 iter 38780: train loss 1.42474. lr 3.000000e-04, running loss 1.42997, it/sec: 4.420208832231219
epoch 1 iter 38800: train loss 1.45359. lr 3.000000e-04, running loss 1.43002, it/sec: 4.389319584160085
epoch 1 iter 38820: train loss 1.42172. lr 3.000000e-04, running loss 1.43006, it/sec: 4.392072242691897
epoch 1 iter 38840: train loss 1.60659. lr 3.000000e-04, running loss 1.43017, it/sec: 4.402341583753537
epoch 1 iter 38860: train loss 1.44943. lr 3.000000e-04, running loss 1.43022, it/sec: 4.418490662949066
epoch 1 iter 38880: train loss 1.42348. lr 3.000000e-04, running loss 1.43008, it/sec: 4.433436969186798
epoch 1 iter 38900: train loss 1.43055. lr 3.000000e-04, running loss 1.43000, it/sec: 4.423948996642182
epoch 1 iter 38920: train loss 1.41963. lr 3.000000e-04, running loss 1.42997, it/sec: 4.371719835747067
epoch 1 iter 38940: train loss 1.42708. lr 3.000000e-04, running loss 1.42988, it/sec: 4.369957280399318
epoch 1 iter 38960: train loss 1.41328. lr 3.000000e-04, running loss 1.42980, it/sec: 4.390374735729756
epoch 1 iter 38980: train loss 1.41452. lr 3.000000e-04, running loss 1.42973, it/sec: 4.376866022505566
epoch 1 iter 39000: train loss 1.42450. lr 3.000000e-04, running loss 1.42967, it/sec: 4.374556911589375
epoch 1 iter 39020: train loss 1.43454. lr 3.000000e-04, running loss 1.42963, it/sec: 4.375588090712386
epoch 1 iter 39040: train loss 1.39492. lr 3.000000e-04, running loss 1.42950, it/sec: 4.36443361043429
epoch 1 iter 39060: train loss 1.42633. lr 3.000000e-04, running loss 1.42944, it/sec: 4.367043928008415
epoch 1 iter 39080: train loss 1.41234. lr 3.000000e-04, running loss 1.42931, it/sec: 4.405082914546822
epoch 1 iter 39100: train loss 1.41026. lr 3.000000e-04, running loss 1.42932, it/sec: 4.378354947574597
epoch 1 iter 39120: train loss 1.42203. lr 3.000000e-04, running loss 1.42926, it/sec: 4.367049611638371
epoch 1 iter 39140: train loss 1.43775. lr 3.000000e-04, running loss 1.42919, it/sec: 4.350048905024926
epoch 1 iter 39160: train loss 1.42487. lr 3.000000e-04, running loss 1.42916, it/sec: 4.3659895908932915
epoch 1 iter 39180: train loss 1.41429. lr 3.000000e-04, running loss 1.42916, it/sec: 4.349659109303101
epoch 1 iter 39200: train loss 1.42624. lr 3.000000e-04, running loss 1.42908, it/sec: 4.390463384794762
epoch 1 iter 39220: train loss 1.43627. lr 3.000000e-04, running loss 1.42910, it/sec: 4.395316390919183
epoch 1 iter 39240: train loss 1.47548. lr 3.000000e-04, running loss 1.42911, it/sec: 4.421857106512039
epoch 1 iter 39260: train loss 1.43996. lr 3.000000e-04, running loss 1.42918, it/sec: 4.427416306051636
epoch 1 iter 39280: train loss 1.42032. lr 3.000000e-04, running loss 1.42918, it/sec: 4.4477924263364015
epoch 1 iter 39300: train loss 1.42334. lr 3.000000e-04, running loss 1.42917, it/sec: 4.399362050155775
epoch 1 iter 39320: train loss 1.43145. lr 3.000000e-04, running loss 1.42921, it/sec: 4.41626040017707
epoch 1 iter 39340: train loss 1.42131. lr 3.000000e-04, running loss 1.42904, it/sec: 4.4076529818922054
epoch 1 iter 39360: train loss 1.42234. lr 3.000000e-04, running loss 1.42906, it/sec: 4.3762400818026475
epoch 1 iter 39380: train loss 1.41963. lr 3.000000e-04, running loss 1.42917, it/sec: 4.384884287895447
epoch 1 iter 39400: train loss 1.41691. lr 3.000000e-04, running loss 1.42937, it/sec: 4.374047780210587
epoch 1 iter 39420: train loss 1.41911. lr 3.000000e-04, running loss 1.42927, it/sec: 4.377150731394883
epoch 1 iter 39440: train loss 1.43356. lr 3.000000e-04, running loss 1.42925, it/sec: 4.390133073290875
epoch 1 iter 39460: train loss 1.43296. lr 3.000000e-04, running loss 1.42919, it/sec: 4.410191953377268
epoch 1 iter 39480: train loss 1.41910. lr 3.000000e-04, running loss 1.42914, it/sec: 4.375193512010468
epoch 1 iter 39500: train loss 1.42260. lr 3.000000e-04, running loss 1.42911, it/sec: 4.377589661714212
epoch 1 iter 39520: train loss 1.43772. lr 3.000000e-04, running loss 1.42910, it/sec: 4.373730973075553
epoch 1 iter 39540: train loss 1.42574. lr 3.000000e-04, running loss 1.42913, it/sec: 4.329541211022079
epoch 1 iter 39560: train loss 1.44167. lr 3.000000e-04, running loss 1.42921, it/sec: 4.391694995304734
epoch 1 iter 39580: train loss 1.44506. lr 3.000000e-04, running loss 1.42932, it/sec: 4.391112258850154
epoch 1 iter 39600: train loss 1.42664. lr 3.000000e-04, running loss 1.42935, it/sec: 4.419850882115577
epoch 1 iter 39620: train loss 1.44176. lr 3.000000e-04, running loss 1.42926, it/sec: 4.41482986389338
epoch 1 iter 39640: train loss 1.42979. lr 3.000000e-04, running loss 1.42922, it/sec: 4.428071090064761
epoch 1 iter 39660: train loss 1.41306. lr 3.000000e-04, running loss 1.42908, it/sec: 4.415988033167366
epoch 1 iter 39680: train loss 1.43594. lr 3.000000e-04, running loss 1.42905, it/sec: 4.389540096499919
epoch 1 iter 39700: train loss 1.43772. lr 3.000000e-04, running loss 1.42905, it/sec: 4.420294801703482
epoch 1 iter 39720: train loss 1.43311. lr 3.000000e-04, running loss 1.42904, it/sec: 4.407019855627973
epoch 1 iter 39740: train loss 1.41786. lr 3.000000e-04, running loss 1.42899, it/sec: 4.411028725632957
epoch 1 iter 39760: train loss 1.42369. lr 3.000000e-04, running loss 1.42895, it/sec: 4.438748089676684
epoch 1 iter 39780: train loss 1.43856. lr 3.000000e-04, running loss 1.42893, it/sec: 4.4353118222023715
epoch 1 iter 39800: train loss 1.41509. lr 3.000000e-04, running loss 1.42880, it/sec: 4.419589575763769
epoch 1 iter 39820: train loss 1.41810. lr 3.000000e-04, running loss 1.42878, it/sec: 4.429159101654301
epoch 1 iter 39840: train loss 1.42489. lr 3.000000e-04, running loss 1.42883, it/sec: 4.383144512089339
epoch 1 iter 39860: train loss 1.41392. lr 3.000000e-04, running loss 1.42880, it/sec: 4.395326707129507
epoch 1 iter 39880: train loss 1.43169. lr 3.000000e-04, running loss 1.42877, it/sec: 4.364964475237452
epoch 1 iter 39900: train loss 1.41962. lr 3.000000e-04, running loss 1.42874, it/sec: 4.372252129700782
epoch 1 iter 39920: train loss 1.42653. lr 3.000000e-04, running loss 1.42873, it/sec: 4.3529754296371825
epoch 1 iter 39940: train loss 1.41736. lr 3.000000e-04, running loss 1.42872, it/sec: 4.3623287250304985
epoch 1 iter 39960: train loss 1.43603. lr 3.000000e-04, running loss 1.42870, it/sec: 4.354284323409682
epoch 1 iter 39980: train loss 1.42899. lr 3.000000e-04, running loss 1.42865, it/sec: 4.389962262283631
epoch 1 iter 40000: train loss 1.43507. lr 3.000000e-04, running loss 1.42872, it/sec: 4.382824330795941
epoch 1 iter 40020: train loss 1.41970. lr 3.000000e-04, running loss 1.42874, it/sec: 4.341459524770695
epoch 1 iter 40040: train loss 1.42246. lr 3.000000e-04, running loss 1.42876, it/sec: 4.345809649931209
epoch 1 iter 40060: train loss 1.42165. lr 3.000000e-04, running loss 1.42878, it/sec: 4.3375105055784084
epoch 1 iter 40080: train loss 1.42429. lr 3.000000e-04, running loss 1.42877, it/sec: 4.343039415592768
epoch 1 iter 40100: train loss 1.43744. lr 3.000000e-04, running loss 1.42878, it/sec: 4.406622889551329
epoch 1 iter 40120: train loss 1.42408. lr 3.000000e-04, running loss 1.42874, it/sec: 4.419815249369069
epoch 1 iter 40140: train loss 1.42867. lr 3.000000e-04, running loss 1.42873, it/sec: 4.436182596425017
epoch 1 iter 40160: train loss 1.41311. lr 3.000000e-04, running loss 1.42882, it/sec: 4.432960829486506
epoch 1 iter 40180: train loss 1.41355. lr 3.000000e-04, running loss 1.42882, it/sec: 4.414714676757976
epoch 1 iter 40200: train loss 1.43085. lr 3.000000e-04, running loss 1.42880, it/sec: 4.425576793673045
epoch 1 iter 40220: train loss 1.41541. lr 3.000000e-04, running loss 1.42880, it/sec: 4.392216346373739
epoch 1 iter 40240: train loss 1.42333. lr 3.000000e-04, running loss 1.42882, it/sec: 4.390400488581798
epoch 1 iter 40260: train loss 1.43572. lr 3.000000e-04, running loss 1.42879, it/sec: 4.344685936524458
epoch 1 iter 40280: train loss 1.43758. lr 3.000000e-04, running loss 1.42886, it/sec: 4.386211751055087
epoch 1 iter 40300: train loss 1.43714. lr 3.000000e-04, running loss 1.42893, it/sec: 4.377359982458445
epoch 1 iter 40320: train loss 1.45616. lr 3.000000e-04, running loss 1.42889, it/sec: 4.372247159132981
epoch 1 iter 40340: train loss 1.41991. lr 3.000000e-04, running loss 1.42891, it/sec: 4.4352180276936135
epoch 1 iter 40360: train loss 1.43258. lr 3.000000e-04, running loss 1.42896, it/sec: 4.399499470511725
epoch 1 iter 40380: train loss 1.42955. lr 3.000000e-04, running loss 1.42898, it/sec: 4.390894075412422
epoch 1 iter 40400: train loss 1.41851. lr 3.000000e-04, running loss 1.42892, it/sec: 4.348225141298202
epoch 1 iter 40420: train loss 1.44046. lr 3.000000e-04, running loss 1.42891, it/sec: 4.3597533871949
epoch 1 iter 40440: train loss 1.42271. lr 3.000000e-04, running loss 1.42884, it/sec: 4.365175534124538
epoch 1 iter 40460: train loss 1.42379. lr 3.000000e-04, running loss 1.42886, it/sec: 4.366850632342948
epoch 1 iter 40480: train loss 1.42898. lr 3.000000e-04, running loss 1.42892, it/sec: 4.362907653421793
epoch 1 iter 40500: train loss 1.43435. lr 3.000000e-04, running loss 1.42893, it/sec: 4.411940268679321
epoch 1 iter 40520: train loss 1.42027. lr 3.000000e-04, running loss 1.42880, it/sec: 4.412921533347482
epoch 1 iter 40540: train loss 1.42319. lr 3.000000e-04, running loss 1.42879, it/sec: 4.419201357253976
epoch 1 iter 40560: train loss 1.42278. lr 3.000000e-04, running loss 1.42877, it/sec: 4.431792798284239
epoch 1 iter 40580: train loss 1.43119. lr 3.000000e-04, running loss 1.42879, it/sec: 4.442814898100576
epoch 1 iter 40600: train loss 1.44101. lr 3.000000e-04, running loss 1.42885, it/sec: 4.415363077354718
epoch 1 iter 40620: train loss 1.42852. lr 3.000000e-04, running loss 1.42880, it/sec: 4.418123171959169
epoch 1 iter 40640: train loss 1.42122. lr 3.000000e-04, running loss 1.42877, it/sec: 4.310340684239459
epoch 1 iter 40660: train loss 1.43311. lr 3.000000e-04, running loss 1.42879, it/sec: 4.397335109245024
epoch 1 iter 40680: train loss 1.42127. lr 3.000000e-04, running loss 1.42874, it/sec: 4.37868007516323
epoch 1 iter 40700: train loss 1.43361. lr 3.000000e-04, running loss 1.42879, it/sec: 4.411285790191844
epoch 1 iter 40720: train loss 1.42417. lr 3.000000e-04, running loss 1.42873, it/sec: 4.395137641645917
epoch 1 iter 40740: train loss 1.41759. lr 3.000000e-04, running loss 1.42868, it/sec: 4.441123293502636
epoch 1 iter 40760: train loss 1.42697. lr 3.000000e-04, running loss 1.42868, it/sec: 4.425967975809134
epoch 1 iter 40780: train loss 1.41193. lr 3.000000e-04, running loss 1.42866, it/sec: 4.424513603180515
epoch 1 iter 40800: train loss 1.43558. lr 3.000000e-04, running loss 1.42866, it/sec: 4.404533537414479
epoch 1 iter 40820: train loss 1.41199. lr 3.000000e-04, running loss 1.42862, it/sec: 4.398474172847685
epoch 1 iter 40840: train loss 1.40591. lr 3.000000e-04, running loss 1.42859, it/sec: 4.390469379759512
epoch 1 iter 40860: train loss 1.42374. lr 3.000000e-04, running loss 1.42858, it/sec: 4.36744300695217
epoch 1 iter 40880: train loss 1.41389. lr 3.000000e-04, running loss 1.42854, it/sec: 4.353684101246172
epoch 1 iter 40900: train loss 1.44321. lr 3.000000e-04, running loss 1.42848, it/sec: 4.404481003166243
epoch 1 iter 40920: train loss 1.42505. lr 3.000000e-04, running loss 1.42851, it/sec: 4.401013618434239
epoch 1 iter 40940: train loss 1.42046. lr 3.000000e-04, running loss 1.42835, it/sec: 4.360620241649662
epoch 1 iter 40960: train loss 1.43289. lr 3.000000e-04, running loss 1.42832, it/sec: 4.349592569878049
epoch 1 iter 40980: train loss 1.45240. lr 3.000000e-04, running loss 1.42835, it/sec: 4.373034978824066
epoch 1 iter 41000: train loss 1.39710. lr 3.000000e-04, running loss 1.42858, it/sec: 4.372895592623833
epoch 1 iter 41020: train loss 1.41547. lr 3.000000e-04, running loss 1.42854, it/sec: 4.323122804832753
epoch 1 iter 41040: train loss 1.42889. lr 3.000000e-04, running loss 1.42847, it/sec: 4.419431971861117
epoch 1 iter 41060: train loss 1.44656. lr 3.000000e-04, running loss 1.42842, it/sec: 4.4158641672060055
epoch 1 iter 41080: train loss 1.43519. lr 3.000000e-04, running loss 1.42838, it/sec: 4.369720037693604
epoch 1 iter 41100: train loss 1.43468. lr 3.000000e-04, running loss 1.42836, it/sec: 4.40443830526245
epoch 1 iter 41120: train loss 1.43298. lr 3.000000e-04, running loss 1.42836, it/sec: 4.41300464977849
epoch 1 iter 41140: train loss 1.42296. lr 3.000000e-04, running loss 1.42846, it/sec: 4.407970175912638
epoch 1 iter 41160: train loss 1.43390. lr 3.000000e-04, running loss 1.42865, it/sec: 4.437890095721614
epoch 1 iter 41180: train loss 1.43006. lr 3.000000e-04, running loss 1.42861, it/sec: 4.432017226217109
epoch 1 iter 41200: train loss 1.41734. lr 3.000000e-04, running loss 1.42858, it/sec: 4.412868448565417
epoch 1 iter 41220: train loss 1.43844. lr 3.000000e-04, running loss 1.42859, it/sec: 4.4314862070923855
epoch 1 iter 41240: train loss 1.43299. lr 3.000000e-04, running loss 1.42856, it/sec: 4.415439325905752
epoch 1 iter 41260: train loss 1.43444. lr 3.000000e-04, running loss 1.42854, it/sec: 4.41262226010168
epoch 1 iter 41280: train loss 1.48892. lr 3.000000e-04, running loss 1.42858, it/sec: 4.42274240042586
epoch 1 iter 41300: train loss 1.42733. lr 3.000000e-04, running loss 1.42847, it/sec: 4.415743329504973
epoch 1 iter 41320: train loss 1.41564. lr 3.000000e-04, running loss 1.42852, it/sec: 4.373335084907253
epoch 1 iter 41340: train loss 1.44474. lr 3.000000e-04, running loss 1.42846, it/sec: 4.3489551513162015
epoch 1 iter 41360: train loss 1.42608. lr 3.000000e-04, running loss 1.42845, it/sec: 4.36922253000456
epoch 1 iter 41380: train loss 1.44816. lr 3.000000e-04, running loss 1.42842, it/sec: 4.35799087346965
epoch 1 iter 41400: train loss 1.41628. lr 3.000000e-04, running loss 1.42852, it/sec: 4.3567310945009
epoch 1 iter 41420: train loss 1.44276. lr 3.000000e-04, running loss 1.42862, it/sec: 4.401649514062008
epoch 1 iter 41440: train loss 1.40643. lr 3.000000e-04, running loss 1.42876, it/sec: 4.399138634926747
epoch 1 iter 41460: train loss 1.41678. lr 3.000000e-04, running loss 1.42888, it/sec: 4.414467814918076
epoch 1 iter 41480: train loss 1.43255. lr 3.000000e-04, running loss 1.42886, it/sec: 4.410507315173335
epoch 1 iter 41500: train loss 1.42658. lr 3.000000e-04, running loss 1.42889, it/sec: 4.389419963962838
epoch 1 iter 41520: train loss 1.41316. lr 3.000000e-04, running loss 1.42890, it/sec: 4.40806400568552
epoch 1 iter 41540: train loss 1.41521. lr 3.000000e-04, running loss 1.42883, it/sec: 4.397810549780558
epoch 1 iter 41560: train loss 1.42898. lr 3.000000e-04, running loss 1.42878, it/sec: 4.411845105717431
epoch 1 iter 41580: train loss 1.44780. lr 3.000000e-04, running loss 1.42875, it/sec: 4.405882011755017
epoch 1 iter 41600: train loss 1.43336. lr 3.000000e-04, running loss 1.42942, it/sec: 4.423290069281766
epoch 1 iter 41620: train loss 1.42276. lr 3.000000e-04, running loss 1.42935, it/sec: 4.3812173638245895
epoch 1 iter 41640: train loss 1.41722. lr 3.000000e-04, running loss 1.42938, it/sec: 4.3933676985035035
epoch 1 iter 41660: train loss 1.41902. lr 3.000000e-04, running loss 1.42936, it/sec: 4.393827126647416
epoch 1 iter 41680: train loss 1.41676. lr 3.000000e-04, running loss 1.42927, it/sec: 4.384950430776101
epoch 1 iter 41700: train loss 1.42066. lr 3.000000e-04, running loss 1.42914, it/sec: 4.356436868531887
epoch 1 iter 41720: train loss 1.40367. lr 3.000000e-04, running loss 1.42912, it/sec: 4.348203359601835
epoch 1 iter 41740: train loss 1.43671. lr 3.000000e-04, running loss 1.42908, it/sec: 4.361445895801678
epoch 1 iter 41760: train loss 1.42716. lr 3.000000e-04, running loss 1.42910, it/sec: 4.365047794594704
epoch 1 iter 41780: train loss 1.42668. lr 3.000000e-04, running loss 1.42900, it/sec: 4.348142215302609
epoch 1 iter 41800: train loss 1.40824. lr 3.000000e-04, running loss 1.42900, it/sec: 4.355482854979064
epoch 1 iter 41820: train loss 1.43314. lr 3.000000e-04, running loss 1.42979, it/sec: 4.3770576387122135
epoch 1 iter 41840: train loss 1.42482. lr 3.000000e-04, running loss 1.42977, it/sec: 4.3795815372146265
epoch 1 iter 41860: train loss 1.41652. lr 3.000000e-04, running loss 1.42972, it/sec: 4.352652421819934
epoch 1 iter 41880: train loss 1.41476. lr 3.000000e-04, running loss 1.42967, it/sec: 4.4095405592199075
epoch 1 iter 41900: train loss 1.42294. lr 3.000000e-04, running loss 1.42972, it/sec: 4.3824967808849244
epoch 1 iter 41920: train loss 1.43864. lr 3.000000e-04, running loss 1.42964, it/sec: 4.421618164614147
epoch 1 iter 41940: train loss 1.42215. lr 3.000000e-04, running loss 1.42958, it/sec: 4.416369622503084
epoch 1 iter 41960: train loss 1.42695. lr 3.000000e-04, running loss 1.42955, it/sec: 4.430197736225728
epoch 1 iter 41980: train loss 1.41988. lr 3.000000e-04, running loss 1.42950, it/sec: 4.421069034706993
epoch 1 iter 42000: train loss 1.42667. lr 3.000000e-04, running loss 1.42953, it/sec: 4.402042910197282
epoch 1 iter 42020: train loss 1.43417. lr 3.000000e-04, running loss 1.42945, it/sec: 4.3542677153192475
epoch 1 iter 42040: train loss 1.41563. lr 3.000000e-04, running loss 1.42937, it/sec: 4.355817288180403
epoch 1 iter 42060: train loss 1.41741. lr 3.000000e-04, running loss 1.42957, it/sec: 4.36396892045965
epoch 1 iter 42080: train loss 1.42901. lr 3.000000e-04, running loss 1.42954, it/sec: 4.352987517912402
epoch 1 iter 42100: train loss 1.43565. lr 3.000000e-04, running loss 1.42956, it/sec: 4.4007152794698525
epoch 1 iter 42120: train loss 1.74049. lr 3.000000e-04, running loss 1.42980, it/sec: 4.405142156773694
epoch 1 iter 42140: train loss 1.40551. lr 3.000000e-04, running loss 1.42984, it/sec: 4.36728845180783
epoch 1 iter 42160: train loss 1.42985. lr 3.000000e-04, running loss 1.42979, it/sec: 4.370284312593093
epoch 1 iter 42180: train loss 1.43657. lr 3.000000e-04, running loss 1.42979, it/sec: 4.380302026179483
epoch 1 iter 42200: train loss 1.41737. lr 3.000000e-04, running loss 1.42974, it/sec: 4.3581439741231875
epoch 1 iter 42220: train loss 1.43821. lr 3.000000e-04, running loss 1.42980, it/sec: 4.338684121256765
epoch 1 iter 42240: train loss 1.42506. lr 3.000000e-04, running loss 1.42980, it/sec: 4.363621447132399
epoch 1 iter 42260: train loss 1.44360. lr 3.000000e-04, running loss 1.42974, it/sec: 4.3937955621239455
epoch 1 iter 42280: train loss 1.43035. lr 3.000000e-04, running loss 1.42986, it/sec: 4.417902393826881
epoch 1 iter 42300: train loss 1.42631. lr 3.000000e-04, running loss 1.42975, it/sec: 4.412008651181745
epoch 1 iter 42320: train loss 1.43770. lr 3.000000e-04, running loss 1.42970, it/sec: 4.4329900515382805
epoch 1 iter 42340: train loss 1.42335. lr 3.000000e-04, running loss 1.42975, it/sec: 4.425536153917144
epoch 1 iter 42360: train loss 1.42248. lr 3.000000e-04, running loss 1.42973, it/sec: 4.424698294028486
epoch 1 iter 42380: train loss 1.42436. lr 3.000000e-04, running loss 1.42966, it/sec: 4.410462244875177
epoch 1 iter 42400: train loss 1.42303. lr 3.000000e-04, running loss 1.42959, it/sec: 4.404242751018066
epoch 1 iter 42420: train loss 1.42982. lr 3.000000e-04, running loss 1.42956, it/sec: 4.392713913285871
epoch 1 iter 42440: train loss 1.42567. lr 3.000000e-04, running loss 1.42946, it/sec: 4.423886487286343
epoch 1 iter 42460: train loss 1.40808. lr 3.000000e-04, running loss 1.42940, it/sec: 4.377879084443628
epoch 1 iter 42480: train loss 1.42833. lr 3.000000e-04, running loss 1.42935, it/sec: 4.428674505190309
epoch 1 iter 42500: train loss 1.43725. lr 3.000000e-04, running loss 1.42924, it/sec: 4.41936146508058
epoch 1 iter 42520: train loss 1.41866. lr 3.000000e-04, running loss 1.42914, it/sec: 4.355589147989823
epoch 1 iter 42540: train loss 1.43573. lr 3.000000e-04, running loss 1.42913, it/sec: 4.352926240093652
epoch 1 iter 42560: train loss 1.41636. lr 3.000000e-04, running loss 1.42909, it/sec: 4.353629342904284
epoch 1 iter 42580: train loss 1.41404. lr 3.000000e-04, running loss 1.42903, it/sec: 4.357759485675026
epoch 1 iter 42600: train loss 1.42979. lr 3.000000e-04, running loss 1.42907, it/sec: 4.355905040856374
epoch 1 iter 42620: train loss 1.41829. lr 3.000000e-04, running loss 1.42905, it/sec: 4.359700374574681
epoch 1 iter 42640: train loss 1.43949. lr 3.000000e-04, running loss 1.42900, it/sec: 4.346367993709156
epoch 1 iter 42660: train loss 1.43673. lr 3.000000e-04, running loss 1.42893, it/sec: 4.358269239058865
epoch 1 iter 42680: train loss 1.42541. lr 3.000000e-04, running loss 1.42893, it/sec: 4.359996315130446
epoch 1 iter 42700: train loss 1.43265. lr 3.000000e-04, running loss 1.42889, it/sec: 4.373064907766384
epoch 1 iter 42720: train loss 1.42041. lr 3.000000e-04, running loss 1.42878, it/sec: 4.380191396607717
epoch 1 iter 42740: train loss 1.42375. lr 3.000000e-04, running loss 1.42883, it/sec: 4.351493071330524
epoch 1 iter 42760: train loss 1.42470. lr 3.000000e-04, running loss 1.42890, it/sec: 4.350387178442899
epoch 1 iter 42780: train loss 1.43690. lr 3.000000e-04, running loss 1.42888, it/sec: 4.414875861920908
epoch 1 iter 42800: train loss 1.44787. lr 3.000000e-04, running loss 1.42884, it/sec: 4.4071630956304855
epoch 1 iter 42820: train loss 1.40722. lr 3.000000e-04, running loss 1.42877, it/sec: 4.399382624864988
epoch 1 iter 42840: train loss 1.42107. lr 3.000000e-04, running loss 1.42872, it/sec: 4.413519521337904
epoch 1 iter 42860: train loss 1.44023. lr 3.000000e-04, running loss 1.42864, it/sec: 4.432342082056767
epoch 1 iter 42880: train loss 1.41523. lr 3.000000e-04, running loss 1.42862, it/sec: 4.419428690839754
epoch 1 iter 42900: train loss 1.42134. lr 3.000000e-04, running loss 1.42875, it/sec: 4.414103993688378
epoch 1 iter 42920: train loss 1.41764. lr 3.000000e-04, running loss 1.42870, it/sec: 4.399952805511228
epoch 1 iter 42940: train loss 1.44690. lr 3.000000e-04, running loss 1.42872, it/sec: 4.374841360245242
epoch 1 iter 42960: train loss 1.43201. lr 3.000000e-04, running loss 1.42874, it/sec: 4.411357479219641
epoch 1 iter 42980: train loss 1.42261. lr 3.000000e-04, running loss 1.42880, it/sec: 4.413508164865511
epoch 1 iter 43000: train loss 1.42907. lr 3.000000e-04, running loss 1.42873, it/sec: 4.393779924429837
epoch 1 iter 43020: train loss 1.43381. lr 3.000000e-04, running loss 1.42871, it/sec: 4.420949436444803
epoch 1 iter 43040: train loss 1.42134. lr 3.000000e-04, running loss 1.42886, it/sec: 4.4087698934094695
epoch 1 iter 43060: train loss 1.43268. lr 3.000000e-04, running loss 1.42879, it/sec: 4.37697456724123
epoch 1 iter 43080: train loss 1.44140. lr 3.000000e-04, running loss 1.42883, it/sec: 4.38838555038856
epoch 1 iter 43100: train loss 1.44840. lr 3.000000e-04, running loss 1.42876, it/sec: 4.359516015402512
epoch 1 iter 43120: train loss 1.41765. lr 3.000000e-04, running loss 1.42875, it/sec: 4.37046190653183
epoch 1 iter 43140: train loss 1.42743. lr 3.000000e-04, running loss 1.42870, it/sec: 4.365021920192871
epoch 1 iter 43160: train loss 1.42790. lr 3.000000e-04, running loss 1.42862, it/sec: 4.390757538859992
epoch 1 iter 43180: train loss 1.43537. lr 3.000000e-04, running loss 1.42863, it/sec: 4.406527042595175
epoch 1 iter 43200: train loss 1.44377. lr 3.000000e-04, running loss 1.42864, it/sec: 4.430525113300892
epoch 1 iter 43220: train loss 1.41726. lr 3.000000e-04, running loss 1.42863, it/sec: 4.398023966199834
epoch 1 iter 43240: train loss 1.43978. lr 3.000000e-04, running loss 1.42872, it/sec: 4.428460633431479
epoch 1 iter 43260: train loss 1.42296. lr 3.000000e-04, running loss 1.42870, it/sec: 4.4292384355980365
epoch 1 iter 43280: train loss 1.44411. lr 3.000000e-04, running loss 1.42875, it/sec: 4.442341736560426
epoch 1 iter 43300: train loss 1.43553. lr 3.000000e-04, running loss 1.42866, it/sec: 4.429792188586636
epoch 1 iter 43320: train loss 1.42153. lr 3.000000e-04, running loss 1.42874, it/sec: 4.398791382870951
epoch 1 iter 43340: train loss 1.43278. lr 3.000000e-04, running loss 1.42870, it/sec: 4.414927416837918
epoch 1 iter 43360: train loss 1.43903. lr 3.000000e-04, running loss 1.42866, it/sec: 4.408484553196716
epoch 1 iter 43380: train loss 1.40030. lr 3.000000e-04, running loss 1.42862, it/sec: 4.411906846968518
epoch 1 iter 43400: train loss 1.42563. lr 3.000000e-04, running loss 1.42858, it/sec: 4.394941831935302
epoch 1 iter 43420: train loss 1.42783. lr 3.000000e-04, running loss 1.42862, it/sec: 4.3790168525633755
epoch 1 iter 43440: train loss 1.43709. lr 3.000000e-04, running loss 1.42861, it/sec: 4.364203576783284
epoch 1 iter 43460: train loss 1.42458. lr 3.000000e-04, running loss 1.42856, it/sec: 4.37450551103102
epoch 1 iter 43480: train loss 1.43107. lr 3.000000e-04, running loss 1.42855, it/sec: 4.378259366772812
epoch 1 iter 43500: train loss 1.42121. lr 3.000000e-04, running loss 1.42843, it/sec: 4.3330295067926565
epoch 1 iter 43520: train loss 1.40533. lr 3.000000e-04, running loss 1.42829, it/sec: 4.384306644608414
epoch 1 iter 43540: train loss 1.43376. lr 3.000000e-04, running loss 1.42828, it/sec: 4.355059707654003
epoch 1 iter 43560: train loss 1.42973. lr 3.000000e-04, running loss 1.42835, it/sec: 4.3681984525644415
epoch 1 iter 43580: train loss 1.44619. lr 3.000000e-04, running loss 1.42830, it/sec: 4.36849862053104
epoch 1 iter 43600: train loss 1.44893. lr 3.000000e-04, running loss 1.42827, it/sec: 4.365385317831164
epoch 1 iter 43620: train loss 1.41077. lr 3.000000e-04, running loss 1.42825, it/sec: 4.365383984525729
epoch 1 iter 43640: train loss 1.43206. lr 3.000000e-04, running loss 1.42827, it/sec: 4.356397867569404
epoch 1 iter 43660: train loss 1.42098. lr 3.000000e-04, running loss 1.42827, it/sec: 4.357572004634985
epoch 1 iter 43680: train loss 1.42927. lr 3.000000e-04, running loss 1.42822, it/sec: 4.361072408675066
epoch 1 iter 43700: train loss 1.43927. lr 3.000000e-04, running loss 1.42822, it/sec: 4.440665378299724
epoch 1 iter 43720: train loss 1.41272. lr 3.000000e-04, running loss 1.42815, it/sec: 4.4124866468770785
epoch 1 iter 43740: train loss 1.42021. lr 3.000000e-04, running loss 1.42817, it/sec: 4.44617647703795
epoch 1 iter 43760: train loss 1.42680. lr 3.000000e-04, running loss 1.42817, it/sec: 4.444623078603885
epoch 1 iter 43780: train loss 1.42655. lr 3.000000e-04, running loss 1.42820, it/sec: 4.419965750664669
epoch 1 iter 43800: train loss 1.42853. lr 3.000000e-04, running loss 1.42819, it/sec: 4.411611252633465
epoch 1 iter 43820: train loss 1.41586. lr 3.000000e-04, running loss 1.42813, it/sec: 4.418223954294186
epoch 1 iter 43840: train loss 1.43835. lr 3.000000e-04, running loss 1.42816, it/sec: 4.3733375330183835
epoch 1 iter 43860: train loss 1.42096. lr 3.000000e-04, running loss 1.42832, it/sec: 4.395542645725193
epoch 1 iter 43880: train loss 1.43743. lr 3.000000e-04, running loss 1.42832, it/sec: 4.423331588013634
epoch 1 iter 43900: train loss 1.41924. lr 3.000000e-04, running loss 1.42824, it/sec: 4.427578616325659
epoch 1 iter 43920: train loss 1.44121. lr 3.000000e-04, running loss 1.42816, it/sec: 4.393313500137075
epoch 1 iter 43940: train loss 1.44406. lr 3.000000e-04, running loss 1.42816, it/sec: 4.4254007455121895
epoch 1 iter 43960: train loss 1.43923. lr 3.000000e-04, running loss 1.42816, it/sec: 4.364377990405084
epoch 1 iter 43980: train loss 1.41768. lr 3.000000e-04, running loss 1.42815, it/sec: 4.380642009847698
epoch 1 iter 44000: train loss 1.44273. lr 3.000000e-04, running loss 1.42814, it/sec: 4.375263956966609
epoch 1 iter 44020: train loss 1.42693. lr 3.000000e-04, running loss 1.42814, it/sec: 4.370363538169899
epoch 1 iter 44040: train loss 1.44741. lr 3.000000e-04, running loss 1.42812, it/sec: 4.346377797728844
epoch 1 iter 44060: train loss 1.44966. lr 3.000000e-04, running loss 1.42812, it/sec: 4.357311632404839
epoch 1 iter 44080: train loss 1.42962. lr 3.000000e-04, running loss 1.42809, it/sec: 4.3598030542285215
epoch 1 iter 44100: train loss 1.43592. lr 3.000000e-04, running loss 1.42807, it/sec: 4.368351318254471
epoch 1 iter 44120: train loss 1.44060. lr 3.000000e-04, running loss 1.42807, it/sec: 4.380801177877583
epoch 1 iter 44140: train loss 1.41883. lr 3.000000e-04, running loss 1.42808, it/sec: 4.400395701142943
epoch 1 iter 44160: train loss 1.42850. lr 3.000000e-04, running loss 1.42811, it/sec: 4.3671872521374375
epoch 1 iter 44180: train loss 1.43247. lr 3.000000e-04, running loss 1.42815, it/sec: 4.382262765672896
epoch 1 iter 44200: train loss 1.41126. lr 3.000000e-04, running loss 1.42814, it/sec: 4.363417108130626
epoch 1 iter 44220: train loss 1.42901. lr 3.000000e-04, running loss 1.42824, it/sec: 4.397778599276106
epoch 1 iter 44240: train loss 1.42877. lr 3.000000e-04, running loss 1.42821, it/sec: 4.409340431163071
epoch 1 iter 44260: train loss 1.42591. lr 3.000000e-04, running loss 1.42811, it/sec: 4.431387135747125
epoch 1 iter 44280: train loss 1.42856. lr 3.000000e-04, running loss 1.42811, it/sec: 4.4214193636515295
epoch 1 iter 44300: train loss 1.42395. lr 3.000000e-04, running loss 1.42812, it/sec: 4.445150354645282
epoch 1 iter 44320: train loss 1.42668. lr 3.000000e-04, running loss 1.42811, it/sec: 4.4193694525301055
epoch 1 iter 44340: train loss 1.43751. lr 3.000000e-04, running loss 1.42815, it/sec: 4.349039203765906
epoch 1 iter 44360: train loss 1.43704. lr 3.000000e-04, running loss 1.42825, it/sec: 4.350386365439712
epoch 1 iter 44380: train loss 1.43018. lr 3.000000e-04, running loss 1.42833, it/sec: 4.341552185684815
epoch 1 iter 44400: train loss 1.43196. lr 3.000000e-04, running loss 1.42833, it/sec: 4.36431564754231
epoch 1 iter 44420: train loss 1.42959. lr 3.000000e-04, running loss 1.42834, it/sec: 4.361914596983556
epoch 1 iter 44440: train loss 1.42525. lr 3.000000e-04, running loss 1.42842, it/sec: 4.367459716811801
epoch 1 iter 44460: train loss 1.42353. lr 3.000000e-04, running loss 1.42834, it/sec: 4.3620232586257774
epoch 1 iter 44480: train loss 1.43436. lr 3.000000e-04, running loss 1.42842, it/sec: 4.348736843925453
epoch 1 iter 44500: train loss 1.44912. lr 3.000000e-04, running loss 1.42855, it/sec: 4.353391879620608
epoch 1 iter 44520: train loss 1.42691. lr 3.000000e-04, running loss 1.42856, it/sec: 4.361086501451083
epoch 1 iter 44540: train loss 1.41992. lr 3.000000e-04, running loss 1.42858, it/sec: 4.361418275915521
epoch 1 iter 44560: train loss 1.43733. lr 3.000000e-04, running loss 1.42854, it/sec: 4.388050448493434
epoch 1 iter 44580: train loss 1.43312. lr 3.000000e-04, running loss 1.42846, it/sec: 4.372635277983856
epoch 1 iter 44600: train loss 1.43033. lr 3.000000e-04, running loss 1.42842, it/sec: 4.403693661300326
epoch 1 iter 44620: train loss 1.43924. lr 3.000000e-04, running loss 1.42843, it/sec: 4.426619548455576
epoch 1 iter 44640: train loss 1.43125. lr 3.000000e-04, running loss 1.42848, it/sec: 4.426016400096949
epoch 1 iter 44660: train loss 1.43708. lr 3.000000e-04, running loss 1.42870, it/sec: 4.42036895392656
epoch 1 iter 44680: train loss 1.42499. lr 3.000000e-04, running loss 1.42876, it/sec: 4.43290873473124
epoch 1 iter 44700: train loss 1.44258. lr 3.000000e-04, running loss 1.42880, it/sec: 4.420705081284445
epoch 1 iter 44720: train loss 1.41064. lr 3.000000e-04, running loss 1.42885, it/sec: 4.41439923819901
epoch 1 iter 44740: train loss 1.42262. lr 3.000000e-04, running loss 1.42880, it/sec: 4.426926309599927
epoch 1 iter 44760: train loss 1.42565. lr 3.000000e-04, running loss 1.42870, it/sec: 4.427560953756404
epoch 1 iter 44780: train loss 1.43376. lr 3.000000e-04, running loss 1.42871, it/sec: 4.418811625989733
epoch 1 iter 44800: train loss 1.41661. lr 3.000000e-04, running loss 1.42865, it/sec: 4.421938331356572
epoch 1 iter 44820: train loss 1.42436. lr 3.000000e-04, running loss 1.42862, it/sec: 4.421240008521766
epoch 1 iter 44840: train loss 1.44374. lr 3.000000e-04, running loss 1.42859, it/sec: 4.397222882693711
epoch 1 iter 44860: train loss 1.42842. lr 3.000000e-04, running loss 1.42850, it/sec: 4.399464785924684
epoch 1 iter 44880: train loss 1.42241. lr 3.000000e-04, running loss 1.42858, it/sec: 4.3746296708690755
epoch 1 iter 44900: train loss 1.44176. lr 3.000000e-04, running loss 1.42860, it/sec: 4.386350661482291
epoch 1 iter 44920: train loss 1.43372. lr 3.000000e-04, running loss 1.42853, it/sec: 4.375787789064937
epoch 1 iter 44940: train loss 1.43098. lr 3.000000e-04, running loss 1.42849, it/sec: 4.336904948926647
epoch 1 iter 44960: train loss 1.43267. lr 3.000000e-04, running loss 1.42854, it/sec: 4.375854940665323
epoch 1 iter 44980: train loss 1.43236. lr 3.000000e-04, running loss 1.42861, it/sec: 4.370469890532155
epoch 1 iter 45000: train loss 1.44018. lr 3.000000e-04, running loss 1.42867, it/sec: 4.414180372288065
epoch 1 iter 45020: train loss 1.43387. lr 3.000000e-04, running loss 1.42871, it/sec: 4.3894115629164565
epoch 1 iter 45040: train loss 1.40821. lr 3.000000e-04, running loss 1.42863, it/sec: 4.350299894701405
epoch 1 iter 45060: train loss 1.40620. lr 3.000000e-04, running loss 1.42841, it/sec: 4.369689353178096
epoch 1 iter 45080: train loss 1.43430. lr 3.000000e-04, running loss 1.42845, it/sec: 4.362804334453919
epoch 1 iter 45100: train loss 1.42577. lr 3.000000e-04, running loss 1.42852, it/sec: 4.400887918123837
epoch 1 iter 45120: train loss 1.42490. lr 3.000000e-04, running loss 1.42844, it/sec: 4.387142840779777
epoch 1 iter 45140: train loss 1.41420. lr 3.000000e-04, running loss 1.42853, it/sec: 4.427793402572234
epoch 1 iter 45160: train loss 1.43348. lr 3.000000e-04, running loss 1.42849, it/sec: 4.4177639963840525
epoch 1 iter 45180: train loss 1.42363. lr 3.000000e-04, running loss 1.42842, it/sec: 4.438255267741285
epoch 1 iter 45200: train loss 1.43166. lr 3.000000e-04, running loss 1.42844, it/sec: 4.4061444942767025
epoch 1 iter 45220: train loss 1.42744. lr 3.000000e-04, running loss 1.42842, it/sec: 4.432117543471499
epoch 1 iter 45240: train loss 1.42023. lr 3.000000e-04, running loss 1.42846, it/sec: 4.421175581179478
epoch 1 iter 45260: train loss 1.43611. lr 3.000000e-04, running loss 1.42844, it/sec: 4.3926869371239246
epoch 1 iter 45280: train loss 1.45307. lr 3.000000e-04, running loss 1.42848, it/sec: 4.360374144331035
epoch 1 iter 45300: train loss 1.42913. lr 3.000000e-04, running loss 1.42840, it/sec: 4.362893320630763
epoch 1 iter 45320: train loss 1.42677. lr 3.000000e-04, running loss 1.42833, it/sec: 4.353069947180293
epoch 1 iter 45340: train loss 1.43104. lr 3.000000e-04, running loss 1.42832, it/sec: 4.363720084153281
epoch 1 iter 45360: train loss 1.44953. lr 3.000000e-04, running loss 1.42834, it/sec: 4.4086749056290735
epoch 1 iter 45380: train loss 1.44483. lr 3.000000e-04, running loss 1.42835, it/sec: 4.406125933285426
epoch 1 iter 45400: train loss 1.44427. lr 3.000000e-04, running loss 1.42838, it/sec: 4.377285043380759
epoch 1 iter 45420: train loss 1.42413. lr 3.000000e-04, running loss 1.42837, it/sec: 4.376271030063418
epoch 1 iter 45440: train loss 1.43975. lr 3.000000e-04, running loss 1.42832, it/sec: 4.352357268947098
epoch 1 iter 45460: train loss 1.43698. lr 3.000000e-04, running loss 1.42830, it/sec: 4.3745953973577505
epoch 1 iter 45480: train loss 1.42940. lr 3.000000e-04, running loss 1.42838, it/sec: 4.369739226849167
epoch 1 iter 45500: train loss 1.45351. lr 3.000000e-04, running loss 1.42894, it/sec: 4.356676219712281
epoch 1 iter 45520: train loss 1.44508. lr 3.000000e-04, running loss 1.42904, it/sec: 4.409966716901208
epoch 1 iter 45540: train loss 1.42315. lr 3.000000e-04, running loss 1.42911, it/sec: 4.399239521794813
epoch 1 iter 45560: train loss 1.41145. lr 3.000000e-04, running loss 1.42902, it/sec: 4.405366377655208
epoch 1 iter 45580: train loss 1.43358. lr 3.000000e-04, running loss 1.42898, it/sec: 4.413394565941388
epoch 1 iter 45600: train loss 1.41440. lr 3.000000e-04, running loss 1.42894, it/sec: 4.395729156962689
epoch 1 iter 45620: train loss 1.42196. lr 3.000000e-04, running loss 1.42899, it/sec: 4.4276450927034015
epoch 1 iter 45640: train loss 1.41522. lr 3.000000e-04, running loss 1.42903, it/sec: 4.404041939575972
epoch 1 iter 45660: train loss 1.41421. lr 3.000000e-04, running loss 1.42898, it/sec: 4.408832735487213
epoch 1 iter 45680: train loss 1.43058. lr 3.000000e-04, running loss 1.42890, it/sec: 4.42427560690551
epoch 1 iter 45700: train loss 1.42031. lr 3.000000e-04, running loss 1.42890, it/sec: 4.424504499798992
epoch 1 iter 45720: train loss 1.43844. lr 3.000000e-04, running loss 1.42892, it/sec: 4.424414451381194
epoch 1 iter 45740: train loss 1.43754. lr 3.000000e-04, running loss 1.42903, it/sec: 4.429728257599387
epoch 1 iter 45760: train loss 1.43210. lr 3.000000e-04, running loss 1.42901, it/sec: 4.4229234818625685
epoch 1 iter 45780: train loss 1.40886. lr 3.000000e-04, running loss 1.42898, it/sec: 4.427360696162754
epoch 1 iter 45800: train loss 1.42744. lr 3.000000e-04, running loss 1.42898, it/sec: 4.405715405271293
epoch 1 iter 45820: train loss 1.43098. lr 3.000000e-04, running loss 1.42899, it/sec: 4.362302729994881
epoch 1 iter 45840: train loss 1.43803. lr 3.000000e-04, running loss 1.42892, it/sec: 4.3649035828178615
epoch 1 iter 45860: train loss 1.43169. lr 3.000000e-04, running loss 1.42886, it/sec: 4.348240568664437
epoch 1 iter 45880: train loss 1.43475. lr 3.000000e-04, running loss 1.42879, it/sec: 4.353076502257677
epoch 1 iter 45900: train loss 1.43547. lr 3.000000e-04, running loss 1.42886, it/sec: 4.380811118862757
epoch 1 iter 45920: train loss 1.43630. lr 3.000000e-04, running loss 1.42877, it/sec: 4.37032917805071
epoch 1 iter 45940: train loss 1.43087. lr 3.000000e-04, running loss 1.42872, it/sec: 4.379198148551058
epoch 1 iter 45960: train loss 1.43580. lr 3.000000e-04, running loss 1.42876, it/sec: 4.309768173376404
epoch 1 iter 45980: train loss 1.41888. lr 3.000000e-04, running loss 1.42870, it/sec: 4.356838814102719
epoch 1 iter 46000: train loss 1.46295. lr 3.000000e-04, running loss 1.42875, it/sec: 4.3442637144629455
epoch 1 iter 46020: train loss 1.42910. lr 3.000000e-04, running loss 1.42875, it/sec: 4.383312162544058
epoch 1 iter 46040: train loss 1.43203. lr 3.000000e-04, running loss 1.42864, it/sec: 4.398820464773015
epoch 1 iter 46060: train loss 1.42072. lr 3.000000e-04, running loss 1.42857, it/sec: 4.409555434419565
epoch 1 iter 46080: train loss 1.44940. lr 3.000000e-04, running loss 1.42854, it/sec: 4.4388587425687565
epoch 1 iter 46100: train loss 1.42436. lr 3.000000e-04, running loss 1.42855, it/sec: 4.390755015098761
epoch 1 iter 46120: train loss 1.42079. lr 3.000000e-04, running loss 1.42851, it/sec: 4.435504477852332
epoch 1 iter 46140: train loss 1.41460. lr 3.000000e-04, running loss 1.42852, it/sec: 4.411161525291235
epoch 1 iter 46160: train loss 1.42105. lr 3.000000e-04, running loss 1.42847, it/sec: 4.360830140089245
epoch 1 iter 46180: train loss 1.43634. lr 3.000000e-04, running loss 1.42845, it/sec: 4.382996450829098
epoch 1 iter 46200: train loss 1.42880. lr 3.000000e-04, running loss 1.42843, it/sec: 4.345160255001005
epoch 1 iter 46220: train loss 1.42985. lr 3.000000e-04, running loss 1.42836, it/sec: 4.333858081605484
epoch 1 iter 46240: train loss 1.43922. lr 3.000000e-04, running loss 1.42835, it/sec: 4.418963131925066
epoch 1 iter 46260: train loss 1.43086. lr 3.000000e-04, running loss 1.42831, it/sec: 4.381940926028634
epoch 1 iter 46280: train loss 1.43053. lr 3.000000e-04, running loss 1.42825, it/sec: 4.419680795704233
epoch 1 iter 46300: train loss 1.41660. lr 3.000000e-04, running loss 1.42834, it/sec: 4.375885252625505
epoch 1 iter 46320: train loss 1.40468. lr 3.000000e-04, running loss 1.42828, it/sec: 4.386857483652831
epoch 1 iter 46340: train loss 1.41037. lr 3.000000e-04, running loss 1.42837, it/sec: 4.397873929805613
epoch 1 iter 46360: train loss 1.40863. lr 3.000000e-04, running loss 1.42832, it/sec: 4.357173360541496
epoch 1 iter 46380: train loss 1.43900. lr 3.000000e-04, running loss 1.42833, it/sec: 4.357079102311542
epoch 1 iter 46400: train loss 1.41211. lr 3.000000e-04, running loss 1.42837, it/sec: 4.385704289879245
epoch 1 iter 46420: train loss 1.42923. lr 3.000000e-04, running loss 1.42833, it/sec: 4.382805389931128
epoch 1 iter 46440: train loss 1.41919. lr 3.000000e-04, running loss 1.42837, it/sec: 4.421825725243567
epoch 1 iter 46460: train loss 1.43573. lr 3.000000e-04, running loss 1.42836, it/sec: 4.4268300291045035
epoch 1 iter 46480: train loss 1.44316. lr 3.000000e-04, running loss 1.42832, it/sec: 4.4387397762814045
epoch 1 iter 46500: train loss 1.42620. lr 3.000000e-04, running loss 1.42832, it/sec: 4.4285933865854945
epoch 1 iter 46520: train loss 1.42425. lr 3.000000e-04, running loss 1.42832, it/sec: 4.4182356861525935
epoch 1 iter 46540: train loss 1.42773. lr 3.000000e-04, running loss 1.42823, it/sec: 4.443639789933052
epoch 1 iter 46560: train loss 1.42394. lr 3.000000e-04, running loss 1.42837, it/sec: 4.4303690443844195
epoch 1 iter 46580: train loss 1.42496. lr 3.000000e-04, running loss 1.42835, it/sec: 4.41089202349135
epoch 1 iter 46600: train loss 1.45063. lr 3.000000e-04, running loss 1.42838, it/sec: 4.452079960114201
epoch 1 iter 46620: train loss 1.43728. lr 3.000000e-04, running loss 1.42844, it/sec: 4.3476131864343825
epoch 1 iter 46640: train loss 1.42844. lr 3.000000e-04, running loss 1.42843, it/sec: 4.427260867942601
epoch 1 iter 46660: train loss 1.41349. lr 3.000000e-04, running loss 1.42844, it/sec: 4.3503752742558355
epoch 1 iter 46680: train loss 1.38827. lr 3.000000e-04, running loss 1.42845, it/sec: 4.377950613284619
epoch 1 iter 46700: train loss 1.42080. lr 3.000000e-04, running loss 1.42838, it/sec: 4.372087637392771
epoch 1 iter 46720: train loss 1.52453. lr 3.000000e-04, running loss 1.42834, it/sec: 4.362557323658878
epoch 1 iter 46740: train loss 1.42440. lr 3.000000e-04, running loss 1.42839, it/sec: 4.340466336371978
epoch 1 iter 46760: train loss 1.43538. lr 3.000000e-04, running loss 1.42835, it/sec: 4.370910729110596
epoch 1 iter 46780: train loss 1.44783. lr 3.000000e-04, running loss 1.42838, it/sec: 4.390147259153633
epoch 1 iter 46800: train loss 1.43723. lr 3.000000e-04, running loss 1.42832, it/sec: 4.403939959669615
epoch 1 iter 46820: train loss 1.42449. lr 3.000000e-04, running loss 1.42830, it/sec: 4.341154581480648
epoch 1 iter 46840: train loss 1.43344. lr 3.000000e-04, running loss 1.42831, it/sec: 4.371584718186237
epoch 1 iter 46860: train loss 1.42614. lr 3.000000e-04, running loss 1.42830, it/sec: 4.419515547461515
epoch 1 iter 46880: train loss 1.41753. lr 3.000000e-04, running loss 1.42843, it/sec: 4.407434278984198
epoch 1 iter 46900: train loss 1.43626. lr 3.000000e-04, running loss 1.42842, it/sec: 4.399613033519449
epoch 1 iter 46920: train loss 1.39773. lr 3.000000e-04, running loss 1.42839, it/sec: 4.41889738427008
epoch 1 iter 46940: train loss 1.40968. lr 3.000000e-04, running loss 1.42837, it/sec: 4.407990188810598
epoch 1 iter 46960: train loss 1.42792. lr 3.000000e-04, running loss 1.42840, it/sec: 4.4167125156666875
epoch 1 iter 46980: train loss 1.42842. lr 3.000000e-04, running loss 1.42840, it/sec: 4.425003888540411
epoch 1 iter 47000: train loss 1.43636. lr 3.000000e-04, running loss 1.42836, it/sec: 4.43124096125068
epoch 1 iter 47020: train loss 1.43833. lr 3.000000e-04, running loss 1.42912, it/sec: 4.424421244737449
epoch 1 iter 47040: train loss 1.43753. lr 3.000000e-04, running loss 1.42908, it/sec: 4.421662584650885
epoch 1 iter 47060: train loss 1.40991. lr 3.000000e-04, running loss 1.42905, it/sec: 4.393711777701227
epoch 1 iter 47080: train loss 1.42855. lr 3.000000e-04, running loss 1.42904, it/sec: 4.4188907465310265
epoch 1 iter 47100: train loss 1.44768. lr 3.000000e-04, running loss 1.42906, it/sec: 4.428046658997376
epoch 1 iter 47120: train loss 1.58322. lr 3.000000e-04, running loss 1.42918, it/sec: 4.409654543862148
epoch 1 iter 47140: train loss 1.43485. lr 3.000000e-04, running loss 1.42924, it/sec: 4.431449229708048
epoch 1 iter 47160: train loss 1.41967. lr 3.000000e-04, running loss 1.42922, it/sec: 4.396066938950157
epoch 1 iter 47180: train loss 1.43115. lr 3.000000e-04, running loss 1.42915, it/sec: 4.377727392934194
epoch 1 iter 47200: train loss 1.42067. lr 3.000000e-04, running loss 1.42915, it/sec: 4.349055735663372
epoch 1 iter 47220: train loss 1.43355. lr 3.000000e-04, running loss 1.42922, it/sec: 4.359251381196752
epoch 1 iter 47240: train loss 1.41911. lr 3.000000e-04, running loss 1.42919, it/sec: 4.354583567746517
epoch 1 iter 47260: train loss 1.41754. lr 3.000000e-04, running loss 1.42913, it/sec: 4.376752864719294
epoch 1 iter 47280: train loss 1.43755. lr 3.000000e-04, running loss 1.42910, it/sec: 4.3697462734739405
epoch 1 iter 47300: train loss 1.42551. lr 3.000000e-04, running loss 1.42911, it/sec: 4.356976648159494
epoch 1 iter 47320: train loss 1.42837. lr 3.000000e-04, running loss 1.42906, it/sec: 4.370215346366649
epoch 1 iter 47340: train loss 1.42992. lr 3.000000e-04, running loss 1.42898, it/sec: 4.373576755305895
epoch 1 iter 47360: train loss 1.44714. lr 3.000000e-04, running loss 1.42902, it/sec: 4.363154703891342
epoch 1 iter 47380: train loss 1.45362. lr 3.000000e-04, running loss 1.42901, it/sec: 4.415400820574669
epoch 1 iter 47400: train loss 1.43028. lr 3.000000e-04, running loss 1.42890, it/sec: 4.370100947007374
epoch 1 iter 47420: train loss 1.42157. lr 3.000000e-04, running loss 1.42895, it/sec: 4.386996145959811
epoch 1 iter 47440: train loss 1.42550. lr 3.000000e-04, running loss 1.42891, it/sec: 4.363217354897342
epoch 1 iter 47460: train loss 1.42589. lr 3.000000e-04, running loss 1.42889, it/sec: 4.360216399624203
epoch 1 iter 47480: train loss 1.43186. lr 3.000000e-04, running loss 1.42891, it/sec: 4.373013675837948
epoch 1 iter 47500: train loss 1.43637. lr 3.000000e-04, running loss 1.42910, it/sec: 4.428456339015656
epoch 1 iter 47520: train loss 1.43861. lr 3.000000e-04, running loss 1.42903, it/sec: 4.3946696733653186
epoch 1 iter 47540: train loss 1.43966. lr 3.000000e-04, running loss 1.42909, it/sec: 4.413857841987636
epoch 1 iter 47560: train loss 1.41723. lr 3.000000e-04, running loss 1.42905, it/sec: 4.414888940856313
epoch 1 iter 47580: train loss 1.45886. lr 3.000000e-04, running loss 1.42911, it/sec: 4.426775491331199
epoch 1 iter 47600: train loss 1.43919. lr 3.000000e-04, running loss 1.42912, it/sec: 4.409506065848202
epoch 1 iter 47620: train loss 1.43645. lr 3.000000e-04, running loss 1.42912, it/sec: 4.417169912800348
epoch 1 iter 47640: train loss 1.42863. lr 3.000000e-04, running loss 1.42905, it/sec: 4.393659095643976
epoch 1 iter 47660: train loss 1.42713. lr 3.000000e-04, running loss 1.42902, it/sec: 4.387101748795244
epoch 1 iter 47680: train loss 1.43343. lr 3.000000e-04, running loss 1.42894, it/sec: 4.42399749524024
epoch 1 iter 47700: train loss 1.41305. lr 3.000000e-04, running loss 1.42886, it/sec: 4.426081870579576
epoch 1 iter 47720: train loss 1.44102. lr 3.000000e-04, running loss 1.42892, it/sec: 4.418654896964188
epoch 1 iter 47740: train loss 1.42746. lr 3.000000e-04, running loss 1.42889, it/sec: 4.430419626158257
epoch 1 iter 47760: train loss 1.42843. lr 3.000000e-04, running loss 1.42887, it/sec: 4.418891468270522
epoch 1 iter 47780: train loss 1.42976. lr 3.000000e-04, running loss 1.42888, it/sec: 4.374370624422578
epoch 1 iter 47800: train loss 1.42178. lr 3.000000e-04, running loss 1.42879, it/sec: 4.376634981987048
epoch 1 iter 47820: train loss 1.46158. lr 3.000000e-04, running loss 1.42883, it/sec: 4.36598492081673
epoch 1 iter 47840: train loss 1.43513. lr 3.000000e-04, running loss 1.42885, it/sec: 4.341500953278558
epoch 1 iter 47860: train loss 1.41500. lr 3.000000e-04, running loss 1.42872, it/sec: 4.378778050416956
epoch 1 iter 47880: train loss 1.43333. lr 3.000000e-04, running loss 1.42870, it/sec: 4.426606185497849
epoch 1 iter 47900: train loss 1.43452. lr 3.000000e-04, running loss 1.42874, it/sec: 4.41087017567167
epoch 1 iter 47920: train loss 1.45159. lr 3.000000e-04, running loss 1.42891, it/sec: 4.410892646358966
epoch 1 iter 47940: train loss 1.43481. lr 3.000000e-04, running loss 1.42888, it/sec: 4.400594786973662
epoch 1 iter 47960: train loss 1.40937. lr 3.000000e-04, running loss 1.42881, it/sec: 4.417405622038278
epoch 1 iter 47980: train loss 1.42590. lr 3.000000e-04, running loss 1.42876, it/sec: 4.4071707880587665
epoch 1 iter 48000: train loss 1.42344. lr 3.000000e-04, running loss 1.42879, it/sec: 4.359481730502264
epoch 1 iter 48020: train loss 1.41702. lr 3.000000e-04, running loss 1.42881, it/sec: 4.36616815086966
epoch 1 iter 48040: train loss 1.42561. lr 3.000000e-04, running loss 1.42873, it/sec: 4.393080585181425
epoch 1 iter 48060: train loss 1.41643. lr 3.000000e-04, running loss 1.42861, it/sec: 4.380193871386161
epoch 1 iter 48080: train loss 1.43414. lr 3.000000e-04, running loss 1.42862, it/sec: 4.420252070735402
epoch 1 iter 48100: train loss 1.42931. lr 3.000000e-04, running loss 1.42854, it/sec: 4.438451391043838
epoch 1 iter 48120: train loss 1.42003. lr 3.000000e-04, running loss 1.42855, it/sec: 4.414407579769229
epoch 1 iter 48140: train loss 1.47486. lr 3.000000e-04, running loss 1.42863, it/sec: 4.416244017521171
epoch 1 iter 48160: train loss 1.44295. lr 3.000000e-04, running loss 1.42858, it/sec: 4.416594673311604
epoch 1 iter 48180: train loss 1.43047. lr 3.000000e-04, running loss 1.42850, it/sec: 4.42525022722867
epoch 1 iter 48200: train loss 1.42405. lr 3.000000e-04, running loss 1.42854, it/sec: 4.415081386472181
epoch 1 iter 48220: train loss 1.42730. lr 3.000000e-04, running loss 1.42852, it/sec: 4.356421040497248
epoch 1 iter 48240: train loss 1.43012. lr 3.000000e-04, running loss 1.42914, it/sec: 4.384698157525494
epoch 1 iter 48260: train loss 1.43515. lr 3.000000e-04, running loss 1.42924, it/sec: 4.343694441073448
epoch 1 iter 48280: train loss 1.40063. lr 3.000000e-04, running loss 1.42914, it/sec: 4.382142914882566
epoch 1 iter 48300: train loss 1.42576. lr 3.000000e-04, running loss 1.42913, it/sec: 4.423408952555495
epoch 1 iter 48320: train loss 1.41367. lr 3.000000e-04, running loss 1.42911, it/sec: 4.40547442010716
epoch 1 iter 48340: train loss 1.42533. lr 3.000000e-04, running loss 1.42903, it/sec: 4.374292075462715
epoch 1 iter 48360: train loss 1.42715. lr 3.000000e-04, running loss 1.42899, it/sec: 4.379867905941048
epoch 1 iter 48380: train loss 1.42082. lr 3.000000e-04, running loss 1.42895, it/sec: 4.34779275955197
epoch 1 iter 48400: train loss 1.41603. lr 3.000000e-04, running loss 1.42889, it/sec: 4.360318722893428
epoch 1 iter 48420: train loss 1.42050. lr 3.000000e-04, running loss 1.42883, it/sec: 4.3606811098069915
epoch 1 iter 48440: train loss 1.43428. lr 3.000000e-04, running loss 1.42880, it/sec: 4.3586601438802335
epoch 1 iter 48460: train loss 1.42275. lr 3.000000e-04, running loss 1.42880, it/sec: 4.360453600893568
epoch 1 iter 48480: train loss 1.40393. lr 3.000000e-04, running loss 1.42876, it/sec: 4.4052948628834905
epoch 1 iter 48500: train loss 1.41520. lr 3.000000e-04, running loss 1.42879, it/sec: 4.392591560661891
epoch 1 iter 48520: train loss 1.41825. lr 3.000000e-04, running loss 1.42881, it/sec: 4.426154256851928
epoch 1 iter 48540: train loss 1.42836. lr 3.000000e-04, running loss 1.42881, it/sec: 4.4063234212116065
epoch 1 iter 48560: train loss 1.40291. lr 3.000000e-04, running loss 1.42870, it/sec: 4.406082485921681
epoch 1 iter 48580: train loss 1.42198. lr 3.000000e-04, running loss 1.42860, it/sec: 4.394913186929602
epoch 1 iter 48600: train loss 1.43151. lr 3.000000e-04, running loss 1.42858, it/sec: 4.370440989945717
epoch 1 iter 48620: train loss 1.42917. lr 3.000000e-04, running loss 1.42861, it/sec: 4.419319708219464
epoch 1 iter 48640: train loss 1.42843. lr 3.000000e-04, running loss 1.42859, it/sec: 4.431896425513269
epoch 1 iter 48660: train loss 1.44166. lr 3.000000e-04, running loss 1.42864, it/sec: 4.42105963262303
epoch 1 iter 48680: train loss 1.43240. lr 3.000000e-04, running loss 1.42889, it/sec: 4.4266472372717445
epoch 1 iter 48700: train loss 1.42552. lr 3.000000e-04, running loss 1.42889, it/sec: 4.412880911449898
epoch 1 iter 48720: train loss 1.44055. lr 3.000000e-04, running loss 1.42895, it/sec: 4.43072235945621
epoch 1 iter 48740: train loss 1.49681. lr 3.000000e-04, running loss 1.42898, it/sec: 4.393689963778297
epoch 1 iter 48760: train loss 1.41479. lr 3.000000e-04, running loss 1.42889, it/sec: 4.37937671553336
epoch 1 iter 48780: train loss 1.41905. lr 3.000000e-04, running loss 1.42882, it/sec: 4.359778343932946
epoch 1 iter 48800: train loss 1.43499. lr 3.000000e-04, running loss 1.42878, it/sec: 4.362004782862948
epoch 1 iter 48820: train loss 1.42612. lr 3.000000e-04, running loss 1.42868, it/sec: 4.364012665305546
epoch 1 iter 48840: train loss 1.43413. lr 3.000000e-04, running loss 1.42862, it/sec: 4.3448575101387
epoch 1 iter 48860: train loss 1.43698. lr 3.000000e-04, running loss 1.42865, it/sec: 4.368364121876078
epoch 1 iter 48880: train loss 1.44425. lr 3.000000e-04, running loss 1.42873, it/sec: 4.371654684327545
epoch 1 iter 48900: train loss 1.43062. lr 3.000000e-04, running loss 1.42866, it/sec: 4.364591983107618
epoch 1 iter 48920: train loss 1.43250. lr 3.000000e-04, running loss 1.42867, it/sec: 4.3912031162766505
epoch 1 iter 48940: train loss 1.43573. lr 3.000000e-04, running loss 1.42871, it/sec: 4.362733813747611
epoch 1 iter 48960: train loss 1.45144. lr 3.000000e-04, running loss 1.42866, it/sec: 4.395424984561374
epoch 1 iter 48980: train loss 1.43312. lr 3.000000e-04, running loss 1.42861, it/sec: 4.403912827388536
epoch 1 iter 49000: train loss 1.42938. lr 3.000000e-04, running loss 1.42858, it/sec: 4.3896315438892195
epoch 1 iter 49020: train loss 1.42150. lr 3.000000e-04, running loss 1.42853, it/sec: 4.410114661027993
epoch 1 iter 49040: train loss 1.57354. lr 3.000000e-04, running loss 1.42868, it/sec: 4.426829578533859
epoch 1 iter 49060: train loss 1.42605. lr 3.000000e-04, running loss 1.42876, it/sec: 4.425722321083943
epoch 1 iter 49080: train loss 1.42012. lr 3.000000e-04, running loss 1.42875, it/sec: 4.413463343879496
epoch 1 iter 49100: train loss 1.44198. lr 3.000000e-04, running loss 1.42870, it/sec: 4.375508712378468
epoch 1 iter 49120: train loss 1.41628. lr 3.000000e-04, running loss 1.42869, it/sec: 4.359255505938567
epoch 1 iter 49140: train loss 1.42810. lr 3.000000e-04, running loss 1.42863, it/sec: 4.3825446823200815
epoch 1 iter 49160: train loss 1.42989. lr 3.000000e-04, running loss 1.42869, it/sec: 4.371169233573502
epoch 1 iter 49180: train loss 1.41935. lr 3.000000e-04, running loss 1.42863, it/sec: 4.384745972803799
epoch 1 iter 49200: train loss 1.40079. lr 3.000000e-04, running loss 1.42862, it/sec: 4.394674057639048
epoch 1 iter 49220: train loss 1.42573. lr 3.000000e-04, running loss 1.42861, it/sec: 4.412078885015784
epoch 1 iter 49240: train loss 1.43079. lr 3.000000e-04, running loss 1.42860, it/sec: 4.393123256379469
epoch 1 iter 49260: train loss 1.42174. lr 3.000000e-04, running loss 1.42857, it/sec: 4.348691060146579
epoch 1 iter 49280: train loss 1.41442. lr 3.000000e-04, running loss 1.42859, it/sec: 4.349256822524423
epoch 1 iter 49300: train loss 1.42474. lr 3.000000e-04, running loss 1.42856, it/sec: 4.378211196344429
epoch 1 iter 49320: train loss 1.42005. lr 3.000000e-04, running loss 1.42852, it/sec: 4.368947688795634
epoch 1 iter 49340: train loss 1.42630. lr 3.000000e-04, running loss 1.42853, it/sec: 4.427547780234737
epoch 1 iter 49360: train loss 1.43286. lr 3.000000e-04, running loss 1.42850, it/sec: 4.409319004983854
epoch 1 iter 49380: train loss 1.42926. lr 3.000000e-04, running loss 1.42844, it/sec: 4.424778585085092
epoch 1 iter 49400: train loss 1.42191. lr 3.000000e-04, running loss 1.42843, it/sec: 4.422145471290561
epoch 1 iter 49420: train loss 1.42252. lr 3.000000e-04, running loss 1.42834, it/sec: 4.4221438094135115
epoch 1 iter 49440: train loss 1.42567. lr 3.000000e-04, running loss 1.42835, it/sec: 4.41957924312298
epoch 1 iter 49460: train loss 1.45199. lr 3.000000e-04, running loss 1.42841, it/sec: 4.419636533722523
epoch 1 iter 49480: train loss 1.43702. lr 3.000000e-04, running loss 1.42846, it/sec: 4.331513681423481
epoch 1 iter 49500: train loss 1.42928. lr 3.000000e-04, running loss 1.42838, it/sec: 4.433930511358915
epoch 1 iter 49520: train loss 1.42105. lr 3.000000e-04, running loss 1.42828, it/sec: 4.412786058144761
epoch 1 iter 49540: train loss 1.41774. lr 3.000000e-04, running loss 1.42823, it/sec: 4.423286117435195
epoch 1 iter 49560: train loss 1.41584. lr 3.000000e-04, running loss 1.42823, it/sec: 4.441136764917441
epoch 1 iter 49580: train loss 1.43021. lr 3.000000e-04, running loss 1.42824, it/sec: 4.418159419392504
epoch 1 iter 49600: train loss 1.44975. lr 3.000000e-04, running loss 1.42824, it/sec: 4.418915388101058
epoch 1 iter 49620: train loss 1.43571. lr 3.000000e-04, running loss 1.42824, it/sec: 4.405558051585901
epoch 1 iter 49640: train loss 1.42093. lr 3.000000e-04, running loss 1.42825, it/sec: 4.374770278312288
epoch 1 iter 49660: train loss 1.42073. lr 3.000000e-04, running loss 1.42818, it/sec: 4.35383252124045
epoch 1 iter 49680: train loss 1.42241. lr 3.000000e-04, running loss 1.42813, it/sec: 4.388863082648651
epoch 1 iter 49700: train loss 1.42792. lr 3.000000e-04, running loss 1.42814, it/sec: 4.3679194091111535
epoch 1 iter 49720: train loss 1.43165. lr 3.000000e-04, running loss 1.42811, it/sec: 4.346439025470179
epoch 1 iter 49740: train loss 1.43603. lr 3.000000e-04, running loss 1.42809, it/sec: 4.350031420531392
epoch 1 iter 49760: train loss 1.43178. lr 3.000000e-04, running loss 1.42816, it/sec: 4.347557709341585
epoch 1 iter 49780: train loss 1.42990. lr 3.000000e-04, running loss 1.42811, it/sec: 4.384847604017514
epoch 1 iter 49800: train loss 1.43560. lr 3.000000e-04, running loss 1.42818, it/sec: 4.375239608173468
epoch 1 iter 49820: train loss 1.40665. lr 3.000000e-04, running loss 1.42818, it/sec: 4.35082143827802
epoch 1 iter 49840: train loss 1.44276. lr 3.000000e-04, running loss 1.42823, it/sec: 4.353235909330734
epoch 1 iter 49860: train loss 1.41942. lr 3.000000e-04, running loss 1.42810, it/sec: 4.36809589352407
epoch 1 iter 49880: train loss 1.42642. lr 3.000000e-04, running loss 1.42811, it/sec: 4.3267209823043045
epoch 1 iter 49900: train loss 1.43118. lr 3.000000e-04, running loss 1.42819, it/sec: 4.421800796271339
epoch 1 iter 49920: train loss 1.42740. lr 3.000000e-04, running loss 1.42836, it/sec: 4.427100364072096
epoch 1 iter 49940: train loss 1.41733. lr 3.000000e-04, running loss 1.42835, it/sec: 4.424922335634092
epoch 1 iter 49960: train loss 1.43842. lr 3.000000e-04, running loss 1.42837, it/sec: 4.424132367762439
epoch 1 iter 49980: train loss 1.42048. lr 3.000000e-04, running loss 1.42842, it/sec: 4.423901713469466
epoch 1 iter 50000: train loss 1.43116. lr 3.000000e-04, running loss 1.42839, it/sec: 1.1281083306009971
epoch 1 iter 50020: train loss 1.44304. lr 3.000000e-04, running loss 1.42841, it/sec: 4.404195364266268
epoch 1 iter 50040: train loss 1.43000. lr 3.000000e-04, running loss 1.42861, it/sec: 4.41048930279545
epoch 1 iter 50060: train loss 1.42644. lr 3.000000e-04, running loss 1.42865, it/sec: 4.411440225232084
epoch 1 iter 50080: train loss 1.42563. lr 3.000000e-04, running loss 1.42861, it/sec: 4.441779711853129
epoch 1 iter 50100: train loss 1.40601. lr 3.000000e-04, running loss 1.42850, it/sec: 4.425472737871738
epoch 1 iter 50120: train loss 1.42911. lr 3.000000e-04, running loss 1.42852, it/sec: 4.422601588787338
epoch 1 iter 50140: train loss 1.43392. lr 3.000000e-04, running loss 1.42855, it/sec: 4.432824218330925
epoch 1 iter 50160: train loss 1.41882. lr 3.000000e-04, running loss 1.42853, it/sec: 4.394070410857474
epoch 1 iter 50180: train loss 1.42549. lr 3.000000e-04, running loss 1.42848, it/sec: 4.411819042894481
epoch 1 iter 50200: train loss 1.40775. lr 3.000000e-04, running loss 1.42844, it/sec: 4.378037055453663
epoch 1 iter 50220: train loss 1.43171. lr 3.000000e-04, running loss 1.42846, it/sec: 4.390033337316088
epoch 1 iter 50240: train loss 1.43585. lr 3.000000e-04, running loss 1.42851, it/sec: 4.360787447488117
epoch 1 iter 50260: train loss 1.43487. lr 3.000000e-04, running loss 1.42855, it/sec: 4.3555424413236095
epoch 1 iter 50280: train loss 1.44553. lr 3.000000e-04, running loss 1.42855, it/sec: 4.373565489890527
epoch 1 iter 50300: train loss 1.40935. lr 3.000000e-04, running loss 1.42862, it/sec: 4.342262571849901
epoch 1 iter 50320: train loss 1.43738. lr 3.000000e-04, running loss 1.42862, it/sec: 4.370860178160538
epoch 1 iter 50340: train loss 1.41779. lr 3.000000e-04, running loss 1.42860, it/sec: 4.35756011746367
epoch 1 iter 50360: train loss 1.40856. lr 3.000000e-04, running loss 1.42867, it/sec: 4.3496128887335255
epoch 1 iter 50380: train loss 1.42304. lr 3.000000e-04, running loss 1.42874, it/sec: 4.372087733080517
epoch 1 iter 50400: train loss 1.44048. lr 3.000000e-04, running loss 1.42892, it/sec: 4.368247396988276
epoch 1 iter 50420: train loss 1.42709. lr 3.000000e-04, running loss 1.42884, it/sec: 4.393919448373965
epoch 1 iter 50440: train loss 1.44466. lr 3.000000e-04, running loss 1.42888, it/sec: 4.41527435495926
epoch 1 iter 50460: train loss 1.43745. lr 3.000000e-04, running loss 1.42891, it/sec: 4.4245349995221295
epoch 1 iter 50480: train loss 1.42213. lr 3.000000e-04, running loss 1.42891, it/sec: 4.4252193243940505
epoch 1 iter 50500: train loss 1.41981. lr 3.000000e-04, running loss 1.42894, it/sec: 4.427077393687674
epoch 1 iter 50520: train loss 1.43003. lr 3.000000e-04, running loss 1.42893, it/sec: 4.424031844310387
epoch 1 iter 50540: train loss 1.43025. lr 3.000000e-04, running loss 1.42890, it/sec: 4.366619656835306
epoch 1 iter 50560: train loss 1.44067. lr 3.000000e-04, running loss 1.42898, it/sec: 4.4239147673335815
epoch 1 iter 50580: train loss 1.42386. lr 3.000000e-04, running loss 1.42896, it/sec: 4.3924239329769605
epoch 1 iter 50600: train loss 1.43506. lr 3.000000e-04, running loss 1.42897, it/sec: 4.354302126840625
epoch 1 iter 50620: train loss 1.42632. lr 3.000000e-04, running loss 1.42896, it/sec: 4.4120933478998206
epoch 1 iter 50640: train loss 1.43243. lr 3.000000e-04, running loss 1.42897, it/sec: 4.393848594693079
epoch 1 iter 50660: train loss 1.41839. lr 3.000000e-04, running loss 1.42894, it/sec: 4.388382255892744
epoch 1 iter 50680: train loss 1.44109. lr 3.000000e-04, running loss 1.42886, it/sec: 4.431483025885998
epoch 1 iter 50700: train loss 1.42200. lr 3.000000e-04, running loss 1.42882, it/sec: 4.396299823059587
epoch 1 iter 50720: train loss 1.43427. lr 3.000000e-04, running loss 1.42878, it/sec: 4.3510306588936905
epoch 1 iter 50740: train loss 1.44887. lr 3.000000e-04, running loss 1.42887, it/sec: 4.3543553669425945
epoch 1 iter 50760: train loss 1.43163. lr 3.000000e-04, running loss 1.42883, it/sec: 4.375165697674516
epoch 1 iter 50780: train loss 1.42646. lr 3.000000e-04, running loss 1.42881, it/sec: 4.337412393915919
epoch 1 iter 50800: train loss 1.43451. lr 3.000000e-04, running loss 1.42882, it/sec: 4.361086843531381
epoch 1 iter 50820: train loss 1.42300. lr 3.000000e-04, running loss 1.42884, it/sec: 4.431126289938315
epoch 1 iter 50840: train loss 1.43123. lr 3.000000e-04, running loss 1.42881, it/sec: 4.426821268658792
epoch 1 iter 50860: train loss 1.42054. lr 3.000000e-04, running loss 1.42894, it/sec: 4.423753741646422
epoch 1 iter 50880: train loss 1.40072. lr 3.000000e-04, running loss 1.42887, it/sec: 4.438376572436079
epoch 1 iter 50900: train loss 1.43296. lr 3.000000e-04, running loss 1.42884, it/sec: 4.415924051647584
epoch 1 iter 50920: train loss 1.44824. lr 3.000000e-04, running loss 1.42892, it/sec: 4.399750506739233
epoch 1 iter 50940: train loss 1.41061. lr 3.000000e-04, running loss 1.42888, it/sec: 4.423391205929768
epoch 1 iter 50960: train loss 1.43632. lr 3.000000e-04, running loss 1.42880, it/sec: 4.412395101500004
epoch 1 iter 50980: train loss 1.41378. lr 3.000000e-04, running loss 1.42876, it/sec: 4.404986765236997
epoch 1 iter 51000: train loss 1.43337. lr 3.000000e-04, running loss 1.42878, it/sec: 4.421077713239153
epoch 1 iter 51020: train loss 1.42818. lr 3.000000e-04, running loss 1.42891, it/sec: 4.435538986373964
epoch 1 iter 51040: train loss 1.41854. lr 3.000000e-04, running loss 1.42894, it/sec: 4.4110105923436045
epoch 1 iter 51060: train loss 1.43511. lr 3.000000e-04, running loss 1.42886, it/sec: 4.434779601229109
epoch 1 iter 51080: train loss 1.42725. lr 3.000000e-04, running loss 1.42886, it/sec: 4.38886953518083
epoch 1 iter 51100: train loss 1.41886. lr 3.000000e-04, running loss 1.42887, it/sec: 4.344451996638578
epoch 1 iter 51120: train loss 1.42843. lr 3.000000e-04, running loss 1.42887, it/sec: 4.351999806293496
epoch 1 iter 51140: train loss 1.43530. lr 3.000000e-04, running loss 1.42893, it/sec: 4.31391662016597
epoch 1 iter 51160: train loss 1.42785. lr 3.000000e-04, running loss 1.42891, it/sec: 4.353190467578507
epoch 1 iter 51180: train loss 1.42729. lr 3.000000e-04, running loss 1.42903, it/sec: 4.377497123677747
epoch 1 iter 51200: train loss 1.41387. lr 3.000000e-04, running loss 1.42903, it/sec: 4.396917344459174
epoch 1 iter 51220: train loss 1.42617. lr 3.000000e-04, running loss 1.42910, it/sec: 4.366311359813468
epoch 1 iter 51240: train loss 1.41774. lr 3.000000e-04, running loss 1.42902, it/sec: 4.378607448921385
epoch 1 iter 51260: train loss 1.44742. lr 3.000000e-04, running loss 1.42896, it/sec: 4.358989231098537
epoch 1 iter 51280: train loss 1.42924. lr 3.000000e-04, running loss 1.42898, it/sec: 4.417982887203863
epoch 1 iter 51300: train loss 1.41470. lr 3.000000e-04, running loss 1.42888, it/sec: 4.408904461346987
epoch 1 iter 51320: train loss 1.43890. lr 3.000000e-04, running loss 1.42892, it/sec: 4.4183011800418175
epoch 1 iter 51340: train loss 1.43665. lr 3.000000e-04, running loss 1.42896, it/sec: 4.432621401725128
epoch 1 iter 51360: train loss 1.44603. lr 3.000000e-04, running loss 1.42903, it/sec: 4.420302070326556
epoch 1 iter 51380: train loss 1.40853. lr 3.000000e-04, running loss 1.42899, it/sec: 4.399011705793476
epoch 1 iter 51400: train loss 1.43514. lr 3.000000e-04, running loss 1.42901, it/sec: 4.407550290783815
epoch 1 iter 51420: train loss 1.43743. lr 3.000000e-04, running loss 1.42898, it/sec: 4.379479441040641
epoch 1 iter 51440: train loss 1.42835. lr 3.000000e-04, running loss 1.42901, it/sec: 4.361233390574687
epoch 1 iter 51460: train loss 1.44850. lr 3.000000e-04, running loss 1.42901, it/sec: 4.3738911114098125
epoch 1 iter 51480: train loss 1.42917. lr 3.000000e-04, running loss 1.42896, it/sec: 4.338834607108068
epoch 1 iter 51500: train loss 1.43544. lr 3.000000e-04, running loss 1.42896, it/sec: 4.347592017087129
epoch 1 iter 51520: train loss 1.40079. lr 3.000000e-04, running loss 1.42895, it/sec: 4.354080061198455
epoch 1 iter 51540: train loss 1.41668. lr 3.000000e-04, running loss 1.43013, it/sec: 4.382881152136364
epoch 1 iter 51560: train loss 1.43502. lr 3.000000e-04, running loss 1.43015, it/sec: 4.351725517026778
epoch 1 iter 51580: train loss 1.43527. lr 3.000000e-04, running loss 1.43014, it/sec: 4.377157016821494
epoch 1 iter 51600: train loss 1.41988. lr 3.000000e-04, running loss 1.43018, it/sec: 4.3631958427206285
epoch 1 iter 51620: train loss 1.43187. lr 3.000000e-04, running loss 1.43015, it/sec: 4.330515059641484
epoch 1 iter 51640: train loss 1.43720. lr 3.000000e-04, running loss 1.43024, it/sec: 4.379936794149917
epoch 1 iter 51660: train loss 1.41982. lr 3.000000e-04, running loss 1.43013, it/sec: 4.426922801842854
epoch 1 iter 51680: train loss 1.42832. lr 3.000000e-04, running loss 1.43010, it/sec: 4.450723644561346
epoch 1 iter 51700: train loss 1.42322. lr 3.000000e-04, running loss 1.43012, it/sec: 4.4185446047096235
epoch 1 iter 51720: train loss 1.43472. lr 3.000000e-04, running loss 1.42998, it/sec: 4.438718360458022
epoch 1 iter 51740: train loss 1.43420. lr 3.000000e-04, running loss 1.43004, it/sec: 4.425814773760381
epoch 1 iter 51760: train loss 1.42140. lr 3.000000e-04, running loss 1.43003, it/sec: 4.427973935767538
epoch 1 iter 51780: train loss 1.41380. lr 3.000000e-04, running loss 1.42990, it/sec: 4.4023511004261975
epoch 1 iter 51800: train loss 1.42073. lr 3.000000e-04, running loss 1.42986, it/sec: 4.4123560848926235
epoch 1 iter 51820: train loss 1.41117. lr 3.000000e-04, running loss 1.42992, it/sec: 4.430473978402455
epoch 1 iter 51840: train loss 1.42484. lr 3.000000e-04, running loss 1.42988, it/sec: 4.426318082597014
epoch 1 iter 51860: train loss 1.43577. lr 3.000000e-04, running loss 1.42987, it/sec: 4.431461562292701
epoch 1 iter 51880: train loss 1.44180. lr 3.000000e-04, running loss 1.42984, it/sec: 4.404536175282654
epoch 1 iter 51900: train loss 1.41952. lr 3.000000e-04, running loss 1.42981, it/sec: 4.4305554207173
epoch 1 iter 51920: train loss 1.42323. lr 3.000000e-04, running loss 1.42985, it/sec: 4.404323562629267
epoch 1 iter 51940: train loss 1.44186. lr 3.000000e-04, running loss 1.42991, it/sec: 4.386267083475508
epoch 1 iter 51960: train loss 1.41755. lr 3.000000e-04, running loss 1.42989, it/sec: 4.378041885230657
epoch 1 iter 51980: train loss 1.42193. lr 3.000000e-04, running loss 1.42979, it/sec: 4.370393582871408
epoch 1 iter 52000: train loss 1.42862. lr 3.000000e-04, running loss 1.42984, it/sec: 4.370298427169412
epoch 1 iter 52020: train loss 1.42952. lr 3.000000e-04, running loss 1.42975, it/sec: 4.3517524267470655
epoch 1 iter 52040: train loss 1.43497. lr 3.000000e-04, running loss 1.42973, it/sec: 4.414114689754178
epoch 1 iter 52060: train loss 1.43963. lr 3.000000e-04, running loss 1.42962, it/sec: 4.388480031324933
epoch 1 iter 52080: train loss 1.42740. lr 3.000000e-04, running loss 1.42965, it/sec: 4.414920165879949
epoch 1 iter 52100: train loss 1.42810. lr 3.000000e-04, running loss 1.42955, it/sec: 4.412327660058773
epoch 1 iter 52120: train loss 1.42140. lr 3.000000e-04, running loss 1.42954, it/sec: 4.420978382034682
epoch 1 iter 52140: train loss 1.42835. lr 3.000000e-04, running loss 1.42944, it/sec: 4.362270951074877
epoch 1 iter 52160: train loss 1.41847. lr 3.000000e-04, running loss 1.42936, it/sec: 4.372973728567717
epoch 1 iter 52180: train loss 1.42994. lr 3.000000e-04, running loss 1.42934, it/sec: 4.367110734907218
epoch 1 iter 52200: train loss 1.41456. lr 3.000000e-04, running loss 1.42931, it/sec: 4.369333504073674
epoch 1 iter 52220: train loss 1.43268. lr 3.000000e-04, running loss 1.42926, it/sec: 4.351258018716302
epoch 1 iter 52240: train loss 1.41174. lr 3.000000e-04, running loss 1.42927, it/sec: 4.374913573057964
epoch 1 iter 52260: train loss 1.43127. lr 3.000000e-04, running loss 1.42922, it/sec: 4.388347053633818
epoch 1 iter 52280: train loss 1.42226. lr 3.000000e-04, running loss 1.42920, it/sec: 4.42155836005991
epoch 1 iter 52300: train loss 1.43217. lr 3.000000e-04, running loss 1.42915, it/sec: 4.421980312998639
epoch 1 iter 52320: train loss 1.42980. lr 3.000000e-04, running loss 1.42910, it/sec: 4.4273689293366845
epoch 1 iter 52340: train loss 1.45298. lr 3.000000e-04, running loss 1.42907, it/sec: 4.435004585945828
epoch 1 iter 52360: train loss 1.41762. lr 3.000000e-04, running loss 1.42896, it/sec: 4.427825006687188
epoch 1 iter 52380: train loss 1.42457. lr 3.000000e-04, running loss 1.42896, it/sec: 4.425420114467515
epoch 1 iter 52400: train loss 1.40223. lr 3.000000e-04, running loss 1.42901, it/sec: 4.421778897156855
epoch 1 iter 52420: train loss 1.43761. lr 3.000000e-04, running loss 1.42903, it/sec: 4.426772983027466
epoch 1 iter 52440: train loss 1.44349. lr 3.000000e-04, running loss 1.42905, it/sec: 4.411893513775598
epoch 1 iter 52460: train loss 1.42079. lr 3.000000e-04, running loss 1.42906, it/sec: 4.435186947701607
epoch 1 iter 52480: train loss 1.43765. lr 3.000000e-04, running loss 1.42916, it/sec: 4.4123798172973245
epoch 1 iter 52500: train loss 1.42101. lr 3.000000e-04, running loss 1.42910, it/sec: 4.436163939578243
epoch 1 iter 52520: train loss 1.42395. lr 3.000000e-04, running loss 1.42903, it/sec: 4.431138052700957
epoch 1 iter 52540: train loss 1.43861. lr 3.000000e-04, running loss 1.42904, it/sec: 4.433823132970518
epoch 1 iter 52560: train loss 1.40955. lr 3.000000e-04, running loss 1.42900, it/sec: 4.43471000049023
epoch 1 iter 52580: train loss 1.42358. lr 3.000000e-04, running loss 1.42900, it/sec: 4.404660843000075
epoch 1 iter 52600: train loss 1.41773. lr 3.000000e-04, running loss 1.42901, it/sec: 4.391651079910265
epoch 1 iter 52620: train loss 1.43005. lr 3.000000e-04, running loss 1.42897, it/sec: 4.417193501672353
epoch 1 iter 52640: train loss 1.43882. lr 3.000000e-04, running loss 1.42892, it/sec: 4.385680324076163
epoch 1 iter 52660: train loss 1.42749. lr 3.000000e-04, running loss 1.42917, it/sec: 4.3497690719765405
epoch 1 iter 52680: train loss 1.43265. lr 3.000000e-04, running loss 1.42920, it/sec: 4.361308141984013
epoch 1 iter 52700: train loss 1.43664. lr 3.000000e-04, running loss 1.42927, it/sec: 4.380474851745506
epoch 1 iter 52720: train loss 1.40189. lr 3.000000e-04, running loss 1.42925, it/sec: 4.422836745152483
epoch 1 iter 52740: train loss 1.43062. lr 3.000000e-04, running loss 1.42929, it/sec: 4.418617390161121
epoch 1 iter 52760: train loss 1.41745. lr 3.000000e-04, running loss 1.42925, it/sec: 4.416808786926982
epoch 1 iter 52780: train loss 1.41923. lr 3.000000e-04, running loss 1.42920, it/sec: 4.435080668275175
epoch 1 iter 52800: train loss 1.43207. lr 3.000000e-04, running loss 1.42929, it/sec: 4.408649540947911
epoch 1 iter 52820: train loss 1.42170. lr 3.000000e-04, running loss 1.42927, it/sec: 4.424632492976479
epoch 1 iter 52840: train loss 1.53991. lr 3.000000e-04, running loss 1.42943, it/sec: 4.4039782066612565
epoch 1 iter 52860: train loss 1.43467. lr 3.000000e-04, running loss 1.42940, it/sec: 4.38714210920823
epoch 1 iter 52880: train loss 1.43742. lr 3.000000e-04, running loss 1.42935, it/sec: 4.3749515470426745
epoch 1 iter 52900: train loss 1.42633. lr 3.000000e-04, running loss 1.42931, it/sec: 4.38595227395411
epoch 1 iter 52920: train loss 1.43513. lr 3.000000e-04, running loss 1.42923, it/sec: 4.363213433200525
epoch 1 iter 52940: train loss 1.42068. lr 3.000000e-04, running loss 1.42918, it/sec: 4.391417646352754
epoch 1 iter 52960: train loss 1.43156. lr 3.000000e-04, running loss 1.42910, it/sec: 4.371007477312481
epoch 1 iter 52980: train loss 1.41266. lr 3.000000e-04, running loss 1.42908, it/sec: 4.375580067941534
epoch 1 iter 53000: train loss 1.43884. lr 3.000000e-04, running loss 1.42908, it/sec: 4.350986132594758
epoch 1 iter 53020: train loss 1.42573. lr 3.000000e-04, running loss 1.42910, it/sec: 4.355957390213541
epoch 1 iter 53040: train loss 1.43942. lr 3.000000e-04, running loss 1.42914, it/sec: 4.358363416108842
epoch 1 iter 53060: train loss 1.43076. lr 3.000000e-04, running loss 1.42913, it/sec: 4.3920295728207215
epoch 1 iter 53080: train loss 1.42495. lr 3.000000e-04, running loss 1.42908, it/sec: 4.372513335753741
epoch 1 iter 53100: train loss 1.43920. lr 3.000000e-04, running loss 1.42915, it/sec: 4.378722084268742
epoch 1 iter 53120: train loss 1.45102. lr 3.000000e-04, running loss 1.42916, it/sec: 4.3415302831784865
epoch 1 iter 53140: train loss 1.42768. lr 3.000000e-04, running loss 1.42922, it/sec: 4.3677902685570125
epoch 1 iter 53160: train loss 1.44851. lr 3.000000e-04, running loss 1.42916, it/sec: 4.346102403634967
epoch 1 iter 53180: train loss 1.42525. lr 3.000000e-04, running loss 1.42912, it/sec: 4.376393317988962
epoch 1 iter 53200: train loss 1.42750. lr 3.000000e-04, running loss 1.42907, it/sec: 4.350724160277467
epoch 1 iter 53220: train loss 1.41066. lr 3.000000e-04, running loss 1.42898, it/sec: 4.40914034050811
epoch 1 iter 53240: train loss 1.43487. lr 3.000000e-04, running loss 1.42895, it/sec: 4.36752281794779
epoch 1 iter 53260: train loss 1.43579. lr 3.000000e-04, running loss 1.42892, it/sec: 4.408716868865323
epoch 1 iter 53280: train loss 1.43683. lr 3.000000e-04, running loss 1.42891, it/sec: 4.400846412679727
epoch 1 iter 53300: train loss 1.45129. lr 3.000000e-04, running loss 1.42892, it/sec: 4.358430926281254
epoch 1 iter 53320: train loss 1.43401. lr 3.000000e-04, running loss 1.42893, it/sec: 4.366627627913545
epoch 1 iter 53340: train loss 1.42399. lr 3.000000e-04, running loss 1.42888, it/sec: 4.381377283205368
epoch 1 iter 53360: train loss 1.40845. lr 3.000000e-04, running loss 1.42882, it/sec: 4.3717262769057035
epoch 1 iter 53380: train loss 1.44136. lr 3.000000e-04, running loss 1.42877, it/sec: 4.326874307521058
epoch 1 iter 53400: train loss 1.43827. lr 3.000000e-04, running loss 1.42874, it/sec: 4.415250884166569
epoch 1 iter 53420: train loss 1.44118. lr 3.000000e-04, running loss 1.42870, it/sec: 4.391288058817227
epoch 1 iter 53440: train loss 1.42772. lr 3.000000e-04, running loss 1.42861, it/sec: 4.408984607937888
epoch 1 iter 53460: train loss 1.42467. lr 3.000000e-04, running loss 1.42861, it/sec: 4.36465052490712
epoch 1 iter 53480: train loss 1.41147. lr 3.000000e-04, running loss 1.42851, it/sec: 4.396047208527853
epoch 1 iter 53500: train loss 1.42631. lr 3.000000e-04, running loss 1.42852, it/sec: 4.418505012212919
epoch 1 iter 53520: train loss 1.40962. lr 3.000000e-04, running loss 1.42858, it/sec: 4.395547823473912
epoch 1 iter 53540: train loss 1.41942. lr 3.000000e-04, running loss 1.42864, it/sec: 4.438014058575709
epoch 1 iter 53560: train loss 1.43372. lr 3.000000e-04, running loss 1.42866, it/sec: 4.414999342275733
epoch 1 iter 53580: train loss 1.42766. lr 3.000000e-04, running loss 1.42867, it/sec: 4.399636396938452
epoch 1 iter 53600: train loss 1.42398. lr 3.000000e-04, running loss 1.42870, it/sec: 4.410489457917765
epoch 1 iter 53620: train loss 1.41683. lr 3.000000e-04, running loss 1.42869, it/sec: 4.377632855724652
epoch 1 iter 53640: train loss 1.43498. lr 3.000000e-04, running loss 1.42869, it/sec: 4.379602867395451
epoch 1 iter 53660: train loss 1.43495. lr 3.000000e-04, running loss 1.42875, it/sec: 4.399750797446563
epoch 1 iter 53680: train loss 1.45392. lr 3.000000e-04, running loss 1.42870, it/sec: 4.400614035210469
epoch 1 iter 53700: train loss 1.42360. lr 3.000000e-04, running loss 1.42862, it/sec: 4.413997454357372
epoch 1 iter 53720: train loss 1.42424. lr 3.000000e-04, running loss 1.42858, it/sec: 4.42109983864435
epoch 1 iter 53740: train loss 1.41762. lr 3.000000e-04, running loss 1.42853, it/sec: 4.431142470039047
epoch 1 iter 53760: train loss 1.43076. lr 3.000000e-04, running loss 1.42846, it/sec: 4.42001324426661
epoch 1 iter 53780: train loss 1.41201. lr 3.000000e-04, running loss 1.42846, it/sec: 4.420908412923539
epoch 1 iter 53800: train loss 1.41428. lr 3.000000e-04, running loss 1.42841, it/sec: 4.433680159434763
epoch 1 iter 53820: train loss 1.43299. lr 3.000000e-04, running loss 1.42841, it/sec: 4.449454906544522
epoch 1 iter 53840: train loss 1.44618. lr 3.000000e-04, running loss 1.42842, it/sec: 4.433748862795405
epoch 1 iter 53860: train loss 1.41387. lr 3.000000e-04, running loss 1.42841, it/sec: 4.432134515198502
epoch 1 iter 53880: train loss 1.42026. lr 3.000000e-04, running loss 1.42829, it/sec: 4.412496810383259
epoch 1 iter 53900: train loss 1.43162. lr 3.000000e-04, running loss 1.42828, it/sec: 4.425014011773186
epoch 1 iter 53920: train loss 1.46415. lr 3.000000e-04, running loss 1.42870, it/sec: 4.4212755652542715
epoch 1 iter 53940: train loss 1.42509. lr 3.000000e-04, running loss 1.42866, it/sec: 4.423638089520506
epoch 1 iter 53960: train loss 1.45583. lr 3.000000e-04, running loss 1.42875, it/sec: 4.412430437912196
epoch 1 iter 53980: train loss 1.40556. lr 3.000000e-04, running loss 1.42869, it/sec: 4.450007883554641
epoch 1 iter 54000: train loss 1.42333. lr 3.000000e-04, running loss 1.42871, it/sec: 4.43518315089778
epoch 1 iter 54020: train loss 1.41250. lr 3.000000e-04, running loss 1.42873, it/sec: 4.399600005501539
epoch 1 iter 54040: train loss 1.43411. lr 3.000000e-04, running loss 1.42871, it/sec: 4.443790238815692
epoch 1 iter 54060: train loss 1.42366. lr 3.000000e-04, running loss 1.42873, it/sec: 4.397857762085423
epoch 1 iter 54080: train loss 1.42310. lr 3.000000e-04, running loss 1.42882, it/sec: 4.375809598341294
epoch 1 iter 54100: train loss 1.42087. lr 3.000000e-04, running loss 1.42877, it/sec: 4.371254910987715
epoch 1 iter 54120: train loss 1.42082. lr 3.000000e-04, running loss 1.42890, it/sec: 4.367710677643779
epoch 1 iter 54140: train loss 1.44762. lr 3.000000e-04, running loss 1.42885, it/sec: 4.349146185511118
epoch 1 iter 54160: train loss 1.44471. lr 3.000000e-04, running loss 1.42882, it/sec: 4.362646452642588
epoch 1 iter 54180: train loss 1.41368. lr 3.000000e-04, running loss 1.42877, it/sec: 4.375161429119468
epoch 1 iter 54200: train loss 1.43132. lr 3.000000e-04, running loss 1.42876, it/sec: 4.375323052833016
epoch 1 iter 54220: train loss 1.43256. lr 3.000000e-04, running loss 1.42870, it/sec: 4.337549601671253
epoch 1 iter 54240: train loss 1.40848. lr 3.000000e-04, running loss 1.42868, it/sec: 4.360793227748845
epoch 1 iter 54260: train loss 1.43451. lr 3.000000e-04, running loss 1.42860, it/sec: 4.385449949557114
epoch 1 iter 54280: train loss 1.43477. lr 3.000000e-04, running loss 1.42854, it/sec: 4.377456556839295
epoch 1 iter 54300: train loss 1.42570. lr 3.000000e-04, running loss 1.42856, it/sec: 4.378825218802565
epoch 1 iter 54320: train loss 1.42907. lr 3.000000e-04, running loss 1.42859, it/sec: 4.37818261504029
epoch 1 iter 54340: train loss 1.43500. lr 3.000000e-04, running loss 1.42864, it/sec: 4.361924186642421
epoch 1 iter 54360: train loss 1.43511. lr 3.000000e-04, running loss 1.42858, it/sec: 4.36876040813598
epoch 1 iter 54380: train loss 1.43959. lr 3.000000e-04, running loss 1.42862, it/sec: 4.28745023170109
epoch 1 iter 54400: train loss 1.42844. lr 3.000000e-04, running loss 1.42862, it/sec: 4.362207754602054
epoch 1 iter 54420: train loss 1.45050. lr 3.000000e-04, running loss 1.42866, it/sec: 4.369136817793017
epoch 1 iter 54440: train loss 1.42076. lr 3.000000e-04, running loss 1.42875, it/sec: 4.389589865793733
epoch 1 iter 54460: train loss 1.42987. lr 3.000000e-04, running loss 1.42867, it/sec: 4.386892683301508
epoch 1 iter 54480: train loss 1.43076. lr 3.000000e-04, running loss 1.42855, it/sec: 4.354835420990368
epoch 1 iter 54500: train loss 1.43252. lr 3.000000e-04, running loss 1.42854, it/sec: 4.351691088899328
epoch 1 iter 54520: train loss 1.42996. lr 3.000000e-04, running loss 1.42854, it/sec: 4.404800690683751
epoch 1 iter 54540: train loss 1.43432. lr 3.000000e-04, running loss 1.42858, it/sec: 4.384850333631445
epoch 1 iter 54560: train loss 1.43474. lr 3.000000e-04, running loss 1.42852, it/sec: 4.375454782133305
epoch 1 iter 54580: train loss 1.42829. lr 3.000000e-04, running loss 1.42853, it/sec: 4.3545838337519305
epoch 1 iter 54600: train loss 1.44831. lr 3.000000e-04, running loss 1.42856, it/sec: 4.362076231394919
epoch 1 iter 54620: train loss 1.43050. lr 3.000000e-04, running loss 1.42849, it/sec: 4.350161366919673
epoch 1 iter 54640: train loss 1.44716. lr 3.000000e-04, running loss 1.42843, it/sec: 4.360912844507906
epoch 1 iter 54660: train loss 1.43667. lr 3.000000e-04, running loss 1.42847, it/sec: 4.364085798155372
epoch 1 iter 54680: train loss 1.42899. lr 3.000000e-04, running loss 1.42844, it/sec: 4.370091838270851
epoch 1 iter 54700: train loss 1.42819. lr 3.000000e-04, running loss 1.42835, it/sec: 4.428236508471065
epoch 1 iter 54720: train loss 1.44246. lr 3.000000e-04, running loss 1.42831, it/sec: 4.424315480631098
epoch 1 iter 54740: train loss 1.43104. lr 3.000000e-04, running loss 1.42835, it/sec: 4.416326147719067
epoch 1 iter 54760: train loss 1.43514. lr 3.000000e-04, running loss 1.42836, it/sec: 4.427664265638935
epoch 1 iter 54780: train loss 1.43389. lr 3.000000e-04, running loss 1.42839, it/sec: 4.4222886409958315
epoch 1 iter 54800: train loss 1.42817. lr 3.000000e-04, running loss 1.42839, it/sec: 4.402696994421221
epoch 1 iter 54820: train loss 1.42254. lr 3.000000e-04, running loss 1.42833, it/sec: 4.405425959633038
epoch 1 iter 54840: train loss 1.43564. lr 3.000000e-04, running loss 1.42831, it/sec: 4.410516011169053
epoch 1 iter 54860: train loss 1.43791. lr 3.000000e-04, running loss 1.42854, it/sec: 4.4079196383578285
epoch 1 iter 54880: train loss 1.42311. lr 3.000000e-04, running loss 1.42854, it/sec: 4.434749864472831
epoch 1 iter 54900: train loss 1.41667. lr 3.000000e-04, running loss 1.42852, it/sec: 4.424100737728329
epoch 1 iter 54920: train loss 1.40802. lr 3.000000e-04, running loss 1.42843, it/sec: 4.441079448252684
epoch 1 iter 54940: train loss 1.42187. lr 3.000000e-04, running loss 1.42907, it/sec: 4.4299722371268055
epoch 1 iter 54960: train loss 1.42747. lr 3.000000e-04, running loss 1.42911, it/sec: 4.37136315979468
epoch 1 iter 54980: train loss 1.42210. lr 3.000000e-04, running loss 1.42917, it/sec: 4.434440385935806
epoch 1 iter 55000: train loss 1.44180. lr 3.000000e-04, running loss 1.42912, it/sec: 4.435754053190284
epoch 1 iter 55020: train loss 1.42161. lr 3.000000e-04, running loss 1.42904, it/sec: 4.402046475640614
epoch 1 iter 55040: train loss 1.45109. lr 3.000000e-04, running loss 1.42904, it/sec: 4.417060553220453
epoch 1 iter 55060: train loss 1.42459. lr 3.000000e-04, running loss 1.42908, it/sec: 4.401949665922478
epoch 1 iter 55080: train loss 1.42840. lr 3.000000e-04, running loss 1.42902, it/sec: 4.418882036801969
epoch 1 iter 55100: train loss 1.43449. lr 3.000000e-04, running loss 1.42895, it/sec: 4.435409849569895
epoch 1 iter 55120: train loss 1.43970. lr 3.000000e-04, running loss 1.42893, it/sec: 4.43968606788322
epoch 1 iter 55140: train loss 1.42505. lr 3.000000e-04, running loss 1.42884, it/sec: 4.423272167587143
epoch 1 iter 55160: train loss 1.41897. lr 3.000000e-04, running loss 1.42883, it/sec: 4.439228194343327
epoch 1 iter 55180: train loss 1.43220. lr 3.000000e-04, running loss 1.42889, it/sec: 4.419309357514673
epoch 1 iter 55200: train loss 1.42265. lr 3.000000e-04, running loss 1.42886, it/sec: 4.379415055063771
epoch 1 iter 55220: train loss 1.43127. lr 3.000000e-04, running loss 1.42890, it/sec: 4.351661868463185
epoch 1 iter 55240: train loss 1.43703. lr 3.000000e-04, running loss 1.42892, it/sec: 4.373624539883781
epoch 1 iter 55260: train loss 1.44833. lr 3.000000e-04, running loss 1.42893, it/sec: 4.351105457678036
epoch 1 iter 55280: train loss 1.42847. lr 3.000000e-04, running loss 1.42897, it/sec: 4.342722614122865
epoch 1 iter 55300: train loss 1.41597. lr 3.000000e-04, running loss 1.42889, it/sec: 4.3722199753231905
epoch 1 iter 55320: train loss 1.41651. lr 3.000000e-04, running loss 1.42890, it/sec: 4.362928743936046
epoch 1 iter 55340: train loss 1.42384. lr 3.000000e-04, running loss 1.42892, it/sec: 4.376327184404592
epoch 1 iter 55360: train loss 1.44627. lr 3.000000e-04, running loss 1.42898, it/sec: 4.3816194029723015
epoch 1 iter 55380: train loss 1.42649. lr 3.000000e-04, running loss 1.42897, it/sec: 4.361008676810416
epoch 1 iter 55400: train loss 1.43011. lr 3.000000e-04, running loss 1.42896, it/sec: 4.386979517043308
epoch 1 iter 55420: train loss 1.42293. lr 3.000000e-04, running loss 1.42892, it/sec: 4.3858724821919415
epoch 1 iter 55440: train loss 1.41953. lr 3.000000e-04, running loss 1.42883, it/sec: 4.377021562960926
epoch 1 iter 55460: train loss 1.42937. lr 3.000000e-04, running loss 1.42888, it/sec: 4.369230472807898
epoch 1 iter 55480: train loss 1.41764. lr 3.000000e-04, running loss 1.42881, it/sec: 4.3604599701604805
epoch 1 iter 55500: train loss 1.43343. lr 3.000000e-04, running loss 1.42898, it/sec: 4.355709428036285
epoch 1 iter 55520: train loss 1.43255. lr 3.000000e-04, running loss 1.42898, it/sec: 4.359073805687805
epoch 1 iter 55540: train loss 1.43559. lr 3.000000e-04, running loss 1.42895, it/sec: 4.344866797485027
epoch 1 iter 55560: train loss 1.41423. lr 3.000000e-04, running loss 1.42894, it/sec: 4.400793501898678
epoch 1 iter 55580: train loss 1.43051. lr 3.000000e-04, running loss 1.42884, it/sec: 4.418935052680285
epoch 1 iter 55600: train loss 1.40780. lr 3.000000e-04, running loss 1.42881, it/sec: 4.426911337504852
epoch 1 iter 55620: train loss 1.43044. lr 3.000000e-04, running loss 1.42880, it/sec: 4.428199427787167
epoch 1 iter 55640: train loss 1.40723. lr 3.000000e-04, running loss 1.42881, it/sec: 4.422762314274516
epoch 1 iter 55660: train loss 1.39185. lr 3.000000e-04, running loss 1.42880, it/sec: 4.409137443647009
epoch 1 iter 55680: train loss 1.42695. lr 3.000000e-04, running loss 1.42879, it/sec: 4.384746644264099
epoch 1 iter 55700: train loss 1.40922. lr 3.000000e-04, running loss 1.42873, it/sec: 4.360978761004846
epoch 1 iter 55720: train loss 1.44264. lr 3.000000e-04, running loss 1.42880, it/sec: 4.391963390236859
epoch 1 iter 55740: train loss 1.42645. lr 3.000000e-04, running loss 1.42871, it/sec: 4.38691766270301
epoch 1 iter 55760: train loss 1.41699. lr 3.000000e-04, running loss 1.42861, it/sec: 4.383955195763309
epoch 1 iter 55780: train loss 1.44561. lr 3.000000e-04, running loss 1.42866, it/sec: 4.422652678749752
epoch 1 iter 55800: train loss 1.40938. lr 3.000000e-04, running loss 1.42857, it/sec: 4.422577413857535
epoch 1 iter 55820: train loss 1.42110. lr 3.000000e-04, running loss 1.42857, it/sec: 4.421741768626882
epoch 1 iter 55840: train loss 1.43771. lr 3.000000e-04, running loss 1.42856, it/sec: 4.400431873557296
epoch 1 iter 55860: train loss 1.42099. lr 3.000000e-04, running loss 1.42851, it/sec: 4.4177540039933
epoch 1 iter 55880: train loss 1.42775. lr 3.000000e-04, running loss 1.42841, it/sec: 4.436915906032026
epoch 1 iter 55900: train loss 1.42819. lr 3.000000e-04, running loss 1.42833, it/sec: 4.3756586809969304
epoch 1 iter 55920: train loss 1.42701. lr 3.000000e-04, running loss 1.42829, it/sec: 4.368428927546837
epoch 1 iter 55940: train loss 1.43810. lr 3.000000e-04, running loss 1.42826, it/sec: 4.403704308102624
epoch 1 iter 55960: train loss 1.43615. lr 3.000000e-04, running loss 1.42829, it/sec: 4.3973616585413735
epoch 1 iter 55980: train loss 1.42994. lr 3.000000e-04, running loss 1.42832, it/sec: 4.421901921734189
epoch 1 iter 56000: train loss 1.42646. lr 3.000000e-04, running loss 1.42827, it/sec: 4.426976715827387
epoch 1 iter 56020: train loss 1.42538. lr 3.000000e-04, running loss 1.42823, it/sec: 4.417663469040558
epoch 1 iter 56040: train loss 1.42195. lr 3.000000e-04, running loss 1.42823, it/sec: 4.432311101820799
epoch 1 iter 56060: train loss 1.42229. lr 3.000000e-04, running loss 1.42820, it/sec: 4.409691451570513
epoch 1 iter 56080: train loss 1.41628. lr 3.000000e-04, running loss 1.42823, it/sec: 4.361800898672084
epoch 1 iter 56100: train loss 1.44243. lr 3.000000e-04, running loss 1.42829, it/sec: 4.38841994504091
epoch 1 iter 56120: train loss 1.40831. lr 3.000000e-04, running loss 1.42823, it/sec: 4.344455224406249
epoch 1 iter 56140: train loss 1.42428. lr 3.000000e-04, running loss 1.42849, it/sec: 4.360815267485647
epoch 1 iter 56160: train loss 1.43171. lr 3.000000e-04, running loss 1.42853, it/sec: 4.4060576561742275
epoch 1 iter 56180: train loss 1.41583. lr 3.000000e-04, running loss 1.42846, it/sec: 4.377818062091336
epoch 1 iter 56200: train loss 1.42592. lr 3.000000e-04, running loss 1.42836, it/sec: 4.33583364238876
epoch 1 iter 56220: train loss 1.42419. lr 3.000000e-04, running loss 1.42838, it/sec: 4.34788863998755
epoch 1 iter 56240: train loss 1.43140. lr 3.000000e-04, running loss 1.42837, it/sec: 4.357336846325195
epoch 1 iter 56260: train loss 1.43749. lr 3.000000e-04, running loss 1.42837, it/sec: 4.376224568819989
epoch 1 iter 56280: train loss 1.42591. lr 3.000000e-04, running loss 1.42838, it/sec: 4.415409750356306
epoch 1 iter 56300: train loss 1.43808. lr 3.000000e-04, running loss 1.42844, it/sec: 4.397454824481019
epoch 1 iter 56320: train loss 1.44694. lr 3.000000e-04, running loss 1.42848, it/sec: 4.366763314956872
epoch 1 iter 56340: train loss 1.42342. lr 3.000000e-04, running loss 1.42844, it/sec: 4.377945955516314
epoch 1 iter 56360: train loss 1.43004. lr 3.000000e-04, running loss 1.42846, it/sec: 4.352122615785942
epoch 1 iter 56380: train loss 1.43449. lr 3.000000e-04, running loss 1.42844, it/sec: 4.358512799973883
epoch 1 iter 56400: train loss 1.44246. lr 3.000000e-04, running loss 1.42847, it/sec: 4.3554526737406265
epoch 1 iter 56420: train loss 1.43707. lr 3.000000e-04, running loss 1.42851, it/sec: 4.403264585640314
epoch 1 iter 56440: train loss 1.40073. lr 3.000000e-04, running loss 1.42851, it/sec: 4.419307717094411
epoch 1 iter 56460: train loss 1.42997. lr 3.000000e-04, running loss 1.42857, it/sec: 4.4112101720631305
epoch 1 iter 56480: train loss 1.41258. lr 3.000000e-04, running loss 1.42860, it/sec: 4.436206074927104
epoch 1 iter 56500: train loss 1.44335. lr 3.000000e-04, running loss 1.42856, it/sec: 4.405724974950519
epoch 1 iter 56520: train loss 1.41549. lr 3.000000e-04, running loss 1.42851, it/sec: 4.444667389416481
epoch 1 iter 56540: train loss 1.41249. lr 3.000000e-04, running loss 1.42852, it/sec: 4.41525950015619
epoch 1 iter 56560: train loss 1.42678. lr 3.000000e-04, running loss 1.42854, it/sec: 4.421416038714304
epoch 1 iter 56580: train loss 1.42273. lr 3.000000e-04, running loss 1.42863, it/sec: 4.426889741362944
epoch 1 iter 56600: train loss 1.42375. lr 3.000000e-04, running loss 1.42861, it/sec: 4.437598058910191
epoch 1 iter 56620: train loss 1.42450. lr 3.000000e-04, running loss 1.42860, it/sec: 4.382287441978197
epoch 1 iter 56640: train loss 1.41966. lr 3.000000e-04, running loss 1.42860, it/sec: 4.4235536711139805
epoch 1 iter 56660: train loss 1.43767. lr 3.000000e-04, running loss 1.42852, it/sec: 4.362523447313709
epoch 1 iter 56680: train loss 1.43730. lr 3.000000e-04, running loss 1.42850, it/sec: 4.359964170363877
epoch 1 iter 56700: train loss 1.42054. lr 3.000000e-04, running loss 1.42849, it/sec: 4.348997196401154
epoch 1 iter 56720: train loss 1.43180. lr 3.000000e-04, running loss 1.42851, it/sec: 4.35082622802735
epoch 1 iter 56740: train loss 1.43301. lr 3.000000e-04, running loss 1.42854, it/sec: 4.370675007400044
epoch 1 iter 56760: train loss 1.43359. lr 3.000000e-04, running loss 1.42854, it/sec: 4.3329800905368
epoch 1 iter 56780: train loss 1.42684. lr 3.000000e-04, running loss 1.42873, it/sec: 4.3128242926836355
epoch 1 iter 56800: train loss 1.42349. lr 3.000000e-04, running loss 1.42870, it/sec: 4.4022035797030314
epoch 1 iter 56820: train loss 1.40987. lr 3.000000e-04, running loss 1.42863, it/sec: 4.430053759238066
epoch 1 iter 56840: train loss 1.40464. lr 3.000000e-04, running loss 1.42858, it/sec: 4.363685198859478
epoch 1 iter 56860: train loss 1.42772. lr 3.000000e-04, running loss 1.42943, it/sec: 4.365023825549847
epoch 1 iter 56880: train loss 1.43789. lr 3.000000e-04, running loss 1.42951, it/sec: 4.383502537271089
epoch 1 iter 56900: train loss 1.43766. lr 3.000000e-04, running loss 1.42956, it/sec: 4.409055484423764
epoch 1 iter 56920: train loss 1.40151. lr 3.000000e-04, running loss 1.42945, it/sec: 4.424750450119924
epoch 1 iter 56940: train loss 1.44085. lr 3.000000e-04, running loss 1.42944, it/sec: 4.36382693034754
epoch 1 iter 56960: train loss 1.40804. lr 3.000000e-04, running loss 1.42946, it/sec: 4.436771451949312
epoch 1 iter 56980: train loss 1.43695. lr 3.000000e-04, running loss 1.42942, it/sec: 4.454277668158904
epoch 1 iter 57000: train loss 1.45453. lr 3.000000e-04, running loss 1.42941, it/sec: 4.444100668594893
epoch 1 iter 57020: train loss 1.42407. lr 3.000000e-04, running loss 1.42944, it/sec: 4.428520017540424
epoch 1 iter 57040: train loss 1.43838. lr 3.000000e-04, running loss 1.42952, it/sec: 4.42182404312091
epoch 1 iter 57060: train loss 1.43967. lr 3.000000e-04, running loss 1.42949, it/sec: 4.4415999252142635
epoch 1 iter 57080: train loss 1.42952. lr 3.000000e-04, running loss 1.42946, it/sec: 4.440010826875687
epoch 1 iter 57100: train loss 1.41668. lr 3.000000e-04, running loss 1.42941, it/sec: 4.417685151333627
epoch 1 iter 57120: train loss 1.43230. lr 3.000000e-04, running loss 1.42939, it/sec: 4.441332807704746
epoch 1 iter 57140: train loss 1.41947. lr 3.000000e-04, running loss 1.42940, it/sec: 4.424661057198173
epoch 1 iter 57160: train loss 1.40372. lr 3.000000e-04, running loss 1.42929, it/sec: 4.426961429684258
epoch 1 iter 57180: train loss 1.42234. lr 3.000000e-04, running loss 1.42923, it/sec: 4.416974826473548
epoch 1 iter 57200: train loss 1.43026. lr 3.000000e-04, running loss 1.42925, it/sec: 4.38670792029021
epoch 1 iter 57220: train loss 1.42728. lr 3.000000e-04, running loss 1.42922, it/sec: 4.42086854256741
epoch 1 iter 57240: train loss 1.44377. lr 3.000000e-04, running loss 1.42920, it/sec: 4.420897526915061
epoch 1 iter 57260: train loss 1.44042. lr 3.000000e-04, running loss 1.42920, it/sec: 4.416565161760759
epoch 1 iter 57280: train loss 1.43129. lr 3.000000e-04, running loss 1.42925, it/sec: 4.3485881294205715
epoch 1 iter 57300: train loss 1.42230. lr 3.000000e-04, running loss 1.42917, it/sec: 4.342621737427271
epoch 1 iter 57320: train loss 1.42657. lr 3.000000e-04, running loss 1.42917, it/sec: 4.37991699609126
epoch 1 iter 57340: train loss 1.40914. lr 3.000000e-04, running loss 1.42934, it/sec: 4.358084221501151
epoch 1 iter 57360: train loss 1.43020. lr 3.000000e-04, running loss 1.42929, it/sec: 4.360992606312775
epoch 1 iter 57380: train loss 1.43310. lr 3.000000e-04, running loss 1.42930, it/sec: 4.339750755030938
epoch 1 iter 57400: train loss 1.41878. lr 3.000000e-04, running loss 1.42922, it/sec: 4.35780965888004
epoch 1 iter 57420: train loss 1.42569. lr 3.000000e-04, running loss 1.42917, it/sec: 4.347050724609798
epoch 1 iter 57440: train loss 1.41370. lr 3.000000e-04, running loss 1.42910, it/sec: 4.40155399948255
epoch 1 iter 57460: train loss 1.43334. lr 3.000000e-04, running loss 1.42922, it/sec: 4.368222952961649
epoch 1 iter 57480: train loss 1.42847. lr 3.000000e-04, running loss 1.42919, it/sec: 4.3692668391706535
epoch 1 iter 57500: train loss 1.41563. lr 3.000000e-04, running loss 1.42914, it/sec: 4.384832125077984
epoch 1 iter 57520: train loss 1.46372. lr 3.000000e-04, running loss 1.42913, it/sec: 4.393994514189074
epoch 1 iter 57540: train loss 1.43172. lr 3.000000e-04, running loss 1.42912, it/sec: 4.366574181624752
epoch 1 iter 57560: train loss 1.43671. lr 3.000000e-04, running loss 1.42911, it/sec: 4.3874047893254415
epoch 1 iter 57580: train loss 1.43365. lr 3.000000e-04, running loss 1.42909, it/sec: 4.421407515882995
epoch 1 iter 57600: train loss 1.41471. lr 3.000000e-04, running loss 1.42898, it/sec: 4.42261811655817
epoch 1 iter 57620: train loss 1.41890. lr 3.000000e-04, running loss 1.42885, it/sec: 4.425913890274874
epoch 1 iter 57640: train loss 1.43438. lr 3.000000e-04, running loss 1.42886, it/sec: 4.407643461507268
epoch 1 iter 57660: train loss 1.43921. lr 3.000000e-04, running loss 1.42881, it/sec: 4.429195433335933
epoch 1 iter 57680: train loss 1.41824. lr 3.000000e-04, running loss 1.42883, it/sec: 4.425931892034229
epoch 1 iter 57700: train loss 1.42250. lr 3.000000e-04, running loss 1.42881, it/sec: 4.3932467957724946
epoch 1 iter 57720: train loss 1.40729. lr 3.000000e-04, running loss 1.42891, it/sec: 4.397265363155115
epoch 1 iter 57740: train loss 1.40839. lr 3.000000e-04, running loss 1.42890, it/sec: 4.418488632218136
epoch 1 iter 57760: train loss 1.43781. lr 3.000000e-04, running loss 1.42889, it/sec: 4.427970426350057
epoch 1 iter 57780: train loss 1.42136. lr 3.000000e-04, running loss 1.42888, it/sec: 4.419095940312739
epoch 1 iter 57800: train loss 1.41614. lr 3.000000e-04, running loss 1.42873, it/sec: 4.416456144343054
epoch 1 iter 57820: train loss 1.42646. lr 3.000000e-04, running loss 1.42855, it/sec: 4.40518919566517
epoch 1 iter 57840: train loss 1.42284. lr 3.000000e-04, running loss 1.42850, it/sec: 4.356273411612185
epoch 1 iter 57860: train loss 1.42021. lr 3.000000e-04, running loss 1.42841, it/sec: 4.364572363445704
epoch 1 iter 57880: train loss 1.43602. lr 3.000000e-04, running loss 1.42852, it/sec: 4.356220410684792
epoch 1 iter 57900: train loss 1.42738. lr 3.000000e-04, running loss 1.42845, it/sec: 4.362134476455678
epoch 1 iter 57920: train loss 1.43451. lr 3.000000e-04, running loss 1.42861, it/sec: 4.365205468716576
epoch 1 iter 57940: train loss 1.43200. lr 3.000000e-04, running loss 1.42861, it/sec: 4.376380562018129
There was an issue while trying to parse BedFrame input
line 1854149: 2	6767504

catch_bed_parser_error is enabled. self.chrom will be None
device: cuda:0
save interval: 100000
batch size: 32
